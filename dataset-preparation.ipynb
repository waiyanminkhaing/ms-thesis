{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (838.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.3/838.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m177.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m187.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m191.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m219.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m191.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.1 torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sentence_transformers) (2.5.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sentence_transformers) (1.15.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Downloading sentence_transformers-3.4.0-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.2 sentence_transformers-3.4.0 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 17:19:54.033450: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 17:20:09.742276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-24 17:20:18.736113: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-24 17:20:18.819274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 17:20:31.224476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-24 17:20:45.519498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Transformers is only compatible with Keras 2, but you have explicitly set `TF_USE_LEGACY_KERAS` to `0`. This may result in unexpected behaviour or errors if Keras 3 objects are passed to Transformers models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, SeamlessM4TForTextToText, AutoProcessor\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 17:22:14.682670: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-24 17:22:27.603861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-24 17:22:27.679899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save gen df\n",
    "def save_gen_df(df, df_name):\n",
    "    df.to_csv(f\"gen/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load generated df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Dataset Loading\n",
    "\n",
    "This step involves loading the datasets `myXNLI` and `ALT Corpus` into pandas DataFrames. \n",
    "The English and Burmese datasets from the ALT Corpus are combined to create a bilingual parallel corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myXNLI dataset loaded successfully with 392702 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load myXNLI dataset\n",
    "myxnli_path = './data/myXNLI.train.tsv'  # Path to the file\n",
    "myxnli_data = pd.read_csv(myxnli_path, sep='\\t', header=0)\n",
    "print(f\"myXNLI dataset loaded successfully with {len(myxnli_data)} records.\")\n",
    "display(myxnli_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT English dataset loaded successfully with 19908 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT English data\n",
    "alt_en_path = './data/ALT_data_en.txt'  # Path to the English ALT corpus\n",
    "alt_en_data = pd.read_csv(alt_en_path, sep='\\t', header=None, names=[\"ID\", \"English_Sentence\"])\n",
    "print(f\"ALT English dataset loaded successfully with {len(alt_en_data)} records.\")\n",
    "display(alt_en_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT Burmese dataset loaded successfully with 19265 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT Burmese data\n",
    "alt_my_path = './data/ALT_data_my.txt'  # Path to the Burmese ALT corpus\n",
    "alt_my_data = pd.read_csv(alt_my_path, sep='\\t', header=None, names=[\"ID\", \"Burmese_Sentence\"])\n",
    "print(f\"ALT Burmese dataset loaded successfully with {len(alt_my_data)} records.\")\n",
    "display(alt_my_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT combined dataset created successfully with 19173 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine ALT datasets (if IDs match)\n",
    "alt_combined = pd.merge(alt_en_data, alt_my_data, on=\"ID\")\n",
    "print(f\"ALT combined dataset created successfully with {len(alt_combined)} records.\")\n",
    "display(alt_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Data Cleaning\n",
    "This step focuses on cleaning the datasets to prepare them for further processing. The cleaning operations include:\n",
    "1. Removing duplicate entries.\n",
    "2. Handling missing values.\n",
    "3. Removing non-standard characters or symbols unrelated to the Burmese or English language.\n",
    "4. Ensuring consistent formatting.\n",
    "\n",
    "The cleaned datasets will be ready for normalization and tokenization in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning myXNLI dataset...\n",
      "myXNLI dataset cleaned successfully.\n",
      "Original Records: 392702.\n",
      "Remaining records: 392682.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning myXNLI dataset\n",
    "print(\"Cleaning myXNLI dataset...\")\n",
    "myxnli_cleaned = myxnli_data.drop_duplicates()  # Remove duplicates\n",
    "myxnli_cleaned = myxnli_cleaned.dropna()  # Remove rows with missing values\n",
    "#myxnli_cleaned = myxnli_cleaned.replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"myXNLI dataset cleaned successfully.\")\n",
    "print(f\"Original Records: {len(myxnli_data)}.\")\n",
    "print(f\"Remaining records: {len(myxnli_cleaned)}.\")\n",
    "display(myxnli_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned myXNLI dataset\n",
    "save_gen_df(myxnli_cleaned, \"myxnli_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT English dataset...\n",
      "ALT English dataset cleaned successfully.\n",
      "Original records: 19908.\n",
      "Remaining records: 19908.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT English data\n",
    "print(\"Cleaning ALT English dataset...\")\n",
    "alt_en_cleaned = alt_en_data.drop_duplicates()  # Remove duplicates\n",
    "alt_en_cleaned = alt_en_cleaned.dropna()  # Remove rows with missing values\n",
    "alt_en_cleaned[\"English_Sentence\"] = alt_en_cleaned[\"English_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT English dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_en_data)}.\")\n",
    "print(f\"Remaining records: {len(alt_en_cleaned)}.\")\n",
    "display(alt_en_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT Burmese dataset...\n",
      "ALT Burmese dataset cleaned successfully.\n",
      "Original records: 19265\n",
      "Remaining records: 19258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT Burmese data\n",
    "print(\"Cleaning ALT Burmese dataset...\")\n",
    "alt_my_cleaned = alt_my_data.drop_duplicates()  # Remove duplicates\n",
    "alt_my_cleaned = alt_my_cleaned.dropna()  # Remove rows with missing values\n",
    "#alt_my_cleaned[\"Burmese_Sentence\"] = alt_my_cleaned[\"Burmese_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT Burmese dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_my_data)}\")\n",
    "print(f\"Remaining records: {len(alt_my_cleaned)}\")\n",
    "display(alt_my_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning combined ALT dataset...\n",
      "Combined ALT dataset cleaned successfully.\n",
      "Original records: 19173\n",
      "Remaining records: 19166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine cleaned ALT datasets\n",
    "print(\"Cleaning combined ALT dataset...\")\n",
    "alt_combined_cleaned = pd.merge(alt_en_cleaned, alt_my_cleaned, on=\"ID\")\n",
    "print(f\"Combined ALT dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_combined)}\")\n",
    "print(f\"Remaining records: {len(alt_combined_cleaned)}\")\n",
    "display(alt_combined_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned combined ALT dataset\n",
    "save_gen_df(alt_combined_cleaned, \"alt_combined_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Data Normalization\n",
    "This step normalizes the text data to ensure consistency across datasets. The normalization process includes:\n",
    "1. Applying Unicode normalization to handle encoding inconsistencies.\n",
    "2. Standardizing text formatting by converting all text to lowercase and standardizing punctuation.\n",
    "3. Normalizing diacritical marks and stacked consonants in the Burmese text to improve text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Apply Unicode normalization\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Convert to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    # Standardize punctuation (e.g., replace unusual punctuation marks)\n",
    "    normalized_text = normalized_text.replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize Burmese text (handles diacritical marks and stacked consonants)\n",
    "def normalize_burmese(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Additional Burmese-specific normalization can be added here if needed\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing myXNLI dataset...\n",
      "myXNLI dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>conceptually cream skimming has two basic dime...</td>\n",
       "      <td>product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>you lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>one of our number will carry out your instruct...</td>\n",
       "      <td>a member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>how do you know? all this is their information...</td>\n",
       "      <td>this information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>the tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  one of our number will carry out your instruct...   \n",
       "3     fiction  entailment  how do you know? all this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  product and geography are what make cream skim...   \n",
       "1  you lose the things to the following level if ...   \n",
       "2  a member of my team will execute your orders w...   \n",
       "3                  this information belongs to them.   \n",
       "4           the tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize myXNLI cleaned dataset\n",
    "print(\"Normalizing myXNLI dataset...\")\n",
    "myxnli_normalized = load_gen_df(\"myxnli_cleaned\")\n",
    "\n",
    "# Normalize English columns\n",
    "myxnli_normalized[\"sentence1_en\"] = myxnli_normalized[\"sentence1_en\"].apply(normalize_text)\n",
    "myxnli_normalized[\"sentence2_en\"] = myxnli_normalized[\"sentence2_en\"].apply(normalize_text)\n",
    "\n",
    "# Normalize Burmese columns\n",
    "myxnli_normalized[\"sentence1_my\"] = myxnli_normalized[\"sentence1_my\"].apply(normalize_burmese)\n",
    "myxnli_normalized[\"sentence2_my\"] = myxnli_normalized[\"sentence2_my\"].apply(normalize_burmese)\n",
    "\n",
    "print(f\"myXNLI dataset normalized successfully.\")\n",
    "display(myxnli_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized myXNLI dataset\n",
    "save_gen_df(myxnli_normalized, \"myxnli_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing combined ALT dataset...\n",
      "Combined ALT dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>italy have defeated portugal 315 in pool c of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>andrea masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>portugal never gave up and david penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  italy have defeated portugal 315 in pool c of ...   \n",
       "1  SNT.80188.2  andrea masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  portugal never gave up and david penalva score...   \n",
       "4  SNT.80188.5  italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize combined ALT cleaned dataset\n",
    "print(\"Normalizing combined ALT dataset...\")\n",
    "alt_combined_normalized = load_gen_df(\"alt_combined_cleaned\")\n",
    "alt_combined_normalized[\"English_Sentence\"] = alt_combined_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "alt_combined_normalized[\"Burmese_Sentence\"] = alt_combined_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"Combined ALT dataset normalized successfully.\")\n",
    "display(alt_combined_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized ALT dataset\n",
    "save_gen_df(alt_combined_normalized, \"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Back-Translation Augmentation\n",
    "This step applies back-translation augmentation to the `myXNLI` and `ALT Combined` datasets. Models (`facebook/nllb-200-distilled-600M` and `facebook/hf-seamless-m4t-large`) are used to generate synthetic data. \n",
    "Results are stored in additional columns for evaluation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized myXNLI and Combined ALT  dataset\n",
    "myxnli_back_translated = load_gen_df(\"myxnli_normalized\")\n",
    "alt_combined_back_translated = load_gen_df(\"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLLB model and tokenizer\n",
    "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name).to(device)\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using NLLB\n",
    "def back_translate_nllb(text, src_lang=\"eng_Latn\", tgt_lang=\"mya_Mymr\"):\n",
    "    try:\n",
    "        # Forward translation: English -> Burmese\n",
    "        inputs = nllb_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        translated_tokens = nllb_model.generate(\n",
    "            **inputs, forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "        )\n",
    "        translated_text = nllb_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Backward translation: Burmese -> English\n",
    "        back_inputs = nllb_tokenizer(translated_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        back_translated_tokens = nllb_model.generate(\n",
    "            **back_inputs, forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(src_lang)\n",
    "        )\n",
    "        back_translated_text = nllb_tokenizer.batch_decode(back_translated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return translated_text, back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### myXNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to myXNLI dataset (sentences 1)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e98fdc35f54c80b7ddb0a121f77ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the sentences 1 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 1)...\")\n",
    "myxnli_nllb_back_translated_start_index_1 = 0\n",
    "\n",
    "with open(\"tmp/myxnli_nllb_back_translated_1.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_nllb_back_translated_start_index_1 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_nllb_back_translated_start_index_1:].iterrows(), \n",
    "                           total=len(myxnli_back_translated) - myxnli_nllb_back_translated_start_index_1):\n",
    "        original = row[\"sentence1_en\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 1)\n",
    "tmp_myxnli_nllb_back_translated_1 = load_tmp_df(\"myxnli_nllb_back_translated_1\")\n",
    "myxnli_back_translated[\"nllb_translated_s1\"] = tmp_myxnli_nllb_back_translated_1[\"translated\"]\n",
    "myxnli_back_translated[\"nllb_back_translated_s1\"] = tmp_myxnli_nllb_back_translated_1[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to myXNLI dataset (sentences 2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22945169d5d4407286b986520569a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/295188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the sentences 2 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 2)...\")\n",
    "myxnli_nllb_back_translated_start_index_2 = 97494\n",
    "with open(\"tmp/myxnli_nllb_back_translated_2.csv\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_nllb_back_translated_start_index_2 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_nllb_back_translated_start_index_2:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_nllb_back_translated_start_index_2):\n",
    "        original = row[\"sentence2_en\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\") if original is not None else None\n",
    "        translated = translated.replace('\"', \"'\") if translated is not None else None\n",
    "        back_translated = back_translated.replace('\"', \"'\") if back_translated is not None else None\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 2)\n",
    "tmp_myxnli_nllb_back_translated_2 = load_tmp_df(\"myxnli_nllb_back_translated_2\")\n",
    "myxnli_back_translated[\"nllb_translated_s2\"] = tmp_myxnli_nllb_back_translated_2[\"translated\"]\n",
    "myxnli_back_translated[\"nllb_back_translated_s2\"] = tmp_myxnli_nllb_back_translated_2[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to alt combined dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbbbd77dd214d19aa9f2416c8d5d00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the alt combined dataset\n",
    "print(\"Applying back-translation to alt combined dataset...\")\n",
    "alt_combined_nllb_back_translated_start_index = 0\n",
    "with open(\"tmp/alt_combined_nllb_back_translated.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if alt_combined_nllb_back_translated_start_index == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(alt_combined_back_translated.iloc[alt_combined_nllb_back_translated_start_index:].iterrows(),\n",
    "                       total=len(alt_combined_back_translated) - alt_combined_nllb_back_translated_start_index):\n",
    "        original = row[\"English_Sentence\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated combined ALT dataset\n",
    "tmp_alt_combined_nllb_back_translated = load_tmp_df(\"alt_combined_nllb_back_translated\")\n",
    "alt_combined_back_translated[\"nllb_translated\"] = tmp_alt_combined_nllb_back_translated[\"translated\"]\n",
    "alt_combined_back_translated[\"nllb_back_translated\"] = tmp_alt_combined_nllb_back_translated[\"back_translated\"]\n",
    "display(alt_combined_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/hf-seamless-m4t-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seamless m4t model and processor\n",
    "seamless_m4t_model_name = \"facebook/hf-seamless-m4t-large\"\n",
    "seamless_m4t_model = SeamlessM4TForTextToText.from_pretrained(seamless_m4t_model_name).to(device)\n",
    "seamless_m4t_processor = AutoProcessor.from_pretrained(seamless_m4t_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using seamless_m4t\n",
    "def back_translate_seamless_m4t(text, src_lang=\"eng\", tgt_lang=\"mya\"):\n",
    "    try:\n",
    "        # Forward translation: English -> Burmese\n",
    "        text_inputs = seamless_m4t_processor(text, src_lang=src_lang, return_tensors=\"pt\", padding=True).to(device)\n",
    "        output_tokens = seamless_m4t_model.generate(**text_inputs, tgt_lang=tgt_lang)\n",
    "        translated_text = seamless_m4t_processor.decode(output_tokens[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Backward translation: Burmese -> English\n",
    "        back_text_inputs = seamless_m4t_processor(translated_text, src_lang=tgt_lang, return_tensors=\"pt\", padding=True).to(device)\n",
    "        back_output_tokens = seamless_m4t_model.generate(**back_text_inputs, tgt_lang=src_lang)\n",
    "        back_translated_text = seamless_m4t_processor.decode(back_output_tokens[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        return translated_text, back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### myXNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the sentences 1 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 1)...\")\n",
    "myxnli_seamless_m4t_back_translated_start_index_1 = 0\n",
    "with open(\"tmp/myxnli_seamless_m4t_back_translated_1.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_seamless_m4t_back_translated_start_index_1 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_seamless_m4t_back_translated_start_index_1:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_seamless_m4t_back_translated_start_index_1):\n",
    "        original = row[\"sentence1_en\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 1)\n",
    "tmp_myxnli_seamless_m4t_back_translated_1 = load_tmp_df(\"myxnli_seamless_m4t_back_translated_1\")\n",
    "myxnli_back_translated[\"seamless_translated_s1\"] = tmp_myxnli_seamless_m4t_back_translated_1[\"translated\"]\n",
    "myxnli_back_translated[\"seamless_back_translated_s1\"] = tmp_myxnli_seamless_m4t_back_translated_1[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the sentences 2 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 2)...\")\n",
    "myxnli_seamless_m4t_back_translated_start_index_2 = 0\n",
    "with open(\"tmp/myxnli_seamless_m4t_back_translated_2.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_seamless_m4t_back_translated_start_index_2 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_seamless_m4t_back_translated_start_index_2:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_seamless_m4t_back_translated_start_index_2):\n",
    "        original = row[\"sentence2_en\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 2)\n",
    "tmp_myxnli_seamless_m4t_back_translated_2 = load_tmp_df(\"myxnli_seamless_m4t_back_translated_2\")\n",
    "myxnli_back_translated[\"seamless_translated_s2\"] = tmp_myxnli_seamless_m4t_back_translated_2[\"translated\"]\n",
    "myxnli_back_translated[\"seamless_back_translated_s2\"] = tmp_myxnli_seamless_m4t_back_translated_2[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to alt combined dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227bfb5e4f7d442894176b8ca21afd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the alt combined dataset\n",
    "print(\"Applying back-translation to alt combined dataset...\")\n",
    "alt_combined_seamless_m4t_back_translated_start_index = 0\n",
    "with open(\"tmp/alt_combined_seamless_m4t_back_translated.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if alt_combined_seamless_m4t_back_translated_start_index == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(alt_combined_back_translated.iloc[alt_combined_seamless_m4t_back_translated_start_index:].iterrows(), \n",
    "                       total=len(alt_combined_back_translated) - alt_combined_seamless_m4t_back_translated_start_index):\n",
    "        original = row[\"English_Sentence\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated combined ALT dataset\n",
    "tmp_alt_combined_seamless_m4t_back_translated = load_tmp_df(\"alt_combined_seamless_m4t_back_translated\")\n",
    "alt_combined_back_translated[\"seamless_translated\"] = tmp_alt_combined_seamless_m4t_back_translated[\"translated\"]\n",
    "alt_combined_back_translated[\"seamless_back_translated\"] = tmp_alt_combined_seamless_m4t_back_translated[\"back_translated\"]\n",
    "display(alt_combined_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Back-Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back-translated myXNLI dataset and combined ALT dataset\n",
    "save_gen_df(myxnli_back_translated, \"myxnli_back_translated\")\n",
    "save_gen_df(alt_combined_back_translated, \"alt_combined_back_translated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pseudo-Parallel Corpus Creation\n",
    "\n",
    "This step involves aligning monolingual English and Burmese text from the datasets to create pseudo-parallel corpora by using models (`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`). \n",
    "Semantic similarity methods are used to identify pairs of sentences with similar meanings. \n",
    "The resulting aligned corpus enhances the dataset and is valuable for low-resource language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized myXNLI and Combined ALT  dataset\n",
    "myxnli_corpus = load_gen_df(\"myxnli_normalized\")\n",
    "alt_combined_corpus = load_gen_df(\"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract monolingual text\n",
    "corpus_english_sentences = list(myxnli_corpus[\"sentence1_en\"]) + list(myxnli_corpus[\"sentence2_en\"]) + list(alt_combined_corpus[\"English_Sentence\"])\n",
    "corpus_burmese_sentences = list(myxnli_corpus[\"sentence1_my\"]) + list(myxnli_corpus[\"sentence2_my\"]) + list(alt_combined_corpus[\"Burmese_Sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b226036970cd4753a3dd1d02594a3798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be0887774844291a80b20439681975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e4e94d4054912b173e4080a6aa561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dba4266c89402dbc87c54011aa1555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7039e6f0a974c4183cb178cd21a5dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff989a2daf844e889ff8ce18b291be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d7c71191e4e8ca35f994517f0dd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476e21596afc4ec7aca2bb4a436e2004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5375ff462bfa4b989937420a44762b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad17e7c0c9a42909353c7fa79fb35d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pretrained multilingual embedding model\n",
    "minilm_embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "minilm_embedding_model = SentenceTransformer(minilm_embedding_model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for embeddings\n",
    "minilm_corpus_english_embedding_file = \"gen/minilm_corpus_english_embeddings.npy\"\n",
    "minilm_corpus_burmese_embedding_file = \"gen/minilm_corpus_burmese_embeddings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings and checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# Initialize progress\n",
    "if os.path.exists(minilm_corpus_english_embedding_file) and os.path.exists(minilm_corpus_burmese_embedding_file):\n",
    "    print(\"Loading existing embeddings and checkpoint...\")\n",
    "    minilm_corpus_english_embeddings = np.load(minilm_corpus_english_embedding_file)\n",
    "    minilm_corpus_burmese_embeddings = np.load(minilm_corpus_burmese_embedding_file)\n",
    "else:\n",
    "    print(\"Starting fresh embedding computation...\")\n",
    "    # Compute embeddings for English and Burmese sentences\n",
    "    minilm_corpus_english_embeddings = minilm_embedding_model.encode(corpus_english_sentences, convert_to_tensor=False)\n",
    "    minilm_corpus_burmese_embeddings = minilm_embedding_model.encode(corpus_burmese_sentences, convert_to_tensor=False)\n",
    "\n",
    "    # Save the embeddings to files\n",
    "    print(\"Saving embeddings to files...\")\n",
    "    np.save(minilm_corpus_english_embedding_file, minilm_corpus_english_embeddings)\n",
    "    np.save(minilm_corpus_burmese_embedding_file, minilm_corpus_burmese_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Similarity and Filter High-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_threshold = 0.8  # Similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to embeddings\n",
    "minilm_corpus_english_embeddings = torch.tensor(minilm_corpus_english_embeddings).to(device)\n",
    "minilm_corpus_burmese_embeddings = torch.tensor(minilm_corpus_burmese_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing similarity in batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83784ca9b6684b2c99a455f71db836af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-parallel corpus creation completed.\n"
     ]
    }
   ],
   "source": [
    "# Batch size for processing similarity computation\n",
    "minilm_batch_size = 1000\n",
    "minilm_threshold = 0.8\n",
    "\n",
    "minilm_corpus_aligned_pairs = []\n",
    "\n",
    "# Compute similarity in batches\n",
    "print(\"Processing similarity in batches...\")\n",
    "for batch_start in tqdm(range(0, len(minilm_corpus_english_embeddings), minilm_batch_size)):\n",
    "    batch_end = min(batch_start + minilm_batch_size, len(minilm_corpus_english_embeddings))\n",
    "    english_batch = minilm_corpus_english_embeddings[batch_start:batch_end]\n",
    "\n",
    "    # Compute similarity matrix for the batch\n",
    "    similarity_matrix = util.cos_sim(english_batch, minilm_corpus_burmese_embeddings)\n",
    "\n",
    "    # Filter pairs exceeding the threshold\n",
    "    aligned_indices = (similarity_matrix > minilm_threshold).nonzero(as_tuple=True)\n",
    "\n",
    "    # Extract aligned pairs for the current batch\n",
    "    for i, j in zip(*aligned_indices):\n",
    "        minilm_corpus_aligned_pairs.append({\n",
    "            \"english\": corpus_english_sentences[batch_start + i.item()],  # Adjust for batch offset\n",
    "            \"burmese\": corpus_burmese_sentences[j.item()],\n",
    "            \"similarity_score\": similarity_matrix[i, j].item(),\n",
    "        })\n",
    "\n",
    "    # Save progress incrementally after each batch\n",
    "    aligned_df = pd.DataFrame(minilm_corpus_aligned_pairs)\n",
    "    save_gen_df(aligned_df, \"minilm_pseudo_parallel_corpus\")\n",
    "\n",
    "print(\"Pseudo-parallel corpus creation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-transformers/LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained multilingual embedding model\n",
    "labse_embedding_model_name = \"sentence-transformers/LaBSE\"\n",
    "labse_embedding_model = SentenceTransformer(labse_embedding_model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for embeddings\n",
    "labse_corpus_english_embedding_file = \"gen/labse_corpus_english_embeddings.npy\"\n",
    "labse_corpus_burmese_embedding_file = \"gen/labse_corpus_burmese_embeddings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress\n",
    "if os.path.exists(labse_corpus_english_embedding_file) and os.path.exists(labse_corpus_burmese_embedding_file):\n",
    "    print(\"Loading existing embeddings and checkpoint...\")\n",
    "    labse_corpus_english_embeddings = np.load(labse_corpus_english_embedding_file)\n",
    "    labse_corpus_burmese_embeddings = np.load(labse_corpus_burmese_embedding_file)\n",
    "else:\n",
    "    print(\"Starting fresh embedding computation...\")\n",
    "    # Compute embeddings for English and Burmese sentences\n",
    "    labse_corpus_english_embeddings = labse_embedding_model.encode(corpus_english_sentences, convert_to_tensor=False)\n",
    "    labse_corpus_burmese_embeddings = labse_embedding_model.encode(corpus_burmese_sentences, convert_to_tensor=False)\n",
    "\n",
    "    # Save the embeddings to files\n",
    "    print(\"Saving embeddings to files...\")\n",
    "    np.save(labse_corpus_english_embedding_file, labse_corpus_english_embeddings)\n",
    "    np.save(labse_corpus_burmese_embedding_file, labse_corpus_burmese_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Similarity and Filter High-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labse_threshold = 0.8  # Similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to embeddings\n",
    "labse_corpus_english_embeddings = torch.tensor(labse_corpus_english_embeddings).to(device)\n",
    "labse_corpus_burmese_embeddings = torch.tensor(labse_corpus_burmese_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for processing similarity computation\n",
    "labse_batch_size = 1000\n",
    "labse_threshold = 0.8\n",
    "\n",
    "labse_corpus_aligned_pairs = []\n",
    "\n",
    "# Compute similarity in batches\n",
    "print(\"Processing similarity in batches...\")\n",
    "for batch_start in tqdm(range(0, len(labse_corpus_english_embeddings), labse_batch_size)):\n",
    "    batch_end = min(batch_start + labse_batch_size, len(labse_corpus_english_embeddings))\n",
    "    english_batch = labse_corpus_english_embeddings[batch_start:batch_end]\n",
    "\n",
    "    # Compute similarity matrix for the batch\n",
    "    similarity_matrix = util.cos_sim(english_batch, labse_corpus_burmese_embeddings)\n",
    "\n",
    "    # Filter pairs exceeding the threshold\n",
    "    aligned_indices = (similarity_matrix > labse_threshold).nonzero(as_tuple=True)\n",
    "\n",
    "    # Extract aligned pairs for the current batch\n",
    "    for i, j in zip(*aligned_indices):\n",
    "        labse_corpus_aligned_pairs.append({\n",
    "            \"english\": corpus_english_sentences[batch_start + i.item()],  # Adjust for batch offset\n",
    "            \"burmese\": corpus_burmese_sentences[j.item()],\n",
    "            \"similarity_score\": similarity_matrix[i, j].item(),\n",
    "        })\n",
    "\n",
    "    # Save progress incrementally after each batch\n",
    "    aligned_df = pd.DataFrame(labse_corpus_aligned_pairs)\n",
    "    save_gen_df(aligned_df, \"labse_pseudo_parallel_corpus\")\n",
    "\n",
    "print(\"Pseudo-parallel corpus creation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
