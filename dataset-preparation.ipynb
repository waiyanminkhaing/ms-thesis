{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 00:41:33.957233: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-11 00:41:33.968071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736523693.978259    5385 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736523693.981195    5385 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-11 00:41:33.992963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import unicodedata\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import torch\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Loading\n",
    "This step loads the primary datasets:\n",
    "1. `myXNLI.train.tsv`: English-Burmese parallel dataset in TSV format.\n",
    "2. `ALT_data_en.txt` and `ALT_data_my.txt`: English and Burmese parts of the ALT corpus, respectively.\n",
    "\n",
    "The datasets will be loaded into Pandas DataFrames for analysis and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myXNLI dataset loaded successfully with 392702 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load myXNLI dataset\n",
    "myxnli_path = './data/myXNLI.train.tsv'  # Path to the file\n",
    "myxnli_data = pd.read_csv(myxnli_path, sep='\\t', header=0)\n",
    "print(f\"myXNLI dataset loaded successfully with {len(myxnli_data)} records.\")\n",
    "display(myxnli_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT English dataset loaded successfully with 19908 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT English data\n",
    "alt_en_path = './data/ALT_data_en.txt'  # Path to the English ALT corpus\n",
    "alt_en_data = pd.read_csv(alt_en_path, sep='\\t', header=None, names=[\"ID\", \"English_Sentence\"])\n",
    "print(f\"ALT English dataset loaded successfully with {len(alt_en_data)} records.\")\n",
    "display(alt_en_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT Burmese dataset loaded successfully with 19265 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT Burmese data\n",
    "alt_my_path = './data/ALT_data_my.txt'  # Path to the Burmese ALT corpus\n",
    "alt_my_data = pd.read_csv(alt_my_path, sep='\\t', header=None, names=[\"ID\", \"Burmese_Sentence\"])\n",
    "print(f\"ALT Burmese dataset loaded successfully with {len(alt_my_data)} records.\")\n",
    "display(alt_my_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT combined dataset created successfully with 19173 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine ALT datasets (if IDs match)\n",
    "alt_combined = pd.merge(alt_en_data, alt_my_data, on=\"ID\")\n",
    "print(f\"ALT combined dataset created successfully with {len(alt_combined)} records.\")\n",
    "display(alt_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning\n",
    "This step focuses on cleaning the datasets to prepare them for further processing. The cleaning operations include:\n",
    "1. Removing duplicate entries.\n",
    "2. Handling missing values.\n",
    "3. Removing non-standard characters or symbols unrelated to the Burmese or English language.\n",
    "4. Ensuring consistent formatting.\n",
    "\n",
    "The cleaned datasets will be ready for normalization and tokenization in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning myXNLI dataset...\n",
      "myXNLI dataset cleaned successfully.\n",
      "Original Records: 392702.\n",
      "Remaining records: 392682.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning myXNLI dataset\n",
    "print(\"Cleaning myXNLI dataset...\")\n",
    "myxnli_cleaned = myxnli_data.drop_duplicates()  # Remove duplicates\n",
    "myxnli_cleaned = myxnli_cleaned.dropna()  # Remove rows with missing values\n",
    "#myxnli_cleaned = myxnli_cleaned.replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"myXNLI dataset cleaned successfully.\")\n",
    "print(f\"Original Records: {len(myxnli_data)}.\")\n",
    "print(f\"Remaining records: {len(myxnli_cleaned)}.\")\n",
    "display(myxnli_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT English dataset...\n",
      "ALT English dataset cleaned successfully.\n",
      "Original records: 19908.\n",
      "Remaining records: 19908.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT English data\n",
    "print(\"Cleaning ALT English dataset...\")\n",
    "alt_en_cleaned = alt_en_data.drop_duplicates()  # Remove duplicates\n",
    "alt_en_cleaned = alt_en_cleaned.dropna()  # Remove rows with missing values\n",
    "alt_en_cleaned[\"English_Sentence\"] = alt_en_cleaned[\"English_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT English dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_en_data)}.\")\n",
    "print(f\"Remaining records: {len(alt_en_cleaned)}.\")\n",
    "display(alt_en_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT Burmese dataset...\n",
      "ALT Burmese dataset cleaned successfully.\n",
      "Original records: 19265\n",
      "Remaining records: 19258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT Burmese data\n",
    "print(\"Cleaning ALT Burmese dataset...\")\n",
    "alt_my_cleaned = alt_my_data.drop_duplicates()  # Remove duplicates\n",
    "alt_my_cleaned = alt_my_cleaned.dropna()  # Remove rows with missing values\n",
    "#alt_my_cleaned[\"Burmese_Sentence\"] = alt_my_cleaned[\"Burmese_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT Burmese dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_my_data)}\")\n",
    "print(f\"Remaining records: {len(alt_my_cleaned)}\")\n",
    "display(alt_my_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning combined ALT dataset...\n",
      "Combined ALT dataset cleaned successfully.\n",
      "Original records: 19173\n",
      "Remaining records: 19166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine cleaned ALT datasets\n",
    "print(\"Cleaning combined ALT dataset...\")\n",
    "alt_combined_cleaned = pd.merge(alt_en_cleaned, alt_my_cleaned, on=\"ID\")\n",
    "print(f\"Combined ALT dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_combined)}\")\n",
    "print(f\"Remaining records: {len(alt_combined_cleaned)}\")\n",
    "display(alt_combined_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Normalization\n",
    "This step normalizes the text data to ensure consistency across datasets. The normalization process includes:\n",
    "1. Applying Unicode normalization to handle encoding inconsistencies.\n",
    "2. Standardizing text formatting by converting all text to lowercase and standardizing punctuation.\n",
    "3. Normalizing diacritical marks and stacked consonants in the Burmese text to improve text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Apply Unicode normalization\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Convert to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    # Standardize punctuation (e.g., replace unusual punctuation marks)\n",
    "    normalized_text = normalized_text.replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize Burmese text (handles diacritical marks and stacked consonants)\n",
    "def normalize_burmese(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Additional Burmese-specific normalization can be added here if needed\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing myXNLI dataset...\n",
      "myXNLI dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>conceptually cream skimming has two basic dime...</td>\n",
       "      <td>product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>you lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>one of our number will carry out your instruct...</td>\n",
       "      <td>a member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>how do you know? all this is their information...</td>\n",
       "      <td>this information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>the tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  one of our number will carry out your instruct...   \n",
       "3     fiction  entailment  how do you know? all this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  product and geography are what make cream skim...   \n",
       "1  you lose the things to the following level if ...   \n",
       "2  a member of my team will execute your orders w...   \n",
       "3                  this information belongs to them.   \n",
       "4           the tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize myXNLI cleaned dataset\n",
    "print(\"Normalizing myXNLI dataset...\")\n",
    "myxnli_normalized = myxnli_cleaned.copy()\n",
    "\n",
    "# Normalize English columns\n",
    "myxnli_normalized[\"sentence1_en\"] = myxnli_normalized[\"sentence1_en\"].apply(normalize_text)\n",
    "myxnli_normalized[\"sentence2_en\"] = myxnli_normalized[\"sentence2_en\"].apply(normalize_text)\n",
    "\n",
    "# Normalize Burmese columns\n",
    "myxnli_normalized[\"sentence1_my\"] = myxnli_normalized[\"sentence1_my\"].apply(normalize_burmese)\n",
    "myxnli_normalized[\"sentence2_my\"] = myxnli_normalized[\"sentence2_my\"].apply(normalize_burmese)\n",
    "\n",
    "print(f\"myXNLI dataset normalized successfully.\")\n",
    "display(myxnli_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing ALT English dataset...\n",
      "ALT English dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>italy have defeated portugal 315 in pool c of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>andrea masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>portugal never gave up and david penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>italy led 165 at half time but were matched by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  italy have defeated portugal 315 in pool c of ...\n",
       "1  SNT.80188.2  andrea masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  portugal never gave up and david penalva score...\n",
       "4  SNT.80188.5  italy led 165 at half time but were matched by..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize ALT English cleaned dataset\n",
    "print(\"Normalizing ALT English dataset...\")\n",
    "alt_en_normalized = alt_en_cleaned.copy()\n",
    "alt_en_normalized[\"English_Sentence\"] = alt_en_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "print(f\"ALT English dataset normalized successfully.\")\n",
    "display(alt_en_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing ALT Burmese dataset...\n",
      "ALT Burmese dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize ALT Burmese cleaned dataset\n",
    "print(\"Normalizing ALT Burmese dataset...\")\n",
    "alt_my_normalized = alt_my_cleaned.copy()\n",
    "alt_my_normalized[\"Burmese_Sentence\"] = alt_my_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"ALT Burmese dataset normalized successfully.\")\n",
    "display(alt_my_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing combined ALT dataset...\n",
      "Combined ALT dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>italy have defeated portugal 315 in pool c of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>andrea masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>portugal never gave up and david penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  italy have defeated portugal 315 in pool c of ...   \n",
       "1  SNT.80188.2  andrea masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  portugal never gave up and david penalva score...   \n",
       "4  SNT.80188.5  italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize combined ALT cleaned dataset\n",
    "print(\"Normalizing combined ALT dataset...\")\n",
    "alt_combined_normalized = alt_combined_cleaned.copy()\n",
    "alt_combined_normalized[\"English_Sentence\"] = alt_combined_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "alt_combined_normalized[\"Burmese_Sentence\"] = alt_combined_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"Combined ALT dataset normalized successfully.\")\n",
    "display(alt_combined_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sentence Segmentation\n",
    "This step segments text into subword units using SentencePiece Tokenization (SPT). \n",
    "The process includes:\n",
    "1. Training a SentencePiece model using the English and Burmese text from the `myXNLI` dataset and the combined ALT dataset.\n",
    "2. Applying the trained model to segment sentences in both datasets.\n",
    "3. Validating the segmentation results with manual or automated benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths for SentencePiece model\n",
    "sp_model_prefix = \"sentencepiece_model\"\n",
    "sp_train_input = \"combined_texts.txt\"  # A temporary file to hold combined dataset text for training\n",
    "sp_model_path = f\"{sp_model_prefix}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for SentencePiece training...\n",
      "Data prepared in combined_texts.txt.\n"
     ]
    }
   ],
   "source": [
    "# Combine text from myXNLI and ALT datasets for SentencePiece training\n",
    "print(\"Preparing data for SentencePiece training...\")\n",
    "with open(sp_train_input, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Add text from myXNLI dataset\n",
    "    for text in myxnli_normalized[\"sentence1_en\"].tolist() + myxnli_normalized[\"sentence2_en\"].tolist() + myxnli_normalized[\"sentence1_my\"].tolist() + myxnli_normalized[\"sentence2_my\"].tolist():\n",
    "        if pd.notnull(text):  # Avoid writing NaN values\n",
    "            f.write(f\"{text}\\n\")\n",
    "        \n",
    "    # Add text from combined ALT dataset\n",
    "    for text in alt_combined_normalized[\"English_Sentence\"].tolist() + alt_combined_normalized[\"Burmese_Sentence\"].tolist():\n",
    "        if pd.notnull(text):  # Avoid writing NaN values\n",
    "            f.write(f\"{text}\\n\")\n",
    "\n",
    "print(f\"Data prepared in {sp_train_input}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece model...\n",
      "SentencePiece model trained and saved as sentencepiece_model.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: combined_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: sentencepiece_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: combined_texts.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4587 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1610059), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1610059 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 4 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=147732904\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9522% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=134\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999522\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1610059 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=81264687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000134 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1610059\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 1162063\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1162063 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=382836 obj=14.9677 num_tokens=2799142 num_tokens/piece=7.3116\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=246142 obj=11.9897 num_tokens=2812346 num_tokens/piece=11.4257\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=184374 obj=11.9257 num_tokens=2841010 num_tokens/piece=15.409\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=183342 obj=11.9113 num_tokens=2845065 num_tokens/piece=15.5178\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=137292 obj=11.9418 num_tokens=2907699 num_tokens/piece=21.1789\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=137165 obj=11.9336 num_tokens=2908683 num_tokens/piece=21.2057\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=102825 obj=12.0181 num_tokens=3011478 num_tokens/piece=29.2874\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=102791 obj=12 num_tokens=3011538 num_tokens/piece=29.2977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=77083 obj=12.1446 num_tokens=3142631 num_tokens/piece=40.7694\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=77073 obj=12.1117 num_tokens=3142700 num_tokens/piece=40.7756\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=57803 obj=12.3141 num_tokens=3296940 num_tokens/piece=57.0375\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=57798 obj=12.2685 num_tokens=3297115 num_tokens/piece=57.0455\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=43347 obj=12.5298 num_tokens=3469802 num_tokens/piece=80.0471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=43346 obj=12.4705 num_tokens=3469851 num_tokens/piece=80.0501\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=32509 obj=12.7903 num_tokens=3661842 num_tokens/piece=112.641\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=32508 obj=12.7219 num_tokens=3662042 num_tokens/piece=112.65\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=24381 obj=13.0905 num_tokens=3876221 num_tokens/piece=158.985\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=24379 obj=13.0264 num_tokens=3876389 num_tokens/piece=159.005\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=18284 obj=13.4509 num_tokens=4121127 num_tokens/piece=225.395\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=18284 obj=13.3708 num_tokens=4121549 num_tokens/piece=225.418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13712 obj=13.8659 num_tokens=4403509 num_tokens/piece=321.143\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=13712 obj=13.7739 num_tokens=4407736 num_tokens/piece=321.451\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10284 obj=14.334 num_tokens=4717305 num_tokens/piece=458.703\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10284 obj=14.2218 num_tokens=4718186 num_tokens/piece=458.789\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=14.5446 num_tokens=4889599 num_tokens/piece=555.636\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=14.5 num_tokens=4890814 num_tokens/piece=555.774\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: sentencepiece_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: sentencepiece_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train SentencePiece model\n",
    "print(\"Training SentencePiece model...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=sp_train_input,\n",
    "    model_prefix=sp_model_prefix,\n",
    "    vocab_size=8000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type=\"unigram\"  # Use unigram language model\n",
    ")\n",
    "print(f\"SentencePiece model trained and saved as {sp_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model loaded from sentencepiece_model.model.\n"
     ]
    }
   ],
   "source": [
    "# Load trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "print(f\"SentencePiece model loaded from {sp_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SentencePiece Tokenization\n",
    "def apply_sentencepiece(data, column_name):\n",
    "    return data[column_name].apply(lambda x: \" \".join(sp.encode_as_pieces(x)) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SentencePiece tokenization to myXNLI dataset...\n",
      "SentencePiece tokenization applied to myXNLI dataset successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>▁concept ual ly ▁cre am ▁ski m ming ▁has ▁two ...</td>\n",
       "      <td>▁product ▁and ▁ ge ography ▁are ▁what ▁make ▁c...</td>\n",
       "      <td>▁ သဘော တရား အရ ▁ ခ ရင် မ် စ ိ မ် ခြင်း တွင် ▁အ...</td>\n",
       "      <td>▁ထုတ်ကုန် နှင့် ▁ပ ထ ဝီ ဝင် အနေအထား သည် ▁ ခ ရင...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁you ▁know ▁during ▁the ▁season ▁and ▁i ▁guess...</td>\n",
       "      <td>▁you ▁lose ▁the ▁things ▁to ▁the ▁following ▁l...</td>\n",
       "      <td>▁ ရာသီ အတွင်း ▁မင်းသိတယ် ၊ ▁မင်းရဲ့ အဆင့် ကို ...</td>\n",
       "      <td>▁လူ တွေ ပြန် ခေါ် ရင် ▁အောက် ပါ အဆင့် အထိ ▁ဆုံ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁one ▁of ▁our ▁number ▁will ▁carry ▁out ▁your ...</td>\n",
       "      <td>▁a ▁member ▁of ▁my ▁team ▁will ▁execut e ▁your...</td>\n",
       "      <td>▁ကျွန်ုပ်တို့၏ နံပါတ် တစ်ခုသည် ▁သင့် ညွှန ် ကြ...</td>\n",
       "      <td>▁ကျွန်ုပ်၏ အဖွဲ့ သား တစ်ဦးသည် ▁သင်၏ အမိန့် စာ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁how ▁do ▁you ▁know ? ▁all ▁this ▁is ▁their ▁i...</td>\n",
       "      <td>▁this ▁information ▁be lo ng s ▁to ▁them .</td>\n",
       "      <td>▁သင် ဘယ်လို သိ သ လဲ ? ▁ဒါ တွေ အားလုံး ဟာ ▁သူတိ...</td>\n",
       "      <td>▁ဒီ အချက်အလက် က ▁သူတို့ ပိုင် တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>▁yeah ▁i ▁tell ▁you ▁what ▁though ▁if ▁you ▁go...</td>\n",
       "      <td>▁the ▁ten ni s ▁ s ho es ▁have ▁a ▁range ▁of ▁...</td>\n",
       "      <td>▁ဟုတ်တယ် ▁ငါမင်းကို ပြောပြ မယ် ▁ဒီ တင်း နစ် ဖိ...</td>\n",
       "      <td>▁ တင်း နစ် ဖိနပ် များသည် ▁ ဈ ေး နှုန်း အမျိုးမ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  ▁concept ual ly ▁cre am ▁ski m ming ▁has ▁two ...   \n",
       "1   telephone  entailment  ▁you ▁know ▁during ▁the ▁season ▁and ▁i ▁guess...   \n",
       "2     fiction  entailment  ▁one ▁of ▁our ▁number ▁will ▁carry ▁out ▁your ...   \n",
       "3     fiction  entailment  ▁how ▁do ▁you ▁know ? ▁all ▁this ▁is ▁their ▁i...   \n",
       "4   telephone     neutral  ▁yeah ▁i ▁tell ▁you ▁what ▁though ▁if ▁you ▁go...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  ▁product ▁and ▁ ge ography ▁are ▁what ▁make ▁c...   \n",
       "1  ▁you ▁lose ▁the ▁things ▁to ▁the ▁following ▁l...   \n",
       "2  ▁a ▁member ▁of ▁my ▁team ▁will ▁execut e ▁your...   \n",
       "3         ▁this ▁information ▁be lo ng s ▁to ▁them .   \n",
       "4  ▁the ▁ten ni s ▁ s ho es ▁have ▁a ▁range ▁of ▁...   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  ▁ သဘော တရား အရ ▁ ခ ရင် မ် စ ိ မ် ခြင်း တွင် ▁အ...   \n",
       "1  ▁ ရာသီ အတွင်း ▁မင်းသိတယ် ၊ ▁မင်းရဲ့ အဆင့် ကို ...   \n",
       "2  ▁ကျွန်ုပ်တို့၏ နံပါတ် တစ်ခုသည် ▁သင့် ညွှန ် ကြ...   \n",
       "3  ▁သင် ဘယ်လို သိ သ လဲ ? ▁ဒါ တွေ အားလုံး ဟာ ▁သူတိ...   \n",
       "4  ▁ဟုတ်တယ် ▁ငါမင်းကို ပြောပြ မယ် ▁ဒီ တင်း နစ် ဖိ...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ▁ထုတ်ကုန် နှင့် ▁ပ ထ ဝီ ဝင် အနေအထား သည် ▁ ခ ရင...  \n",
       "1  ▁လူ တွေ ပြန် ခေါ် ရင် ▁အောက် ပါ အဆင့် အထိ ▁ဆုံ...  \n",
       "2  ▁ကျွန်ုပ်၏ အဖွဲ့ သား တစ်ဦးသည် ▁သင်၏ အမိန့် စာ ...  \n",
       "3                 ▁ဒီ အချက်အလက် က ▁သူတို့ ပိုင် တယ်။  \n",
       "4  ▁ တင်း နစ် ဖိနပ် များသည် ▁ ဈ ေး နှုန်း အမျိုးမ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply SentencePiece Tokenization to myXNLI dataset\n",
    "print(\"Applying SentencePiece tokenization to myXNLI dataset...\")\n",
    "myxnli_segmented = myxnli_normalized.copy()\n",
    "myxnli_segmented[\"sentence1_en\"] = apply_sentencepiece(myxnli_segmented, \"sentence1_en\")\n",
    "myxnli_segmented[\"sentence2_en\"] = apply_sentencepiece(myxnli_segmented, \"sentence2_en\")\n",
    "myxnli_segmented[\"sentence1_my\"] = apply_sentencepiece(myxnli_segmented, \"sentence1_my\")\n",
    "myxnli_segmented[\"sentence2_my\"] = apply_sentencepiece(myxnli_segmented, \"sentence2_my\")\n",
    "print(\"SentencePiece tokenization applied to myXNLI dataset successfully.\")\n",
    "display(myxnli_segmented.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SentencePiece tokenization to combined ALT dataset...\n",
      "SentencePiece tokenization applied to combined ALT dataset successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>▁italy ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in ...</td>\n",
       "      <td>▁ပြင်သစ် နိုင်ငံ ▁ပါရီ မြို့ ▁ပါ ့ ဒ က်စ် ▁ပ ရ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>▁and re a ▁ma s i ▁open ed ▁the ▁sc or ing ▁in...</td>\n",
       "      <td>▁အ န် ဒ ရီ ယာ ▁ မာ စီ ▁သည် ▁အီတလီ ▁အတွက် ▁ စမ်...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>▁de spite ▁control ling ▁the ▁game ▁for ▁much ...</td>\n",
       "      <td>▁ပထမ ▁ တစ်ဝက် ▁၏ ▁ တော်တော်များများ ▁အတွက် ▁ကစ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>▁port u gal ▁never ▁gave ▁up ▁and ▁david ▁pen ...</td>\n",
       "      <td>▁ပေါ်တူဂီ ▁သည် ▁ဘယ်သောအခါမှ ▁စွန့်လွှတ် မှု ▁မ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>▁italy ▁ led ▁16 5 ▁at ▁half ▁time ▁but ▁were ...</td>\n",
       "      <td>▁အီတလီ ▁သည် ▁ပထမ ပိုင်း ▁ ၌ ▁၁ ၆ - ၅ ▁ ဖြင့် ▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  ▁italy ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in ...   \n",
       "1  SNT.80188.2  ▁and re a ▁ma s i ▁open ed ▁the ▁sc or ing ▁in...   \n",
       "2  SNT.80188.3  ▁de spite ▁control ling ▁the ▁game ▁for ▁much ...   \n",
       "3  SNT.80188.4  ▁port u gal ▁never ▁gave ▁up ▁and ▁david ▁pen ...   \n",
       "4  SNT.80188.5  ▁italy ▁ led ▁16 5 ▁at ▁half ▁time ▁but ▁were ...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ▁ပြင်သစ် နိုင်ငံ ▁ပါရီ မြို့ ▁ပါ ့ ဒ က်စ် ▁ပ ရ...  \n",
       "1  ▁အ န် ဒ ရီ ယာ ▁ မာ စီ ▁သည် ▁အီတလီ ▁အတွက် ▁ စမ်...  \n",
       "2  ▁ပထမ ▁ တစ်ဝက် ▁၏ ▁ တော်တော်များများ ▁အတွက် ▁ကစ...  \n",
       "3  ▁ပေါ်တူဂီ ▁သည် ▁ဘယ်သောအခါမှ ▁စွန့်လွှတ် မှု ▁မ...  \n",
       "4  ▁အီတလီ ▁သည် ▁ပထမ ပိုင်း ▁ ၌ ▁၁ ၆ - ၅ ▁ ဖြင့် ▁...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply SentencePiece Tokenization to ALT English and Burmese datasets\n",
    "print(\"Applying SentencePiece tokenization to combined ALT dataset...\")\n",
    "alt_combined_segmented = alt_combined_normalized.copy()\n",
    "alt_combined_segmented[\"English_Sentence\"] = apply_sentencepiece(alt_combined_segmented, \"English_Sentence\")\n",
    "alt_combined_segmented[\"Burmese_Sentence\"] = apply_sentencepiece(alt_combined_segmented, \"Burmese_Sentence\")\n",
    "print(\"SentencePiece tokenization applied to combined ALT dataset successfully.\")\n",
    "display(alt_combined_segmented.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Morphological Processing\n",
    "This step involves advanced processing to capture morphological nuances in the text. The operations include:\n",
    "1. Segmenting words into morphemes, handling prefixes, suffixes, and compound words.\n",
    "2. Normalizing compounded forms while preserving semantic meanings.\n",
    "3. Incorporating loanwords for better representation in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment words into morphemes\n",
    "def segment_morphemes(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Example: Handle prefixes, suffixes, and compounds\n",
    "    # For demonstration, splitting by common Burmese and English morphemes\n",
    "    segmented_text = re.sub(r'(\\bpre|un|re|in|dis|mis|non)(\\w+)', r'\\1-\\2', text)  # English prefixes\n",
    "    segmented_text = re.sub(r'(\\w+)(ing|ly|ed|er|ion|able|ible|ment|ness|ship|ous|ive|ish|ize)\\b', r'\\1-\\2', segmented_text)  # English suffixes\n",
    "    # Add custom Burmese rules here for morpheme segmentation\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize compounded forms\n",
    "def normalize_compounds(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Example: Handle English hyphenated compounds (adjust as needed for Burmese)\n",
    "    normalized_text = re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to incorporate loanwords\n",
    "def incorporate_loanwords(text, loanword_dict):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Replace loanwords based on a predefined dictionary\n",
    "    for loanword, replacement in loanword_dict.items():\n",
    "        text = re.sub(rf'\\b{loanword}\\b', replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample loanword dictionary for Burmese\n",
    "loanword_dict = {\n",
    "    \"ဘဏ်\": \"bank\",  # Example: Burmese word for 'bank'\n",
    "    \"အင်တာနက်\": \"internet\",  # Example: Burmese word for 'internet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing myXNLI dataset...\n",
      "Morphological processing applied to myXNLI dataset successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>▁concept ual ly ▁cre am ▁ski m min g ▁has ▁two...</td>\n",
       "      <td>▁product ▁and ▁ ge ography ▁are ▁what ▁make ▁c...</td>\n",
       "      <td>▁ သဘော တရား အရ ▁ ခ ရင် မ် စ ိ မ် ခြင်း တွင် ▁အ...</td>\n",
       "      <td>▁ထုတ်ကုန် နှင့် ▁ပ ထ ဝီ ဝင် အနေအထား သည် ▁ ခ ရင...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁you ▁know ▁durin g ▁the ▁season ▁and ▁i ▁gues...</td>\n",
       "      <td>▁you ▁lose ▁the ▁thin gs ▁to ▁the ▁followin g ...</td>\n",
       "      <td>▁ ရာသီ အတွင်း ▁မင်းသိတယ် ၊ ▁မင်းရဲ့ အဆင့် ကို ...</td>\n",
       "      <td>▁လူ တွေ ပြန် ခေါ် ရင် ▁အောက် ပါ အဆင့် အထိ ▁ဆုံ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁one ▁of ▁our ▁numb er ▁will ▁carry ▁out ▁your...</td>\n",
       "      <td>▁a ▁memb er ▁of ▁my ▁team ▁will ▁execut e ▁you...</td>\n",
       "      <td>▁ကျွန်ုပ်တို့၏ နံပါတ် တစ်ခုသည် ▁သင့် ညွှန ် ကြ...</td>\n",
       "      <td>▁ကျွန်ုပ်၏ အဖွဲ့ သား တစ်ဦးသည် ▁သင်၏ အမိန့် စာ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>▁how ▁do ▁you ▁know ? ▁all ▁this ▁is ▁their ▁i...</td>\n",
       "      <td>▁this ▁in format-ion ▁be lo ng s ▁to ▁them .</td>\n",
       "      <td>▁သင် ဘယ်လို သိ သ လဲ ? ▁ဒါ တွေ အားလုံး ဟာ ▁သူတိ...</td>\n",
       "      <td>▁ဒီ အချက်အလက် က ▁သူတို့ ပိုင် တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>▁yeah ▁i ▁tell ▁you ▁what ▁though ▁if ▁you ▁go...</td>\n",
       "      <td>▁the ▁ten ni s ▁ s ho es ▁have ▁a ▁range ▁of ▁...</td>\n",
       "      <td>▁ဟုတ်တယ် ▁ငါမင်းကို ပြောပြ မယ် ▁ဒီ တင်း နစ် ဖိ...</td>\n",
       "      <td>▁ တင်း နစ် ဖိနပ် များသည် ▁ ဈ ေး နှုန်း အမျိုးမ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  ▁concept ual ly ▁cre am ▁ski m min g ▁has ▁two...   \n",
       "1   telephone  entailment  ▁you ▁know ▁durin g ▁the ▁season ▁and ▁i ▁gues...   \n",
       "2     fiction  entailment  ▁one ▁of ▁our ▁numb er ▁will ▁carry ▁out ▁your...   \n",
       "3     fiction  entailment  ▁how ▁do ▁you ▁know ? ▁all ▁this ▁is ▁their ▁i...   \n",
       "4   telephone     neutral  ▁yeah ▁i ▁tell ▁you ▁what ▁though ▁if ▁you ▁go...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  ▁product ▁and ▁ ge ography ▁are ▁what ▁make ▁c...   \n",
       "1  ▁you ▁lose ▁the ▁thin gs ▁to ▁the ▁followin g ...   \n",
       "2  ▁a ▁memb er ▁of ▁my ▁team ▁will ▁execut e ▁you...   \n",
       "3       ▁this ▁in format-ion ▁be lo ng s ▁to ▁them .   \n",
       "4  ▁the ▁ten ni s ▁ s ho es ▁have ▁a ▁range ▁of ▁...   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  ▁ သဘော တရား အရ ▁ ခ ရင် မ် စ ိ မ် ခြင်း တွင် ▁အ...   \n",
       "1  ▁ ရာသီ အတွင်း ▁မင်းသိတယ် ၊ ▁မင်းရဲ့ အဆင့် ကို ...   \n",
       "2  ▁ကျွန်ုပ်တို့၏ နံပါတ် တစ်ခုသည် ▁သင့် ညွှန ် ကြ...   \n",
       "3  ▁သင် ဘယ်လို သိ သ လဲ ? ▁ဒါ တွေ အားလုံး ဟာ ▁သူတိ...   \n",
       "4  ▁ဟုတ်တယ် ▁ငါမင်းကို ပြောပြ မယ် ▁ဒီ တင်း နစ် ဖိ...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ▁ထုတ်ကုန် နှင့် ▁ပ ထ ဝီ ဝင် အနေအထား သည် ▁ ခ ရင...  \n",
       "1  ▁လူ တွေ ပြန် ခေါ် ရင် ▁အောက် ပါ အဆင့် အထိ ▁ဆုံ...  \n",
       "2  ▁ကျွန်ုပ်၏ အဖွဲ့ သား တစ်ဦးသည် ▁သင်၏ အမိန့် စာ ...  \n",
       "3                 ▁ဒီ အချက်အလက် က ▁သူတို့ ပိုင် တယ်။  \n",
       "4  ▁ တင်း နစ် ဖိနပ် များသည် ▁ ဈ ေး နှုန်း အမျိုးမ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply morphological processing to myXNLI dataset\n",
    "print(\"Processing myXNLI dataset...\")\n",
    "myxnli_processed = myxnli_segmented.copy()\n",
    "# Apply morpheme segmentation\n",
    "myxnli_processed[\"sentence1_en\"] = myxnli_processed[\"sentence1_en\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence2_en\"] = myxnli_processed[\"sentence2_en\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(segment_morphemes)\n",
    "# Normalize compounded forms\n",
    "myxnli_processed[\"sentence1_en\"] = myxnli_processed[\"sentence1_en\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence2_en\"] = myxnli_processed[\"sentence2_en\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(normalize_compounds)\n",
    "# Incorporate loanwords\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "    \n",
    "print(\"Morphological processing applied to myXNLI dataset successfully.\")\n",
    "display(myxnli_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combined ALT dataset...\n",
      "Morphological processing applied to combined ALT dataset successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>▁ita ly ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in...</td>\n",
       "      <td>▁ပြင်သစ် နိုင်ငံ ▁ပါရီ မြို့ ▁ပါ ့ ဒ က်စ် ▁ပ ရ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>▁and re a ▁ma s i ▁open ed ▁the ▁sc or in g ▁i...</td>\n",
       "      <td>▁အ န် ဒ ရီ ယာ ▁ မာ စီ ▁သည် ▁အီတလီ ▁အတွက် ▁ စမ်...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>▁de spite ▁control lin g ▁the ▁game ▁for ▁much...</td>\n",
       "      <td>▁ပထမ ▁ တစ်ဝက် ▁၏ ▁ တော်တော်များများ ▁အတွက် ▁ကစ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>▁port u gal ▁nev er ▁gave ▁up ▁and ▁david ▁pen...</td>\n",
       "      <td>▁ပေါ်တူဂီ ▁သည် ▁ဘယ်သောအခါမှ ▁စွန့်လွှတ် မှု ▁မ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>▁ita ly ▁ l ed ▁16 5 ▁at ▁half ▁time ▁but ▁wer...</td>\n",
       "      <td>▁အီတလီ ▁သည် ▁ပထမ ပိုင်း ▁ ၌ ▁၁ ၆ - ၅ ▁ ဖြင့် ▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  ▁ita ly ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in...   \n",
       "1  SNT.80188.2  ▁and re a ▁ma s i ▁open ed ▁the ▁sc or in g ▁i...   \n",
       "2  SNT.80188.3  ▁de spite ▁control lin g ▁the ▁game ▁for ▁much...   \n",
       "3  SNT.80188.4  ▁port u gal ▁nev er ▁gave ▁up ▁and ▁david ▁pen...   \n",
       "4  SNT.80188.5  ▁ita ly ▁ l ed ▁16 5 ▁at ▁half ▁time ▁but ▁wer...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ▁ပြင်သစ် နိုင်ငံ ▁ပါရီ မြို့ ▁ပါ ့ ဒ က်စ် ▁ပ ရ...  \n",
       "1  ▁အ န် ဒ ရီ ယာ ▁ မာ စီ ▁သည် ▁အီတလီ ▁အတွက် ▁ စမ်...  \n",
       "2  ▁ပထမ ▁ တစ်ဝက် ▁၏ ▁ တော်တော်များများ ▁အတွက် ▁ကစ...  \n",
       "3  ▁ပေါ်တူဂီ ▁သည် ▁ဘယ်သောအခါမှ ▁စွန့်လွှတ် မှု ▁မ...  \n",
       "4  ▁အီတလီ ▁သည် ▁ပထမ ပိုင်း ▁ ၌ ▁၁ ၆ - ၅ ▁ ဖြင့် ▁...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply morphological processing to combined ALT dataset\n",
    "print(\"Processing combined ALT dataset...\")\n",
    "alt_combined_processed = alt_combined_segmented.copy()\n",
    "# Apply morpheme segmentation\n",
    "alt_combined_processed[\"English_Sentence\"] = alt_combined_processed[\"English_Sentence\"].apply(segment_morphemes)\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(segment_morphemes)\n",
    "# Normalize compounded forms\n",
    "alt_combined_processed[\"English_Sentence\"] = alt_combined_processed[\"English_Sentence\"].apply(normalize_compounds)\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(normalize_compounds)\n",
    "# Incorporate loanwords\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "\n",
    "print(\"Morphological processing applied to combined ALT dataset successfully.\")\n",
    "display(alt_combined_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Data Augmentation\n",
    "This step enhances the dataset by generating additional data using the following methods:\n",
    "1. **Back-Translation**:\n",
    "    - Translate Burmese sentences to English and back to Burmese using `facebook/m2m100_418M` and `facebook/mbart-large-50` to create diverse translations while preserving semantic meaning for both `myXNLI` and `combined ALT dataset`.\n",
    "2. **Pseudo-Parallel Corpus Creation**:\n",
    "    - Use semantic similarity alignment to identify and align semantically similar sentences from monolingual data to generate pseudo-parallel corpora for the `combined ALT dataset` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create temp df\n",
    "def create_temp_df():\n",
    "    return pd.DataFrame(columns=[\"isNull\", \"original\", \"translated\", \"back_translated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add row to temp df\n",
    "def add_row_to_temp_df(df, row):\n",
    "    # Convert the row dictionary to a DataFrame\n",
    "    row_df = pd.DataFrame([row])\n",
    "\n",
    "    # Use pd.concat to add the row\n",
    "    updated_df = pd.concat([df, row_df], ignore_index=True)\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save and display temp df\n",
    "def save_display_temp_df(temp_df, tmp_df_name):\n",
    "    temp_df.to_csv(f\"{tmp_df_name}.csv\", index=False)\n",
    "    display(temp_df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back-Translation (facebook/m2m100_418M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load M2M100 model and tokenizer\n",
    "m2m_model_name = \"facebook/m2m100_418M\"\n",
    "m2m_translation_model = M2M100ForConditionalGeneration.from_pretrained(m2m_model_name).to(device)\n",
    "m2m_translation_tokenizer = M2M100Tokenizer.from_pretrained(m2m_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using M2M100\n",
    "def m2m_back_translate(text, src_lang, tgt_lang, df, df_name):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        df.append({\"isNull\": True})\n",
    "        save_display_temp_df(df, df_name)\n",
    "        return text  # Skip null values\n",
    "    # Translate to the target language\n",
    "    m2m_translation_tokenizer.src_lang = src_lang\n",
    "    encoded = m2m_translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    translated = m2m_translation_model.generate(**encoded)\n",
    "    translated_text = m2m_translation_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Translate back to the source language\n",
    "    m2m_translation_tokenizer.src_lang = tgt_lang\n",
    "    encoded_back = m2m_translation_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    back_translated = m2m_translation_model.generate(**encoded_back)\n",
    "    back_translated_text = m2m_translation_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "    \n",
    "    new_row = {\"isNull\": False, \"original\": text, \"translated\": translated_text, \"back_translated\": back_translated_text}\n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "    save_display_temp_df(df, df_name)\n",
    "\n",
    "    return back_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences in myXNLI\n",
    "myxnli_m2m_back_translated = myxnli_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 1 in myXNLI\n",
    "myxnli_m2m_back_translated_temp_df1 = create_temp_df()\n",
    "myxnli_m2m_back_translated[\"sentence1_my\"] = myxnli_m2m_back_translated[\"sentence1_my\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\", df=myxnli_m2m_back_translated_temp_df1, df_name='myxnli_m2m_back_translated_temp_df1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 2 in myXNLI\n",
    "myxnli_m2m_back_translated_temp_df2 = create_temp_df()\n",
    "myxnli_m2m_back_translated[\"sentence2_my\"] = myxnli_m2m_back_translated[\"sentence2_my\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\", df=myxnli_m2m_back_translated_temp_df2, df_name='myxnli_m2m_back_translated_temp_df2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display and save back-translated myXNLI dataset\n",
    "print(\"Back-translation applied to myXNLI dataset with m2m100.\")\n",
    "display(myxnli_m2m_back_translated.head())\n",
    "myxnli_m2m_back_translated.to_csv('myxnli_m2m_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to combined ALT dataset\n",
    "print(\"Applying back-translation to combined ALT dataset with m2m100...\")\n",
    "alt_m2m_back_translated = alt_combined_processed.copy()\n",
    "alt_m2m_back_translated[\"Burmese_Sentence\"] = alt_m2m_back_translated[\"Burmese_Sentence\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\") if pd.notnull(x) else x\n",
    ")\n",
    "print(\"Back-translation applied to combined ALT dataset with m2m100.\")\n",
    "display(alt_m2m_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back-translated alt dataset\n",
    "alt_m2m_back_translated.to_csv('alt_m2m_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back-Translation (facebook/mbart-large-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mBART-50 model and tokenizer\n",
    "mbart_model_name = \"facebook/mbart-large-50\"\n",
    "mbart_translation_model = MBartForConditionalGeneration.from_pretrained(mbart_model_name).to(device)\n",
    "mbart_translation_tokenizer = MBart50TokenizerFast.from_pretrained(mbart_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using mBART-50\n",
    "def mbart_back_translate(text, src_lang, tgt_lang, df, df_name):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        df.append({\"isNull\": True})\n",
    "        save_display_temp_df(df, df_name)\n",
    "        return text  # Skip null values\n",
    "\n",
    "    # Translate to the target language\n",
    "    mbart_translation_tokenizer.src_lang = src_lang\n",
    "    encoded = mbart_translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    translated = mbart_translation_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=mbart_translation_tokenizer.lang_code_to_id[tgt_lang]\n",
    "    )\n",
    "    translated_text = mbart_translation_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Translate back to the source language\n",
    "    mbart_translation_tokenizer.src_lang = tgt_lang\n",
    "    encoded_back = mbart_translation_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    back_translated = mbart_translation_model.generate(\n",
    "        **encoded_back,\n",
    "        forced_bos_token_id=mbart_translation_tokenizer.lang_code_to_id[src_lang]\n",
    "    )\n",
    "    back_translated_text = mbart_translation_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    new_row = {\"isNull\": False, \"original\": text, \"translated\": translated_text, \"back_translated\": back_translated_text}\n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "    save_display_temp_df(df, df_name)\n",
    "\n",
    "    return back_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences in myXNLI\n",
    "myxnli_mbart_back_translated = myxnli_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isNull</th>\n",
       "      <th>original</th>\n",
       "      <th>translated</th>\n",
       "      <th>back_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>False</td>\n",
       "      <td>▁ဆေး ဖက် ဝင် အ ပင် များ ▁ ထုတ် ယူ မှုသည် ▁အ ပျ...</td>\n",
       "      <td>ဆေး ဖက် ဝင် အ ပင် များ ထုတ် ယူ မှုသည် အ ပျော့ ...</td>\n",
       "      <td>ဆေး ဖက် ဝင် အ ပင် များ ထုတ် ယူ မှုသည် အ ပျော့ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     isNull                                           original  \\\n",
       "120   False  ▁ဆေး ဖက် ဝင် အ ပင် များ ▁ ထုတ် ယူ မှုသည် ▁အ ပျ...   \n",
       "\n",
       "                                            translated  \\\n",
       "120  ဆေး ဖက် ဝင် အ ပင် များ ထုတ် ယူ မှုသည် အ ပျော့ ...   \n",
       "\n",
       "                                       back_translated  \n",
       "120  ဆေး ဖက် ဝင် အ ပင် များ ထုတ် ယူ မှုသည် အ ပျော့ ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the Burmese sentences 1 in myXNLI\n",
    "myxnli_mbart_back_translated_temp_df1 = create_temp_df()\n",
    "myxnli_mbart_back_translated[\"sentence1_my\"] = myxnli_mbart_back_translated[\"sentence1_my\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=myxnli_mbart_back_translated_temp_df1, df_name=\"myxnli_mbart_back_translated_temp_df1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 2 in myXNLI\n",
    "myxnli_mbart_back_translated_temp_df2 = create_temp_df()\n",
    "myxnli_mbart_back_translated[\"sentence2_my\"] = myxnli_mbart_back_translated[\"sentence2_my\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=myxnli_mbart_back_translated_temp_df2, df_name=\"myxnli_mbart_back_translated_temp_df2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display and save back-translated myXNLI dataset\n",
    "print(\"Back-translation applied to myXNLI dataset with mBART-50.\")\n",
    "display(myxnli_mbart_back_translated.head())\n",
    "myxnli_mbart_back_translated.to_csv('myxnli_mbart_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to combined ALT dataset\n",
    "alt_mbart_back_translated_temp_df = create_temp_df()\n",
    "alt_mbart_back_translated = alt_combined_processed.copy()\n",
    "alt_mbart_back_translated[\"Burmese_Sentence\"] = alt_mbart_back_translated[\"Burmese_Sentence\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=alt_mbart_back_translated_temp_df, df_name=\"alt_mbart_back_translated_temp_df\")\n",
    ")\n",
    "print(\"Back-translation applied to combined ALT dataset with mBART-50.\")\n",
    "display(alt_mbart_back_translated.head())\n",
    "alt_mbart_back_translated.to_csv('alt_mbart_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pseudo-Parallel Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic similarity model\n",
    "similarity_model_name = \"all-MiniLM-L6-v2\"\n",
    "similarity_model = SentenceTransformer(similarity_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create pseudo-parallel corpus\n",
    "def create_pseudo_parallel(data_en, data_my, similarity_model, top_k=1):\n",
    "    pseudo_parallel = []\n",
    "    embeddings_en = similarity_model.encode(data_en, convert_to_tensor=True, device=device)\n",
    "    embeddings_my = similarity_model.encode(data_my, convert_to_tensor=True, device=device)\n",
    "    similarity_scores = util.pytorch_cos_sim(embeddings_en, embeddings_my)\n",
    "\n",
    "    for idx_en, scores in enumerate(similarity_scores):\n",
    "        top_matches = scores.topk(k=top_k)\n",
    "        for match_idx in top_matches.indices:\n",
    "            pseudo_parallel.append((data_en[idx_en], data_my[match_idx.item()], scores[match_idx].item()))\n",
    "    \n",
    "    return pseudo_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pseudo-parallel corpus from combined ALT dataset...\n",
      "Pseudo-parallel corpus created successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁ita ly ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in...</td>\n",
       "      <td>▁ ရီ ဂ ႀ င်း နင်း ▁ ဆန်း ရှ ိုင်း ▁ကို ႕ ▁စ ကူ...</td>\n",
       "      <td>0.766668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁and re a ▁ma s i ▁open ed ▁the ▁sc or in g ▁i...</td>\n",
       "      <td>▁သူတို့ ▁သည် ▁I C A O ▁၊ ▁A T S B ▁နှင့် ▁ “ ▁...</td>\n",
       "      <td>0.645176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁de spite ▁control lin g ▁the ▁game ▁for ▁much...</td>\n",
       "      <td>▁ဘ န် ကောက် မြို့ ▁၏ ▁P ra we t ▁ ခရိုင် ▁ ဆင်...</td>\n",
       "      <td>0.647617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁port u gal ▁nev er ▁gave ▁up ▁and ▁david ▁pen...</td>\n",
       "      <td>▁I H S ▁G lo b al ▁In s ight ▁၏ ▁ဆန်းစစ် လေ့လာ...</td>\n",
       "      <td>0.709018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁ita ly ▁ l ed ▁16 5 ▁at ▁half ▁time ▁but ▁wer...</td>\n",
       "      <td>▁ယင်း သည် ▁200 5 ခု နှစ် ▁တွင် ▁ပြုလုပ်ခဲ့ သော...</td>\n",
       "      <td>0.655455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    English_Sentence  \\\n",
       "0  ▁ita ly ▁have ▁defeat ed ▁port u gal ▁3 15 ▁in...   \n",
       "1  ▁and re a ▁ma s i ▁open ed ▁the ▁sc or in g ▁i...   \n",
       "2  ▁de spite ▁control lin g ▁the ▁game ▁for ▁much...   \n",
       "3  ▁port u gal ▁nev er ▁gave ▁up ▁and ▁david ▁pen...   \n",
       "4  ▁ita ly ▁ l ed ▁16 5 ▁at ▁half ▁time ▁but ▁wer...   \n",
       "\n",
       "                                    Burmese_Sentence  Similarity_Score  \n",
       "0  ▁ ရီ ဂ ႀ င်း နင်း ▁ ဆန်း ရှ ိုင်း ▁ကို ႕ ▁စ ကူ...          0.766668  \n",
       "1  ▁သူတို့ ▁သည် ▁I C A O ▁၊ ▁A T S B ▁နှင့် ▁ “ ▁...          0.645176  \n",
       "2  ▁ဘ န် ကောက် မြို့ ▁၏ ▁P ra we t ▁ ခရိုင် ▁ ဆင်...          0.647617  \n",
       "3  ▁I H S ▁G lo b al ▁In s ight ▁၏ ▁ဆန်းစစ် လေ့လာ...          0.709018  \n",
       "4  ▁ယင်း သည် ▁200 5 ခု နှစ် ▁တွင် ▁ပြုလုပ်ခဲ့ သော...          0.655455  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply pseudo-parallel creation to combined ALT dataset\n",
    "print(\"Creating pseudo-parallel corpus from combined ALT dataset...\")\n",
    "alt_combined_en = alt_combined_processed[\"English_Sentence\"].dropna().tolist()\n",
    "alt_combined_my = alt_combined_processed[\"Burmese_Sentence\"].dropna().tolist()\n",
    "pseudo_parallel_data = create_pseudo_parallel(alt_combined_en, alt_combined_my, similarity_model)\n",
    "    \n",
    "pseudo_parallel_df = pd.DataFrame(pseudo_parallel_data, columns=[\"English_Sentence\", \"Burmese_Sentence\", \"Similarity_Score\"])\n",
    "print(\"Pseudo-parallel corpus created successfully.\")\n",
    "display(pseudo_parallel_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pseudo-parallel alt dataset\n",
    "pseudo_parallel_df.to_csv('pseudo_parallel_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
