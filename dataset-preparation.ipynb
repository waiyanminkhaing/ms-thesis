{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import unicodedata\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import torch\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Loading\n",
    "This step loads the primary datasets:\n",
    "1. `myXNLI.train.tsv`: English-Burmese parallel dataset in TSV format.\n",
    "2. `ALT_data_en.txt` and `ALT_data_my.txt`: English and Burmese parts of the ALT corpus, respectively.\n",
    "\n",
    "The datasets will be loaded into Pandas DataFrames for analysis and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load myXNLI dataset\n",
    "myxnli_path = './data/myXNLI.train.tsv'  # Path to the file\n",
    "myxnli_data = pd.read_csv(myxnli_path, sep='\\t', header=0)\n",
    "print(f\"myXNLI dataset loaded successfully with {len(myxnli_data)} records.\")\n",
    "display(myxnli_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALT English data\n",
    "alt_en_path = './data/ALT_data_en.txt'  # Path to the English ALT corpus\n",
    "alt_en_data = pd.read_csv(alt_en_path, sep='\\t', header=None, names=[\"ID\", \"English_Sentence\"])\n",
    "print(f\"ALT English dataset loaded successfully with {len(alt_en_data)} records.\")\n",
    "display(alt_en_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALT Burmese data\n",
    "alt_my_path = './data/ALT_data_my.txt'  # Path to the Burmese ALT corpus\n",
    "alt_my_data = pd.read_csv(alt_my_path, sep='\\t', header=None, names=[\"ID\", \"Burmese_Sentence\"])\n",
    "print(f\"ALT Burmese dataset loaded successfully with {len(alt_my_data)} records.\")\n",
    "display(alt_my_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ALT datasets (if IDs match)\n",
    "alt_combined = pd.merge(alt_en_data, alt_my_data, on=\"ID\")\n",
    "print(f\"ALT combined dataset created successfully with {len(alt_combined)} records.\")\n",
    "display(alt_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning\n",
    "This step focuses on cleaning the datasets to prepare them for further processing. The cleaning operations include:\n",
    "1. Removing duplicate entries.\n",
    "2. Handling missing values.\n",
    "3. Removing non-standard characters or symbols unrelated to the Burmese or English language.\n",
    "4. Ensuring consistent formatting.\n",
    "\n",
    "The cleaned datasets will be ready for normalization and tokenization in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning myXNLI dataset\n",
    "print(\"Cleaning myXNLI dataset...\")\n",
    "myxnli_cleaned = myxnli_data.drop_duplicates()  # Remove duplicates\n",
    "myxnli_cleaned = myxnli_cleaned.dropna()  # Remove rows with missing values\n",
    "#myxnli_cleaned = myxnli_cleaned.replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"myXNLI dataset cleaned successfully.\")\n",
    "print(f\"Original Records: {len(myxnli_data)}.\")\n",
    "print(f\"Remaining records: {len(myxnli_cleaned)}.\")\n",
    "display(myxnli_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning ALT English data\n",
    "print(\"Cleaning ALT English dataset...\")\n",
    "alt_en_cleaned = alt_en_data.drop_duplicates()  # Remove duplicates\n",
    "alt_en_cleaned = alt_en_cleaned.dropna()  # Remove rows with missing values\n",
    "alt_en_cleaned[\"English_Sentence\"] = alt_en_cleaned[\"English_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT English dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_en_data)}.\")\n",
    "print(f\"Remaining records: {len(alt_en_cleaned)}.\")\n",
    "display(alt_en_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning ALT Burmese data\n",
    "print(\"Cleaning ALT Burmese dataset...\")\n",
    "alt_my_cleaned = alt_my_data.drop_duplicates()  # Remove duplicates\n",
    "alt_my_cleaned = alt_my_cleaned.dropna()  # Remove rows with missing values\n",
    "#alt_my_cleaned[\"Burmese_Sentence\"] = alt_my_cleaned[\"Burmese_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT Burmese dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_my_data)}\")\n",
    "print(f\"Remaining records: {len(alt_my_cleaned)}\")\n",
    "display(alt_my_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cleaned ALT datasets\n",
    "print(\"Cleaning combined ALT dataset...\")\n",
    "alt_combined_cleaned = pd.merge(alt_en_cleaned, alt_my_cleaned, on=\"ID\")\n",
    "print(f\"Combined ALT dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_combined)}\")\n",
    "print(f\"Remaining records: {len(alt_combined_cleaned)}\")\n",
    "display(alt_combined_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Normalization\n",
    "This step normalizes the text data to ensure consistency across datasets. The normalization process includes:\n",
    "1. Applying Unicode normalization to handle encoding inconsistencies.\n",
    "2. Standardizing text formatting by converting all text to lowercase and standardizing punctuation.\n",
    "3. Normalizing diacritical marks and stacked consonants in the Burmese text to improve text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Apply Unicode normalization\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Convert to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    # Standardize punctuation (e.g., replace unusual punctuation marks)\n",
    "    normalized_text = normalized_text.replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize Burmese text (handles diacritical marks and stacked consonants)\n",
    "def normalize_burmese(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Additional Burmese-specific normalization can be added here if needed\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize myXNLI cleaned dataset\n",
    "print(\"Normalizing myXNLI dataset...\")\n",
    "myxnli_normalized = myxnli_cleaned.copy()\n",
    "\n",
    "# Normalize English columns\n",
    "myxnli_normalized[\"sentence1_en\"] = myxnli_normalized[\"sentence1_en\"].apply(normalize_text)\n",
    "myxnli_normalized[\"sentence2_en\"] = myxnli_normalized[\"sentence2_en\"].apply(normalize_text)\n",
    "\n",
    "# Normalize Burmese columns\n",
    "myxnli_normalized[\"sentence1_my\"] = myxnli_normalized[\"sentence1_my\"].apply(normalize_burmese)\n",
    "myxnli_normalized[\"sentence2_my\"] = myxnli_normalized[\"sentence2_my\"].apply(normalize_burmese)\n",
    "\n",
    "print(f\"myXNLI dataset normalized successfully.\")\n",
    "display(myxnli_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ALT English cleaned dataset\n",
    "print(\"Normalizing ALT English dataset...\")\n",
    "alt_en_normalized = alt_en_cleaned.copy()\n",
    "alt_en_normalized[\"English_Sentence\"] = alt_en_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "print(f\"ALT English dataset normalized successfully.\")\n",
    "display(alt_en_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize ALT Burmese cleaned dataset\n",
    "print(\"Normalizing ALT Burmese dataset...\")\n",
    "alt_my_normalized = alt_my_cleaned.copy()\n",
    "alt_my_normalized[\"Burmese_Sentence\"] = alt_my_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"ALT Burmese dataset normalized successfully.\")\n",
    "display(alt_my_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize combined ALT cleaned dataset\n",
    "print(\"Normalizing combined ALT dataset...\")\n",
    "alt_combined_normalized = alt_combined_cleaned.copy()\n",
    "alt_combined_normalized[\"English_Sentence\"] = alt_combined_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "alt_combined_normalized[\"Burmese_Sentence\"] = alt_combined_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"Combined ALT dataset normalized successfully.\")\n",
    "display(alt_combined_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sentence Segmentation\n",
    "This step segments text into subword units using SentencePiece Tokenization (SPT). \n",
    "The process includes:\n",
    "1. Training a SentencePiece model using the English and Burmese text from the `myXNLI` dataset and the combined ALT dataset.\n",
    "2. Applying the trained model to segment sentences in both datasets.\n",
    "3. Validating the segmentation results with manual or automated benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare paths for SentencePiece model\n",
    "sp_model_prefix = \"sentencepiece_model\"\n",
    "sp_train_input = \"combined_texts.txt\"  # A temporary file to hold combined dataset text for training\n",
    "sp_model_path = f\"{sp_model_prefix}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text from myXNLI and ALT datasets for SentencePiece training\n",
    "print(\"Preparing data for SentencePiece training...\")\n",
    "with open(sp_train_input, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Add text from myXNLI dataset\n",
    "    for text in myxnli_normalized[\"sentence1_en\"].tolist() + myxnli_normalized[\"sentence2_en\"].tolist() + myxnli_normalized[\"sentence1_my\"].tolist() + myxnli_normalized[\"sentence2_my\"].tolist():\n",
    "        if pd.notnull(text):  # Avoid writing NaN values\n",
    "            f.write(f\"{text}\\n\")\n",
    "        \n",
    "    # Add text from combined ALT dataset\n",
    "    for text in alt_combined_normalized[\"English_Sentence\"].tolist() + alt_combined_normalized[\"Burmese_Sentence\"].tolist():\n",
    "        if pd.notnull(text):  # Avoid writing NaN values\n",
    "            f.write(f\"{text}\\n\")\n",
    "\n",
    "print(f\"Data prepared in {sp_train_input}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece model\n",
    "print(\"Training SentencePiece model...\")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=sp_train_input,\n",
    "    model_prefix=sp_model_prefix,\n",
    "    vocab_size=8000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type=\"unigram\"  # Use unigram language model\n",
    ")\n",
    "print(f\"SentencePiece model trained and saved as {sp_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "print(f\"SentencePiece model loaded from {sp_model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SentencePiece Tokenization\n",
    "def apply_sentencepiece(data, column_name):\n",
    "    return data[column_name].apply(lambda x: \" \".join(sp.encode_as_pieces(x)) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SentencePiece Tokenization to myXNLI dataset\n",
    "print(\"Applying SentencePiece tokenization to myXNLI dataset...\")\n",
    "myxnli_segmented = myxnli_normalized.copy()\n",
    "myxnli_segmented[\"sentence1_en\"] = apply_sentencepiece(myxnli_segmented, \"sentence1_en\")\n",
    "myxnli_segmented[\"sentence2_en\"] = apply_sentencepiece(myxnli_segmented, \"sentence2_en\")\n",
    "myxnli_segmented[\"sentence1_my\"] = apply_sentencepiece(myxnli_segmented, \"sentence1_my\")\n",
    "myxnli_segmented[\"sentence2_my\"] = apply_sentencepiece(myxnli_segmented, \"sentence2_my\")\n",
    "print(\"SentencePiece tokenization applied to myXNLI dataset successfully.\")\n",
    "display(myxnli_segmented.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SentencePiece Tokenization to ALT English and Burmese datasets\n",
    "print(\"Applying SentencePiece tokenization to combined ALT dataset...\")\n",
    "alt_combined_segmented = alt_combined_normalized.copy()\n",
    "alt_combined_segmented[\"English_Sentence\"] = apply_sentencepiece(alt_combined_segmented, \"English_Sentence\")\n",
    "alt_combined_segmented[\"Burmese_Sentence\"] = apply_sentencepiece(alt_combined_segmented, \"Burmese_Sentence\")\n",
    "print(\"SentencePiece tokenization applied to combined ALT dataset successfully.\")\n",
    "display(alt_combined_segmented.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Morphological Processing\n",
    "This step involves advanced processing to capture morphological nuances in the text. The operations include:\n",
    "1. Segmenting words into morphemes, handling prefixes, suffixes, and compound words.\n",
    "2. Normalizing compounded forms while preserving semantic meanings.\n",
    "3. Incorporating loanwords for better representation in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment words into morphemes\n",
    "def segment_morphemes(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Example: Handle prefixes, suffixes, and compounds\n",
    "    # For demonstration, splitting by common Burmese and English morphemes\n",
    "    segmented_text = re.sub(r'(\\bpre|un|re|in|dis|mis|non)(\\w+)', r'\\1-\\2', text)  # English prefixes\n",
    "    segmented_text = re.sub(r'(\\w+)(ing|ly|ed|er|ion|able|ible|ment|ness|ship|ous|ive|ish|ize)\\b', r'\\1-\\2', segmented_text)  # English suffixes\n",
    "    # Add custom Burmese rules here for morpheme segmentation\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize compounded forms\n",
    "def normalize_compounds(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Example: Handle English hyphenated compounds (adjust as needed for Burmese)\n",
    "    normalized_text = re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to incorporate loanwords\n",
    "def incorporate_loanwords(text, loanword_dict):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Replace loanwords based on a predefined dictionary\n",
    "    for loanword, replacement in loanword_dict.items():\n",
    "        text = re.sub(rf'\\b{loanword}\\b', replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample loanword dictionary for Burmese\n",
    "loanword_dict = {\n",
    "    \"ဘဏ်\": \"bank\",  # Example: Burmese word for 'bank'\n",
    "    \"အင်တာနက်\": \"internet\",  # Example: Burmese word for 'internet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply morphological processing to myXNLI dataset\n",
    "print(\"Processing myXNLI dataset...\")\n",
    "myxnli_processed = myxnli_segmented.copy()\n",
    "# Apply morpheme segmentation\n",
    "myxnli_processed[\"sentence1_en\"] = myxnli_processed[\"sentence1_en\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence2_en\"] = myxnli_processed[\"sentence2_en\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(segment_morphemes)\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(segment_morphemes)\n",
    "# Normalize compounded forms\n",
    "myxnli_processed[\"sentence1_en\"] = myxnli_processed[\"sentence1_en\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence2_en\"] = myxnli_processed[\"sentence2_en\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(normalize_compounds)\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(normalize_compounds)\n",
    "# Incorporate loanwords\n",
    "myxnli_processed[\"sentence1_my\"] = myxnli_processed[\"sentence1_my\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "myxnli_processed[\"sentence2_my\"] = myxnli_processed[\"sentence2_my\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "    \n",
    "print(\"Morphological processing applied to myXNLI dataset successfully.\")\n",
    "display(myxnli_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply morphological processing to combined ALT dataset\n",
    "print(\"Processing combined ALT dataset...\")\n",
    "alt_combined_processed = alt_combined_segmented.copy()\n",
    "# Apply morpheme segmentation\n",
    "alt_combined_processed[\"English_Sentence\"] = alt_combined_processed[\"English_Sentence\"].apply(segment_morphemes)\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(segment_morphemes)\n",
    "# Normalize compounded forms\n",
    "alt_combined_processed[\"English_Sentence\"] = alt_combined_processed[\"English_Sentence\"].apply(normalize_compounds)\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(normalize_compounds)\n",
    "# Incorporate loanwords\n",
    "alt_combined_processed[\"Burmese_Sentence\"] = alt_combined_processed[\"Burmese_Sentence\"].apply(lambda x: incorporate_loanwords(x, loanword_dict))\n",
    "\n",
    "print(\"Morphological processing applied to combined ALT dataset successfully.\")\n",
    "display(alt_combined_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Data Augmentation\n",
    "This step enhances the dataset by generating additional data using the following methods:\n",
    "1. **Back-Translation**:\n",
    "    - Translate Burmese sentences to English and back to Burmese using `facebook/m2m100_418M` and `facebook/mbart-large-50` to create diverse translations while preserving semantic meaning for both `myXNLI` and `combined ALT dataset`.\n",
    "2. **Pseudo-Parallel Corpus Creation**:\n",
    "    - Use semantic similarity alignment to identify and align semantically similar sentences from monolingual data to generate pseudo-parallel corpora for the `combined ALT dataset` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create temp df\n",
    "def create_temp_df():\n",
    "    return pd.DataFrame(columns=[\"isNull\", \"original\", \"translated\", \"back_translated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add row to temp df\n",
    "def add_row_to_temp_df(df, row):\n",
    "    # Convert the row dictionary to a DataFrame\n",
    "    row_df = pd.DataFrame([row])\n",
    "\n",
    "    # Use pd.concat to add the row\n",
    "    updated_df = pd.concat([df, row_df], ignore_index=True)\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save and display temp df\n",
    "def save_display_temp_df(temp_df, tmp_df_name):\n",
    "    temp_df.to_csv(f\"{tmp_df_name}.csv\", index=False)\n",
    "    display(temp_df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back-Translation (facebook/m2m100_418M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load M2M100 model and tokenizer\n",
    "m2m_model_name = \"facebook/m2m100_418M\"\n",
    "m2m_translation_model = M2M100ForConditionalGeneration.from_pretrained(m2m_model_name).to(device)\n",
    "m2m_translation_tokenizer = M2M100Tokenizer.from_pretrained(m2m_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using M2M100\n",
    "def m2m_back_translate(text, src_lang, tgt_lang, df, df_name):\n",
    "    if pd.isnull(text):\n",
    "        df.append({\"isNull\": True})\n",
    "        save_display_temp_df(df, df_name)\n",
    "        return text  # Skip null values\n",
    "    # Translate to the target language\n",
    "    m2m_translation_tokenizer.src_lang = src_lang\n",
    "    encoded = m2m_translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    translated = m2m_translation_model.generate(**encoded)\n",
    "    translated_text = m2m_translation_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Translate back to the source language\n",
    "    m2m_translation_tokenizer.src_lang = tgt_lang\n",
    "    encoded_back = m2m_translation_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    back_translated = m2m_translation_model.generate(**encoded_back)\n",
    "    back_translated_text = m2m_translation_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "    \n",
    "    new_row = {\"isNull\": False, \"original\": text, \"translated\": translated_text, \"back_translated\": back_translated_text}\n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "    save_display_temp_df(df, df_name)\n",
    "\n",
    "    return back_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences in myXNLI\n",
    "myxnli_m2m_back_translated = myxnli_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 1 in myXNLI\n",
    "myxnli_m2m_back_translated_temp_df1 = create_temp_df()\n",
    "myxnli_m2m_back_translated[\"sentence1_my\"] = myxnli_m2m_back_translated[\"sentence1_my\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\", df=myxnli_m2m_back_translated_temp_df1, df_name='myxnli_m2m_back_translated_temp_df1')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 2 in myXNLI\n",
    "myxnli_m2m_back_translated_temp_df2 = create_temp_df()\n",
    "myxnli_m2m_back_translated[\"sentence2_my\"] = myxnli_m2m_back_translated[\"sentence2_my\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\", df=myxnli_m2m_back_translated_temp_df2, df_name='myxnli_m2m_back_translated_temp_df2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display and save back-translated myXNLI dataset\n",
    "print(\"Back-translation applied to myXNLI dataset with m2m100.\")\n",
    "display(myxnli_m2m_back_translated.head())\n",
    "myxnli_m2m_back_translated.to_csv('myxnli_m2m_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to combined ALT dataset\n",
    "print(\"Applying back-translation to combined ALT dataset with m2m100...\")\n",
    "alt_m2m_back_translated = alt_combined_processed.copy()\n",
    "alt_m2m_back_translated[\"Burmese_Sentence\"] = alt_m2m_back_translated[\"Burmese_Sentence\"].apply(\n",
    "    lambda x: m2m_back_translate(x, src_lang=\"my\", tgt_lang=\"en\") if pd.notnull(x) else x\n",
    ")\n",
    "print(\"Back-translation applied to combined ALT dataset with m2m100.\")\n",
    "display(alt_m2m_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back-translated alt dataset\n",
    "alt_m2m_back_translated.to_csv('alt_m2m_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back-Translation (facebook/mbart-large-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mBART-50 model and tokenizer\n",
    "mbart_model_name = \"facebook/mbart-large-50\"\n",
    "mbart_translation_model = MBartForConditionalGeneration.from_pretrained(mbart_model_name).to(device)\n",
    "mbart_translation_tokenizer = MBart50TokenizerFast.from_pretrained(mbart_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using mBART-50\n",
    "def mbart_back_translate(text, src_lang, tgt_lang, df, df_name):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        df.append({\"isNull\": True})\n",
    "        save_display_temp_df(df, df_name)\n",
    "        return text  # Skip null values\n",
    "\n",
    "    # Translate to the target language\n",
    "    mbart_translation_tokenizer.src_lang = src_lang\n",
    "    encoded = mbart_translation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    translated = mbart_translation_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=mbart_translation_tokenizer.lang_code_to_id[tgt_lang]\n",
    "    )\n",
    "    translated_text = mbart_translation_tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Translate back to the source language\n",
    "    mbart_translation_tokenizer.src_lang = tgt_lang\n",
    "    encoded_back = mbart_translation_tokenizer(translated_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    back_translated = mbart_translation_model.generate(\n",
    "        **encoded_back,\n",
    "        forced_bos_token_id=mbart_translation_tokenizer.lang_code_to_id[src_lang]\n",
    "    )\n",
    "    back_translated_text = mbart_translation_tokenizer.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    new_row = {\"isNull\": False, \"original\": text, \"translated\": translated_text, \"back_translated\": back_translated_text}\n",
    "    df.loc[len(df)] = new_row\n",
    "\n",
    "    save_display_temp_df(df, df_name)\n",
    "\n",
    "    return back_translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences in myXNLI\n",
    "myxnli_mbart_back_translated = myxnli_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 1 in myXNLI\n",
    "myxnli_mbart_back_translated_temp_df1 = create_temp_df()\n",
    "myxnli_mbart_back_translated[\"sentence1_my\"] = myxnli_mbart_back_translated[\"sentence1_my\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=myxnli_mbart_back_translated_temp_df1, df_name=\"myxnli_mbart_back_translated_temp_df1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the Burmese sentences 2 in myXNLI\n",
    "myxnli_mbart_back_translated_temp_df2 = create_temp_df()\n",
    "myxnli_mbart_back_translated[\"sentence2_my\"] = myxnli_mbart_back_translated[\"sentence2_my\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=myxnli_mbart_back_translated_temp_df2, df_name=\"myxnli_mbart_back_translated_temp_df2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display and save back-translated myXNLI dataset\n",
    "print(\"Back-translation applied to myXNLI dataset with mBART-50.\")\n",
    "display(myxnli_mbart_back_translated.head())\n",
    "myxnli_mbart_back_translated.to_csv('myxnli_mbart_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to combined ALT dataset\n",
    "alt_mbart_back_translated_temp_df = create_temp_df()\n",
    "alt_mbart_back_translated = alt_combined_processed.copy()\n",
    "alt_mbart_back_translated[\"Burmese_Sentence\"] = alt_mbart_back_translated[\"Burmese_Sentence\"].apply(\n",
    "    lambda x: mbart_back_translate(x, src_lang=\"my_MM\", tgt_lang=\"en_XX\", df=alt_mbart_back_translated_temp_df, df_name=\"alt_mbart_back_translated_temp_df\")\n",
    ")\n",
    "print(\"Back-translation applied to combined ALT dataset with mBART-50.\")\n",
    "display(alt_mbart_back_translated.head())\n",
    "alt_mbart_back_translated.to_csv('alt_mbart_back_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pseudo-Parallel Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic similarity model\n",
    "similarity_model_name = \"all-MiniLM-L6-v2\"\n",
    "similarity_model = SentenceTransformer(similarity_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create pseudo-parallel corpus\n",
    "def create_pseudo_parallel(data_en, data_my, similarity_model, top_k=1):\n",
    "    pseudo_parallel = []\n",
    "    embeddings_en = similarity_model.encode(data_en, convert_to_tensor=True, device=device)\n",
    "    embeddings_my = similarity_model.encode(data_my, convert_to_tensor=True, device=device)\n",
    "    similarity_scores = util.pytorch_cos_sim(embeddings_en, embeddings_my)\n",
    "\n",
    "    for idx_en, scores in enumerate(similarity_scores):\n",
    "        top_matches = scores.topk(k=top_k)\n",
    "        for match_idx in top_matches.indices:\n",
    "            pseudo_parallel.append((data_en[idx_en], data_my[match_idx.item()], scores[match_idx].item()))\n",
    "    \n",
    "    return pseudo_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pseudo-parallel creation to combined ALT dataset\n",
    "print(\"Creating pseudo-parallel corpus from combined ALT dataset...\")\n",
    "alt_combined_en = alt_combined_processed[\"English_Sentence\"].dropna().tolist()\n",
    "alt_combined_my = alt_combined_processed[\"Burmese_Sentence\"].dropna().tolist()\n",
    "pseudo_parallel_data = create_pseudo_parallel(alt_combined_en, alt_combined_my, similarity_model)\n",
    "    \n",
    "pseudo_parallel_df = pd.DataFrame(pseudo_parallel_data, columns=[\"English_Sentence\", \"Burmese_Sentence\", \"Similarity_Score\"])\n",
    "print(\"Pseudo-parallel corpus created successfully.\")\n",
    "display(pseudo_parallel_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pseudo-parallel alt dataset\n",
    "pseudo_parallel_df.to_csv('pseudo_parallel_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
