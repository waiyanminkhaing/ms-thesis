{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n",
      "2025-02-05 00:37:34.798154: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-05 00:37:34.815300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-05 00:37:34.840247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-05 00:37:34.840275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-05 00:37:34.855632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-05 00:37:35.857426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "=======\n",
      "2025-02-05 00:35:20.073335: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-05 00:35:20.088990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-05 00:35:20.113032: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-05 00:35:20.113071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-05 00:35:20.127938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-05 00:35:21.029616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import bert_score\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_chrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n",
      "2025-02-05 00:37:42.127506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-05 00:37:42.212323: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-05 00:37:42.213543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "=======\n",
      "2025-02-05 00:35:26.603708: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-05 00:35:26.650643: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-05 00:35:26.652003: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLEU and ROUGE metric objects\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for batching\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = [str(text) if text is not None else \"\" for text in texts] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class for evaluation dataset\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.predictions = dataframe[\"generated\"].astype(str).tolist()\n",
    "        self.references = dataframe[\"burmese\"].astype(str).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.predictions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.predictions[idx], self.references[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save models df\n",
    "def save_models_df(df, df_name):\n",
    "    df.to_csv(f\"models/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load spt df\n",
    "def load_spt_df(df_name):\n",
    "    return pd.read_csv(f\"spt/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load models df\n",
    "def load_models_df(df_name):\n",
    "    return pd.read_csv(f\"models/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute bleu score\n",
    "def compute_bleu(reference, prediction):\n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference.split()], prediction.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    return ast.literal_eval(val) if isinstance(val, str) else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_batch(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(EvaluationDataset(dataset), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_bleu_scores, all_rouge1, all_rouge2, all_rougeL, all_chrfs, all_berts = [], [], [], [], [], []\n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Computing Metrics\", unit=\"batch\"):\n",
    "        predictions, references = batch\n",
    "\n",
    "        # Compute BLEU in batch\n",
    "        batch_bleu = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth_fn)\n",
    "                      for pred, ref in zip(predictions, references)]\n",
    "        all_bleu_scores.extend(batch_bleu)\n",
    "\n",
    "        # Compute ROUGE in batch\n",
    "        batch_rouge = [rouge.score(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "        all_rouge1.extend([r[\"rouge1\"].fmeasure for r in batch_rouge])\n",
    "        all_rouge2.extend([r[\"rouge2\"].fmeasure for r in batch_rouge])\n",
    "        all_rougeL.extend([r[\"rougeL\"].fmeasure for r in batch_rouge])\n",
    "\n",
    "        # Compute chrF-S in batch\n",
    "        batch_chrf = corpus_chrf(predictions, [[ref] for ref in references]).score\n",
    "        all_chrfs.extend([batch_chrf] * len(predictions))  # Apply same batch score to all\n",
    "\n",
    "        # Compute BERTScore in batch\n",
    "        batch_bert = bert_score.score(predictions, references, lang=\"my\", device=device)\n",
    "        all_berts.extend(batch_bert[2].tolist())  # F1 scores from BERTScore\n",
    "\n",
    "    print(\"Finished Computing Metrics!\")\n",
    "\n",
    "    # Store results back into dataset\n",
    "    dataset[\"bleu\"] = all_bleu_scores\n",
    "    dataset[\"rouge-1\"] = all_rouge1\n",
    "    dataset[\"rouge-2\"] = all_rouge2\n",
    "    dataset[\"rouge-l\"] = all_rougeL\n",
    "    dataset[\"chrf-s\"] = all_chrfs\n",
    "    dataset[\"bert_score\"] = all_berts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity in batch\n",
    "def compute_perplexity_batch(texts, tokenizer, model):\n",
    "    # Ensure all inputs are valid strings\n",
    "    valid_texts = [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
    "\n",
    "    # Tokenize batch with padding & truncation\n",
    "    inputs = tokenizer(valid_texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # Compute log probabilities\n",
    "\n",
    "    # Get token log-likelihoods using true token IDs\n",
    "    target_ids = inputs[\"input_ids\"]  # (batch_size, seq_len)\n",
    "    log_likelihood = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Apply attention mask to remove padding tokens\n",
    "    mask = inputs[\"attention_mask\"]  # (batch_size, seq_len)\n",
    "    masked_log_likelihood = log_likelihood * mask  # Zero out padding contributions\n",
    "\n",
    "    # Compute sentence-level mean log-likelihood (only over valid tokens)\n",
    "    sentence_log_likelihood = masked_log_likelihood.sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "    # Clamp values to avoid numerical instability\n",
    "    sentence_log_likelihood = torch.clamp(sentence_log_likelihood, min=-100, max=100)\n",
    "\n",
    "    # Convert log-likelihood to perplexity\n",
    "    log_perplexity = -sentence_log_likelihood\n",
    "    perplexities = torch.exp(log_perplexity)\n",
    "\n",
    "    # 🔍 Print warning if perplexities contain `inf`\n",
    "    if torch.isinf(perplexities).any():\n",
    "        print(\"\\n [WARNING] Perplexity contains `inf` for some texts!\")\n",
    "        print(f\"  Log-Likelihood Shape: {log_likelihood.shape}\")\n",
    "        print(f\"  Log-Likelihood Mean: {sentence_log_likelihood.mean().item()}\")\n",
    "        print(f\"  Computed Perplexity Values: {perplexities}\")\n",
    "\n",
    "    return perplexities.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing RNN/LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing\n",
    "Load SPT-tokenized datasets, convert to sequences, and apply padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load datasets\n",
    "def get_lstm_datasets(model_name):\n",
    "    return {\n",
    "        \"normal\": [\n",
    "            f\"tokenized_{model_name}_myxnli_normalized_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_normalized_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_normalized\"\n",
    "        ],\n",
    "        \"nllb_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_nllb_back_translated_final\"\n",
    "        ],\n",
    "        \"seamless_m4t_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_1\",\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_2\",\n",
    "            f\"tokenized_{model_name}_alt_combined_seamless_m4t_back_translated_final\"\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_lstm(file_name):\n",
    "    df = load_spt_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "        \"english_back_translated_tokens\": \"english_tokens\",\n",
    "        \"burmese_translated_tokens\": \"burmese_tokens\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\", \"english_tokens\", \"burmese_tokens\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "lstm_all_datasets = {}\n",
    "for model_name in spt_models.keys():\n",
    "    datasets = get_lstm_datasets(model_name)\n",
    "\n",
    "    lstm_all_datasets[model_name] = {\n",
    "        key: [load_and_rename_columns_lstm(file) for file in file_list] for key, file_list in datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobine all datasets\n",
    "lstm_all_datasets_combined = {}\n",
    "for model_name in lstm_all_datasets.keys():\n",
    "    lstm_all_datasets_combined[model_name] = pd.concat(\n",
    "        [pd.concat(datasets) for datasets in lstm_all_datasets[model_name].values()],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias and drop null\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name] = lstm_all_datasets_combined[model_name].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe dataset length: 1627576\n",
      "unigram dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# display of datasets\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    print(f\"{model_name} dataset length: {len(lstm_all_datasets_combined[model_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9a8580ffe4e71bf432aabe37f3e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0dfee7ebcc4bce8f4310359d590ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821c5216a41c4a1091a5814be6f84558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bdd8a4c9564f75a3c4e3d9e587ee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert tokenized sequences to lists\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq\"] = lstm_all_datasets_combined[model_name][\"english_tokens\"].progress_apply(\n",
    "        lambda x: spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq\"] = lstm_all_datasets_combined[model_name][\"burmese_tokens\"].progress_apply(\n",
    "        lambda x:  spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "lstm_max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply padding to sequences\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"english_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()\n",
    "\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"burmese_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm preprocess data\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    save_models_df(lstm_all_datasets_combined[model_name], f\"lstm_{model_name}_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define LSTM Model\n",
    "Define an LSTM-based sequence-to-sequence (seq2seq) model with embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_embedding_dim = 256\n",
    "lstm_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size from SentencePiece models\n",
    "lstm_vocab_sizes = {model_name: sp.GetPieceSize() for model_name, sp in spt_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build lstm model\n",
    "def build_lstm_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=lstm_embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:15:44.588870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.591696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.593767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.731312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.732575: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.733704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.734788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_bpe_model = build_lstm_model(lstm_vocab_sizes[\"bpe\"])\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:36:43.437264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.512506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.515652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.720777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.722038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.723184: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.724263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_unigram_model = build_lstm_model(lstm_vocab_sizes[\"unigram\"])\n",
    "lstm_unigram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Train the model using Categorical Cross-Entropy loss & Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model train batch size\n",
    "lstm_train_batch_size = 64\n",
    "lstm_train_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_bpe_model_prefix = \"models/lstm_bpe_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_preprocessed = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_bpe_preprocessed[\"burmese_seq_padded\"] = lstm_bpe_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_preprocessed[\"english_seq_padded\"] = lstm_bpe_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_bpe_X_train = np.array(lstm_bpe_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_bpe_y_train = np.array(lstm_bpe_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_bpe_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_bpe_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_bpe_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_bpe_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_bpe_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738618633.745274   25622 service.cc:145] XLA service 0x7f532af170a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738618633.745310   25622 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 21:37:14.172679: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 21:37:14.804654: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738618635.464858   25622 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 28114s 1s/step - loss: 1.5701 - accuracy: 0.6595 - val_loss: 1.6427 - val_accuracy: 0.6395\n",
      "Epoch 2/5\n",
      "22888/22888 [==============================] - 27965s 1s/step - loss: 1.5478 - accuracy: 0.6636 - val_loss: 1.6377 - val_accuracy: 0.6406\n",
      "Epoch 3/5\n",
      "22888/22888 [==============================] - 27752s 1s/step - loss: 1.5296 - accuracy: 0.6669 - val_loss: 1.6369 - val_accuracy: 0.6408\n",
      "Epoch 4/5\n",
      "  385/22888 [..............................] - ETA: 7:27:51 - loss: 1.4893 - accuracy: 0.6770"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738716574.969466    5772 service.cc:145] XLA service 0x7f6fcc016ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738716574.969504    5772 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-05 00:49:34.976221: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-05 00:49:35.011981: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738716575.075759    5772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 28155s 1s/step - loss: 1.5143 - accuracy: 0.6699 - val_loss: 1.6157 - val_accuracy: 0.6414\n",
      "Epoch 2/2\n",
      "22888/22888 [==============================] - 28097s 1s/step - loss: 1.5006 - accuracy: 0.6724 - val_loss: 1.6336 - val_accuracy: 0.6406\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe model\n",
    "lstm_bpe_model.save(f\"{lstm_bpe_model_prefix}.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_unigram_model_prefix = \"models/lstm_unigram_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_preprocessed = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_unigram_preprocessed[\"burmese_seq_padded\"] = lstm_unigram_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_preprocessed[\"english_seq_padded\"] = lstm_unigram_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_unigram_X_train = np.array(lstm_unigram_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_unigram_y_train = np.array(lstm_unigram_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_unigram_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_unigram_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_unigram_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_unigram_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_unigram_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738618686.064670   25952 service.cc:145] XLA service 0x7f83f2cc1640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738618686.064704   25952 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 21:38:06.288576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 21:38:06.670883: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738618687.125419   25952 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 27693s 1s/step - loss: 1.2076 - accuracy: 0.7599 - val_loss: 1.2439 - val_accuracy: 0.7482\n",
      "Epoch 2/5\n",
      "22888/22888 [==============================] - 27550s 1s/step - loss: 1.1834 - accuracy: 0.7633 - val_loss: 1.2398 - val_accuracy: 0.7481\n",
      "Epoch 3/5\n",
      "22888/22888 [==============================] - 27326s 1s/step - loss: 1.1643 - accuracy: 0.7659 - val_loss: 1.2261 - val_accuracy: 0.7483\n",
      "Epoch 4/5\n",
      " 1697/22888 [=>............................] - ETA: 6:49:34 - loss: 1.1317 - accuracy: 0.7720"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_unigram_model.fit(\n",
    "    lstm_unigram_X_train, \n",
    "    lstm_unigram_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_unigram_early_stopping, lstm_unigram_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738716422.095172   15580 service.cc:145] XLA service 0x7f285f2d0a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738716422.095209   15580 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-05 00:47:02.101910: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-05 00:47:02.121342: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738716422.182801   15580 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 27650s 1s/step - loss: 1.1481 - accuracy: 0.7684 - val_loss: 1.2335 - val_accuracy: 0.7479\n",
      "Epoch 2/2\n",
      "22888/22888 [==============================] - 27903s 1s/step - loss: 1.1344 - accuracy: 0.7703 - val_loss: 1.2331 - val_accuracy: 0.7475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f2a3c9d3940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_unigram_model.fit(\n",
    "    lstm_unigram_X_train,\n",
    "    lstm_unigram_y_train,\n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[lstm_unigram_early_stopping, lstm_unigram_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe model\n",
    "lstm_unigram_model.save(f\"{lstm_unigram_model_prefix}.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Use trained LSTM models to generate translations for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Text Dataset\n",
    "class LstmTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for text sequences\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokenized_text = self.tokenizer.encode(text, out_type=int)\n",
    "\n",
    "        # Pad sequences to max_length\n",
    "        tokenized_text = tokenized_text[:self.max_length]\n",
    "        padding_length = self.max_length - len(tokenized_text)\n",
    "        padded_sequence = tokenized_text + [0] * padding_length  # Padding with 0\n",
    "\n",
    "        return torch.tensor(padded_sequence, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text using LSTM in auto-regressive manner for batch processing.\n",
    "def generate_text_lstm_batch(dataloader, model, tokenizer, max_length=50):\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Generating Text with LSTM\"):\n",
    "        batch = batch.numpy()  # Convert to NumPy for TensorFlow processing\n",
    "        batch_generated = []\n",
    "\n",
    "        for seq in batch:\n",
    "            generated_sequence = seq.tolist()\n",
    "\n",
    "            for _ in range(max_length - len(seq)):\n",
    "                input_padded = pad_sequences(\n",
    "                    [generated_sequence], maxlen=max_length, padding='pre'\n",
    "                )\n",
    "                input_tensor = tf.convert_to_tensor(input_padded, dtype=tf.int32)\n",
    "\n",
    "                predictions = model.predict(input_tensor, verbose=0)\n",
    "                next_token = np.argmax(predictions[0], axis=-1)\n",
    "\n",
    "                if next_token == tokenizer.eos_id():\n",
    "                    break  # Stop at EOS token\n",
    "\n",
    "                generated_sequence.append(next_token)\n",
    "\n",
    "            batch_generated.append(tokenizer.decode(generated_sequence))\n",
    "\n",
    "        all_predictions.extend(batch_generated)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(f\"models/lstm_bpe_model.h5\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_predictions = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_bpe_predictions[\"burmese_seq_padded\"] = lstm_bpe_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_predictions_sequences = np.array(lstm_bpe_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_bpe_predictions_dataset = LstmTextDataset(lstm_bpe_predictions_sequences, lstm_bpe_tokenizer)\n",
    "lstm_bpe_predictions_dataloader = DataLoader(lstm_bpe_predictions_dataset, batch_size=lstm_bpe_predictions_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_bpe_predictions[\"generated\"] = generate_text_lstm_batch(lstm_bpe_predictions_dataloader, lstm_bpe_model, lstm_bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns and display\n",
    "lstm_bpe_predictions = lstm_bpe_predictions[lstm_bpe_predictions[\"english\", \"burmese\", \"generated\"]]\n",
    "display(lstm_bpe_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe predictions\n",
    "save_models_df(lstm_bpe_predictions, \"lstm_bpe_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.h5\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_predictions = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_unigram_predictions[\"burmese_seq_padded\"] = lstm_unigram_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_predictions_sequences = np.array(lstm_unigram_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_unigram_predictions_dataset = LstmTextDataset(lstm_unigram_predictions_sequences, lstm_unigram_tokenizer)\n",
    "lstm_unigram_predictions_dataloader = DataLoader(lstm_unigram_predictions_dataset, batch_size=lstm_unigram_predictions_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_unigram_predictions[\"generated\"] = generate_text_lstm_batch(lstm_unigram_predictions_dataloader, lstm_unigram_model, lstm_unigram_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns and display\n",
    "lstm_unigram_predictions = lstm_unigram_predictions[lstm_unigram_predictions[\"english\", \"burmese\", \"generated\"]]\n",
    "display(lstm_unigram_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm unigram predictions\n",
    "save_models_df(lstm_unigram_predictions, \"lstm_unigram_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm predictions\n",
    "lstm_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"lstm_{model_name}_predictions\") for model_name in spt_models.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Processing Data for {model_name}...\")\n",
    "    compute_metrics_batch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resutls\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"lstm_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(f\"models/lstm_bpe_model.h5\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_bpe_generated_texts = lstm_evaluation_results_datasets[\"bpe\"][\"generated\"].tolist()\n",
    "lstm_bpe_text_dataset = TextDataset(lstm_bpe_generated_texts)\n",
    "lstm_bpe_dataloader = DataLoader(\n",
    "    lstm_bpe_text_dataset, \n",
    "    batch_size=lstm_bpe_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "lstm_bpe_perplexity_scores = []\n",
    "for batch in tqdm(lstm_bpe_dataloader, desc=\"Computing Perplexity for LSTM BPE\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, lstm_bpe_tokenizer, lstm_bpe_model)\n",
    "    lstm_bpe_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "lstm_evaluation_results_datasets[\"bpe\"][\"perplexity\"] = lstm_bpe_perplexity_scores\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['bpe']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"bpe\"], f\"lstm_bpe_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.h5\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_unigram_generated_texts = lstm_evaluation_results_datasets[\"unigram\"][\"generated\"].tolist()\n",
    "lstm_unigram_text_dataset = TextDataset(lstm_unigram_generated_texts)\n",
    "lstm_unigram_dataloader = DataLoader(\n",
    "    lstm_unigram_text_dataset, \n",
    "    batch_size=lstm_unigram_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "lstm_unigram_perplexity_scores = []\n",
    "for batch in tqdm(lstm_unigram_dataloader, desc=\"Computing Perplexity for LSTM Unigram\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, lstm_unigram_tokenizer, lstm_unigram_model)\n",
    "    lstm_unigram_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "lstm_evaluation_results_datasets[\"unigram\"][\"perplexity\"] = lstm_unigram_perplexity_scores\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['unigram']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"unigram\"], f\"lstm_unigram_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"lstm_{model_name}_metrics\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"lstm_{model_name}_perplexity\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(lstm_evaluation_results_datasets[model_name], f\"lstm_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(lstm_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Implementing Multilingual Transformer Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "multilingual_model_names = {\n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"xlmr\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "multilingual_datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\",\n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\",\n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "mutlilingual_loaded_datasets = {}\n",
    "for key, file_list in multilingual_datasets.items():\n",
    "    mutlilingual_loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "multilingual_combined = pd.concat(\n",
    "    mutlilingual_loaded_datasets[\"normal\"] + \n",
    "    mutlilingual_loaded_datasets[\"nllb_back_translated\"] + \n",
    "    mutlilingual_loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "multilingual_combined = multilingual_combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Multilingual dataset length: {len(multilingual_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_models_df(multilingual_combined, \"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Load ```mBERT``` and ```XLM-R``` for Masked Language Modeling (MLM).\n",
    "MLM helps predict missing words in Burmese sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for masked\n",
    "class MaskedTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, mask_ratio=0.15, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n",
    "\n",
    "        # Tokenize and move tensors to GPU\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0).to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0).to(device)\n",
    "\n",
    "        # Apply random masking\n",
    "        seq_length = input_ids.shape[0]\n",
    "        num_to_mask = max(1, int(self.mask_ratio * (seq_length - 2)))  # Avoid CLS/SEP\n",
    "        mask_indices = torch.randperm(seq_length - 2)[:num_to_mask] + 1  # Avoid first and last token\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[mask_indices] = self.tokenizer.mask_token_id  # Replace with [MASK] token\n",
    "\n",
    "        return masked_input_ids, attention_mask, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(dataloader, model, tokenizer):\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Masked Predictions\"):\n",
    "            # Move batch data to GPU\n",
    "            masked_input_ids, attention_mask, original_input_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            # Run model inference on GPU\n",
    "            outputs = model(input_ids=masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Replace masked tokens with predicted tokens\n",
    "            predicted_tokens_batch = masked_input_ids.clone()\n",
    "            for i in range(masked_input_ids.shape[0]):  # Loop over batch\n",
    "                mask_positions = (masked_input_ids[i] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "                for pos in mask_positions:\n",
    "                    predicted_token_id = torch.argmax(outputs.logits[i, pos], dim=-1).item()\n",
    "                    predicted_tokens_batch[i, pos] = predicted_token_id\n",
    "\n",
    "            # Decode predictions\n",
    "            batch_predictions = tokenizer.batch_decode(predicted_tokens_batch.cpu(), skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both mBERT\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)\n",
    "multilingual_mbert_model = torch.compile(multilingual_mbert_model)\n",
    "multilingual_mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_mbert_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_mbert_predictions_texts = multilingual_mbert_predictions[\"burmese\"].tolist()\n",
    "multilingual_mbert_predictions_dataset = MaskedTextDataset(multilingual_mbert_predictions_texts, multilingual_mbert_tokenizer)\n",
    "multilingual_mbert_predictions_dataloader = DataLoader(\n",
    "    multilingual_mbert_predictions_dataset, \n",
    "    batch_size=multilingual_mbert_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35939d50c1ae4413b2a520d4c04ad813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Masked Predictions:   0%|          | 0/203447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run text generation\n",
    "multilingual_mbert_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_mbert_predictions_dataloader, \n",
    "    multilingual_mbert_model, \n",
    "    multilingual_mbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "      <td>စွန့်စားချင်သီတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...</td>\n",
       "      <td>သူမက စီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတဲ့ ၊ိမ်းကောင်း...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...</td>\n",
       "      <td>ထိုစုံပွဲသည် ဇင်ဘာတွေ ၏ အကြိမ်များကို ပြန်လည်ရ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...</td>\n",
       "      <td>အခွက်ထမ်းများထံ ၎ င်း ၏ သတိပြချက်များကို ရှင်း...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...   \n",
       "1  စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...   \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...   \n",
       "3  ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...   \n",
       "4  အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...   \n",
       "\n",
       "                                           generated  \n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...  \n",
       "1  စွန့်စားချင်သီတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...  \n",
       "2  သူမက စီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတဲ့ ၊ိမ်းကောင်း...  \n",
       "3  ထိုစုံပွဲသည် ဇင်ဘာတွေ ၏ အကြိမ်များကို ပြန်လည်ရ...  \n",
       "4  အခွက်ထမ်းများထံ ၎ င်း ၏ သတိပြချက်များကို ရှင်း...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display\n",
    "display(multilingual_mbert_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual mbert predictions\n",
    "save_models_df(multilingual_mbert_predictions, \"multilingual_mbert_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): XLMRobertaForMaskedLM(\n",
       "    (roberta): XLMRobertaModel(\n",
       "      (embeddings): XLMRobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): XLMRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): XLMRobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)\n",
    "multilingual_xlmr_model = torch.compile(multilingual_xlmr_model)\n",
    "multilingual_xlmr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_xlmr_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinlingual batch size\n",
    "multilingual_xlmr_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_xlmr_predictions_texts = multilingual_xlmr_predictions[\"burmese\"].tolist()\n",
    "multilingual_xlmr_predictions_dataset = MaskedTextDataset(multilingual_xlmr_predictions_texts, multilingual_xlmr_tokenizer)\n",
    "multilingual_xlmr_predictions_dataloader = DataLoader(\n",
    "    multilingual_xlmr_predictions_dataset, \n",
    "    batch_size=multilingual_xlmr_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "multilingual_xlmr_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_xlmr_predictions_dataloader, \n",
    "    multilingual_xlmr_model, \n",
    "    multilingual_xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(multilingual_xlmr_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual xlmr predictions\n",
    "save_models_df(multilingual_xlmr_predictions, \"multilingual_xlmr_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual predictions\n",
    "multilingual_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"multilingual_{model_name}_predictions\") for model_name in multilingual_model_names.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for mbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659290dc08594893a7041b7474fc6192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Computing Metrics!\n",
      "Processing Data for xlmr...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f013e1e1d74804bf2d6a7b5572e350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Computing Metrics!\n"
     ]
    }
   ],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Processing Data for {model_name}...\"),\n",
    "    compute_metrics_batch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scores for mbert:\n",
      " BLEU Score: 0.11161891030998354\n",
      " ROUGE-1 Score: 0.21893887065417755\n",
      " ROUGE-2 Score: 0.0979642909372018\n",
      " ROUGE-L Score: 0.2188918748703697\n",
      " chrF-S Score: 79.22023133392412\n",
      " BERT Score: 0.8790649549057732\n",
      "Metrics scores for xlmr:\n",
      " BLEU Score: 0.4581550312737298\n",
      " ROUGE-1 Score: 0.25353074248638974\n",
      " ROUGE-2 Score: 0.11654468643909702\n",
      " ROUGE-L Score: 0.2535243519409113\n",
      " chrF-S Score: 88.05020715366919\n",
      " BERT Score: 0.9653381555889077\n"
     ]
    }
   ],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"multilingual_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both mbert\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)\n",
    "multilingual_mbert_model = torch.compile(multilingual_mbert_model)\n",
    "multilingual_mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_mbert_generated_texts = multilingual_evaluation_results_datasets[\"mbert\"][\"generated\"].tolist()\n",
    "multilingual_mbert_text_dataset = TextDataset(multilingual_mbert_generated_texts)\n",
    "multilingual_mbert_dataloader = DataLoader(\n",
    "    multilingual_mbert_text_dataset, \n",
    "    batch_size=multilingual_mbert_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eaf3beb2914831b514b50606344ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity for mBert:   0%|          | 0/203447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 15:42:02.372000 13899 site-packages/torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "# Compute perplexity in batches\n",
    "multilingual_mbert_perplexity_scores = []\n",
    "for batch in tqdm(multilingual_mbert_dataloader, desc=\"Computing Perplexity for mBert\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, multilingual_mbert_tokenizer, multilingual_mbert_model)\n",
    "    multilingual_mbert_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 2.2175958210491413\n"
     ]
    }
   ],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"mbert\"][\"perplexity\"] = multilingual_mbert_perplexity_scores\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['mbert']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"mbert\"], f\"multilingual_mbert_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_xlmr_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)\n",
    "multilingual_xlmr_model = torch.compile(multilingual_xlmr_model)\n",
    "multilingual_xlmr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_xlmr_generated_texts = multilingual_evaluation_results_datasets[\"xlmr\"][\"generated\"].tolist()\n",
    "multilingual_xlmr_text_dataset = TextDataset(multilingual_xlmr_generated_texts)\n",
    "multilingual_xlmr_dataloader = DataLoader(\n",
    "    multilingual_xlmr_text_dataset, \n",
    "    batch_size=multilingual_xlmr_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "multilingual_xlmr_perplexity_scores = []\n",
    "for batch in tqdm(multilingual_xlmr_dataloader, desc=\"Computing Perplexity for XLM-R\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, multilingual_xlmr_tokenizer, multilingual_xlmr_model)\n",
    "    multilingual_xlmr_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"xlmr\"][\"perplexity\"] = multilingual_xlmr_perplexity_scores\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['xlmr']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"xlmr\"], f\"multilingual_xlmr_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mbert...\n",
      "Processing xlmr...\n"
     ]
    }
   ],
   "source": [
    "# combine evaluation results\n",
    "for model_name in multilingual_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"multilingual_{model_name}_metrics\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"multilingual_{model_name}_perplexity\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(multilingual_evaluation_results_datasets[model_name], f\"multilingual_{model_name}_evaluation_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")\n",
    "    print(f\" Perplexity: {dataset['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, and Perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "benchmarking_models = {\n",
    "    \"lstm\": [\n",
    "        \"bpe\", \n",
    "        \"unigram\"\n",
    "    ],\n",
    "    \"multilingual\": [\n",
    "        \"mbert\", \n",
    "        \"xlmr\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_benchmarking(key, model_name):\n",
    "    df = load_models_df(f\"{key}_{model_name}_evaluation_results\")\n",
    "\n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"bleu\", \"rouge\", \"perplexity\"]]\n",
    "\n",
    "    column_mapping = {\n",
    "        \"bleu\": f\"bleu_{model_name}\",\n",
    "        \"rouge\": f\"rouge_{model_name}\",\n",
    "        \"perplexity\": f\"perplexity_{model_name}\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "benchmarking_loaded_datasets = []\n",
    "for key, file_list in benchmarking_models.items():\n",
    "    df_list= [load_and_rename_columns_multilingual(file) for file in file_list]\n",
    "    benchmarking_loaded_datasets.append(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on 'english' using an outer join\n",
    "benchmarking_results = benchmarking_loaded_datasets[0]\n",
    "for df in benchmarking_loaded_datasets[1:]:\n",
    "    benchmarking_results = benchmarking_results.merge(df, on=\"english\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display merge dataset\n",
    "display(benchmarking_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores for Comparison\n",
    "Get mean BLEU, ROUGE, and Perplexity for LSTM (BPE & Unigram), mBERT, and XLM-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model names and their respective column prefixes\n",
    "benchmarking_model_names = [\"LSTM BPE\", \"LSTM Unigram\", \"mBERT\", \"XLM-R\"]\n",
    "benchmarking_column_prefixes = [\"bpe\", \"unigram\", \"mBERT\", \"XLM-R\"]\n",
    "\n",
    "# Compute mean scores dynamically using a dictionary comprehension\n",
    "benchmarking_mean_scores = {\n",
    "    model: {\n",
    "        \"BLEU\": df[f\"bleu_{prefix}\"].mean(),\n",
    "        \"ROUGE\": df[f\"rouge_{prefix}\"].mean(),\n",
    "        \"Perplexity\": df[f\"perplexity_{prefix}\"].mean(),\n",
    "    }\n",
    "    for model, prefix in zip(benchmarking_model_names, benchmarking_column_prefixes)\n",
    "}\n",
    "\n",
    "# Display mean scores\n",
    "pprint.pprint(benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Benchmarking Results\n",
    "Plot BLEU, ROUGE, and Perplexity scores for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each metric\n",
    "benchmarking_metrics = [\"BLEU\", \"ROUGE\", \"Perplexity\"]\n",
    "for metric in benchmarking_metrics:\n",
    "    values = [benchmarking_mean_scores[\"LSTM BPE\"][metric], \n",
    "              benchmarking_mean_scores[\"LSTM Unigram\"][metric], \n",
    "              benchmarking_mean_scores[\"mBERT\"][metric], \n",
    "              benchmarking_mean_scores[\"XLM-R\"][metric]]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(benchmarking_model_names, values, color=[\"blue\", \"green\", \"orange\", \"red\"])\n",
    "    plt.title(f\"{metric} Score Comparison\")\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Benchmarking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "save_models_df(benchmarking_results, \"benchmarking_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
