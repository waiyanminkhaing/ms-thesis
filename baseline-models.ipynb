{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 02:54:53.584486: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-31 02:54:53.600550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-31 02:54:53.625137: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-31 02:54:53.625161: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-31 02:54:53.640192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-31 02:54:54.532457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from rouge_score import rouge_scorer\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 02:54:59.522330: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:54:59.571481: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:54:59.573859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save models df\n",
    "def save_models_df(df, df_name):\n",
    "    df.to_csv(f\"models/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load spt df\n",
    "def load_spt_df(df_name):\n",
    "    return pd.read_csv(f\"spt/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load models df\n",
    "def load_models_df(df_name):\n",
    "    return pd.read_csv(f\"models/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute bleu score\n",
    "def compute_bleu(reference, prediction):\n",
    "    return sentence_bleu([reference.split()], prediction.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    return ast.literal_eval(val) if isinstance(val, str) else val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing RNN/LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing\n",
    "Load SPT-tokenized datasets, convert to sequences, and apply padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load datasets\n",
    "def get_lstm_datasets(model_name):\n",
    "    return {\n",
    "        \"normal\": [\n",
    "            f\"tokenized_{model_name}_myxnli_normalized_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_normalized_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_normalized\"\n",
    "        ],\n",
    "        \"nllb_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_nllb_back_translated_final\"\n",
    "        ],\n",
    "        \"seamless_m4t_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_1\",\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_2\",\n",
    "            f\"tokenized_{model_name}_alt_combined_seamless_m4t_back_translated_final\"\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_lstm(file_name):\n",
    "    df = load_spt_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "        \"english_back_translated_tokens\": \"english_tokens\",\n",
    "        \"burmese_translated_tokens\": \"burmese_tokens\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\", \"english_tokens\", \"burmese_tokens\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "lstm_all_datasets = {}\n",
    "for model_name in spt_models.keys():\n",
    "    datasets = get_lstm_datasets(model_name)\n",
    "\n",
    "    lstm_all_datasets[model_name] = {\n",
    "        key: [load_and_rename_columns_lstm(file) for file in file_list] for key, file_list in datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobine all datasets\n",
    "lstm_all_datasets_combined = {}\n",
    "for model_name in lstm_all_datasets.keys():\n",
    "    lstm_all_datasets_combined[model_name] = pd.concat(\n",
    "        [pd.concat(datasets) for datasets in lstm_all_datasets[model_name].values()],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name] = lstm_all_datasets_combined[model_name].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe dataset length: 1627576\n",
      "unigram dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# display of datasets\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    print(f\"{model_name} dataset length: {len(lstm_all_datasets_combined[model_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9a8580ffe4e71bf432aabe37f3e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0dfee7ebcc4bce8f4310359d590ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821c5216a41c4a1091a5814be6f84558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bdd8a4c9564f75a3c4e3d9e587ee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert tokenized sequences to lists\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq\"] = lstm_all_datasets_combined[model_name][\"english_tokens\"].progress_apply(\n",
    "        lambda x: spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq\"] = lstm_all_datasets_combined[model_name][\"burmese_tokens\"].progress_apply(\n",
    "        lambda x:  spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "lstm_max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply padding to sequences\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"english_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()\n",
    "\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"burmese_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm preprocess data\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    save_models_df(lstm_all_datasets_combined[model_name], f\"lstm_{model_name}_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model\n",
    "Define an LSTM-based sequence-to-sequence (seq2seq) model with embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_embedding_dim = 256\n",
    "lstm_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size from SentencePiece models\n",
    "lstm_vocab_sizes = {model_name: sp.GetPieceSize() for model_name, sp in spt_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build lstm model\n",
    "def build_lstm_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=lstm_embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 02:55:07.965352: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:07.968045: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:07.970038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:08.105960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:08.107210: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:08.108344: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-31 02:55:08.109419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_bpe_model = build_lstm_model(lstm_vocab_sizes[\"bpe\"])\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build lstm bpe model\n",
    "lstm_unigram_model = build_lstm_model(lstm_vocab_sizes[\"unigram\"])\n",
    "lstm_unigram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Train the model using Categorical Cross-Entropy loss & Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model train batch size\n",
    "lstm_train_batch_size = 32\n",
    "lstm_train_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_bpe_model_prefix = \"models/lstm_bpe_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_preprocessed = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bpe_preprocessed[\"burmese_seq_padded\"] = lstm_bpe_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_preprocessed[\"english_seq_padded\"] = lstm_bpe_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_bpe_X_train = np.array(lstm_bpe_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_bpe_y_train = np.array(lstm_bpe_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_bpe_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_bpe_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_bpe_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_bpe_checkpoint = ModelCheckpoint(f\"{lstm_bpe_model_prefix}.h5\", save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738293014.912576    5404 service.cc:145] XLA service 0x7f27530fb0f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738293014.912615    5404 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-01-31 03:10:14.952954: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-31 03:10:15.510909: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738293016.068799    5404 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29/45776 [..............................] - ETA: 14:22:09 - loss: 5.2780 - accuracy: 0.2249      "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe model\n",
    "lstm_bpe_model.save(lstm_bpe_model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_preprocessed = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences to numpy arrays\n",
    "lstm_unigram_X_train = np.array(lstm_unigram_preprocessed[\"burmese_seq_padded\"].tolist())\n",
    "lstm_unigram_y_train = np.array(lstm_unigram_preprocessed[\"english_seq_padded\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lstm unigram model\n",
    "lstm_unigram_model.fit(lstm_unigram_X_train, lstm_unigram_y_train, batch_size=lstm_train_batch_size, epochs=lstm_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm unigram model\n",
    "lstm_unigram_model.save(f\"models/lstm_unigram_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "Use trained LSTM models to generate translations for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size for lstm\n",
    "lstm_predictions_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate predictions\n",
    "def lstm_generate_preditions_batch(model, tokenizer, input_seqs):\n",
    "    input_seqs = np.array(input_seqs)\n",
    "\n",
    "    num_batches = int(np.ceil(len(input_seqs) / lstm_predictions_batch_size))\n",
    "    translated_texts = []\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=\"Translating in Batches\"):\n",
    "        batch_start = i * lstm_predictions_batch_size\n",
    "        batch_end = min((i + 1) * lstm_predictions_batch_size, len(input_seqs))\n",
    "        \n",
    "        batch_input = input_seqs[batch_start:batch_end]  # Extract batch\n",
    "        batch_predictions = model.predict(batch_input)  # Run model inference\n",
    "\n",
    "        # Convert predictions to text\n",
    "        batch_texts = [tokenizer.DecodeIds(pred.argmax(axis=-1).tolist()) for pred in batch_predictions]\n",
    "        translated_texts.extend(batch_texts)\n",
    "\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_predictions = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe model\n",
    "lstm_bpe_model = tf.keras.models.load_model(f\"models/lstm_bpe_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "lstm_bpe_predictions[\"generated\"] = lstm_generate_preditions_batch(\n",
    "    lstm_bpe_model, spt_models[\"bpe\"], lstm_bpe_predictions[\"burmese_seq_padded\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe predictions\n",
    "save_models_df(lstm_bpe_predictions, \"lstm_bpe_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_predictions = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram model\n",
    "lstm_unigram_model = tf.keras.models.load_model(f\"models/lstm_unigram_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "lstm_unigram_predictions[\"generated\"] = lstm_generate_preditions_batch(\n",
    "    lstm_unigram_model, spt_models[\"unigram\"], lstm_unigram_predictions[\"burmese_seq_padded\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm unigram predictions\n",
    "save_models_df(lstm_unigram_predictions, \"lstm_unigram_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model with BLEU Score\n",
    "Compute BLEU, ROUGE, and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm predictions\n",
    "lstm_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"lstm_{model_name}_predictions\") for model_name in spt_models.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute bleu score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    lstm_evaluation_results_datasets[model_name][\"bleu\"] = lstm_evaluation_results_datasets[model_name].progress_apply(\n",
    "        lambda x: compute_bleu(x[\"english\"], x[\"generated\"]), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display bleu score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"{model_name} BLEU Score: {lstm_evaluation_results_datasets[model_name]['bleu'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bleu score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    save_tmp_df(lstm_evaluation_results_datasets, f\"lstm_{model_name}_predictions_bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores\n",
    "lstm_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rouge score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge\"] = lstm_evaluation_results_datasets[model_name].progress_apply(\n",
    "        lambda x: lstm_scorer.score(x[\"english\"], x[\"generated\"])[\"rougeL\"].fmeasure, axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display rouge score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"{model_name} ROUGE Score: {lstm_evaluation_results_datasets[model_name]['rouge'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rouge score\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    save_tmp_df(lstm_evaluation_results_datasets, f\"lstm_{model_name}_predictions_rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Perplexity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm perplexity model\n",
    "lstm_perplexity_model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "lstm_perplexity_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "lstm_perplexity_model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "lstm_perplexity_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size for compute perplexity\n",
    "lstm_perplexity_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute perplexity\n",
    "def compute_perplexity_lstm_batch(texts):\n",
    "    num_batches = int(np.ceil(len(texts) / lstm_perplexity_batch_size))\n",
    "    perplexities = []\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=\"Computing Perplexity in Batches\"):\n",
    "        batch_start = i * lstm_perplexity_batch_size\n",
    "        batch_end = min((i + 1) * lstm_perplexity_batch_size, len(texts))\n",
    "\n",
    "        batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = lstm_perplexity_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        # Compute perplexity\n",
    "        with torch.no_grad():\n",
    "            outputs = lstm_perplexity_model(**inputs)\n",
    "            log_likelihood = F.log_softmax(outputs.logits, dim=-1)\n",
    "            batch_perplexity = torch.exp(-log_likelihood.mean(dim=[1, 2])).cpu().numpy()  # Move to CPU for storage\n",
    "\n",
    "        perplexities.extend(batch_perplexity)\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity\n",
    "lstm_evaluation_results_datasets[\"bpe\"][\"perplexity\"] = compute_perplexity_lstm_batch(\n",
    "    lstm_evaluation_results_datasets[\"bpe\"][\"generated\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display Perplexity\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets[model_name]['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets, f\"lstm_bpe_predictions_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity\n",
    "lstm_evaluation_results_datasets[\"unigram\"][\"perplexity\"] = compute_perplexity_lstm_batch(\n",
    "    lstm_evaluation_results_datasets[\"unigram\"][\"generated\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display Perplexity\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets[model_name]['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets, f\"lstm_unigram_predictions_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    # load bleu and set\n",
    "    bleu = load_tmp_df(f\"lstm_{model_name}_predictions_bleu\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"bleu\"] = bleu[\"bleu\"]\n",
    "\n",
    "    # load rouge and set\n",
    "    rouge = load_tmp_df(f\"lstm_{model_name}_predictions_rouge\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge\"] = rouge[\"rouge\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"lstm_{model_name}_predictions_perplexity\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"perplexity\"] = rouge[\"perplexity\"]\n",
    "\n",
    "    save_models_df(lstm_evaluation_results_datasets[model_name], f\"lstm_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(lstm_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Multilingual Transformer Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "multilingual_datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\",\n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\",\n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "mutlilingual_loaded_datasets = {}\n",
    "for key, file_list in multilingual_datasets.items():\n",
    "    mutlilingual_loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "multilingual_combined = pd.concat(\n",
    "    mutlilingual_loaded_datasets[\"normal\"] + \n",
    "    mutlilingual_loaded_datasets[\"nllb_back_translated\"] + \n",
    "    mutlilingual_loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "multilingual_combined = multilingual_combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Multilingual dataset length: {len(multilingual_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_models_df(multilingual_combined, \"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "Load ```mBERT``` and ```XLM-R``` for Masked Language Modeling (MLM).\n",
    "MLM helps predict missing words in Burmese sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "multilingual_model_names = {\n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"xlmr\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(texts, tokenizer, model):\n",
    "    # Ensure all inputs are strings, replace NaN/None with an empty string\n",
    "    valid_texts = [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
    "    \n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(valid_texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Ensure at least 3 tokens (CLS + Masked + SEP)\n",
    "    seq_lengths = inputs[\"input_ids\"].shape[1]\n",
    "    mask_indices = [\n",
    "        torch.randint(1, seq_lengths - 1, (1,)).item() if seq_lengths > 2 else None\n",
    "        for _ in valid_texts\n",
    "    ]\n",
    "\n",
    "    # Apply masking\n",
    "    for i, idx in enumerate(mask_indices):\n",
    "        if idx is not None:\n",
    "            inputs[\"input_ids\"][i, idx] = tokenizer.mask_token_id  # Replace token with [MASK]\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted tokens\n",
    "    predicted_tokens = []\n",
    "    for i, idx in enumerate(mask_indices):\n",
    "        if idx is not None:\n",
    "            predicted_token_id = torch.argmax(outputs.logits[i, idx], dim=-1).item()\n",
    "            predicted_token = tokenizer.decode([predicted_token_id])\n",
    "            predicted_tokens.append(predicted_token)\n",
    "        else:\n",
    "            predicted_tokens.append(valid_texts[i])  # Return original text if no masking was possible\n",
    "\n",
    "    # Replace [MASK] with predicted tokens\n",
    "    masked_replaced_texts = [\n",
    "        text.replace(tokenizer.mask_token, pred) if tokenizer.mask_token in text else text\n",
    "        for text, pred in zip(valid_texts, predicted_tokens)\n",
    "    ]\n",
    "\n",
    "    return masked_replaced_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinlingual batch size\n",
    "multinlingual_predictions_batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both mBERT\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_mbert_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions masks\n",
    "multilingual_mbert_predictions[\"generated\"] = [\n",
    "        pred for batch in tqdm(\n",
    "            [multilingual_mbert_predictions[\"burmese\"][i : i + multinlingual_predictions_batch_size] for i in range(0, len(multilingual_mbert_predictions), multinlingual_predictions_batch_size)],\n",
    "            desc=f\"Generating {model_name}\"\n",
    "        )\n",
    "        for pred in generate_masked_predictions_batch(batch, model_name)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display(multilingual_mbert_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction\n",
    "save_models_df(multilingual_mbert_predictions, \"multilingual_mbert_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_xlmr_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions masks\n",
    "multilingual_xlmr_predictions[\"generated\"] = [\n",
    "        pred for batch in tqdm(\n",
    "            [multilingual_xlmr_predictions[\"burmese\"][i : i + multinlingual_predictions_batch_size] for i in range(0, len(multilingual_xlmr_predictions), multinlingual_predictions_batch_size)],\n",
    "            desc=f\"Generating {model_name}\"\n",
    "        )\n",
    "        for pred in generate_masked_predictions_batch(batch, model_name)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display(multilingual_xlmr_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction\n",
    "save_models_df(multilingual_xlmr_predictions, \"multilingual_xlmr_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compare BLEU, ROUGE, and Perplexity scores between ```mBERT``` and ```XLM-R```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual predictions\n",
    "multilingual_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"multilingual_{model_name}_predictions\") for model_name in multilingual_model_names.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute BLEU score\n",
    "def compute_bleu_multilingual(reference, prediction):\n",
    "    return sentence_bleu([reference.split()], prediction.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all models in batch\n",
    "lstm_bleu_batch_size = 32\n",
    "for model_name in multilingual_model_names:\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    mBERT_XLMR_evaludation_results[f\"{model_name}_bleu\"] = [\n",
    "        compute_bleu_multilingual(row[\"english\"], row[f\"{model_name}_generated\"])\n",
    "        for _, row in tqdm(mBERT_XLMR_evaludation_results.iterrows(), total=len(mBERT_XLMR_evaludation_results), desc=f\"Computing BLEU {model_name}\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display BLEU scores\n",
    "for model_name in multilingual_model_names:\n",
    "    print(f\"{model_name} BLEU Score: {mBERT_XLMR_evaludation_results[f'{model_name}_bleu'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bleu scores\n",
    "save_tmp_df(mBERT_XLMR_evaludation_results, \"mBERT_XLMR_evaludation_results_bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE Score\n",
    "multilingual_rouge_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "for model_name in multilingual_model_names:\n",
    "    mBERT_XLMR_evaludation_results[f\"{model_name}_rouge\"] = mBERT_XLMR_evaludation_results.progress_apply(\n",
    "        lambda row: multilingual_rouge_scorer.score(row[\"english\"], row[f\"{model_name}_generated\"])[\"rougeL\"].fmeasure, axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ROUGE scores\n",
    "for model_name in multilingual_model_names:\n",
    "    print(f\"{model_name} ROUGE Score: {mBERT_XLMR_evaludation_results[f'{model_name}_rouge'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ROUGE scores\n",
    "save_tmp_df(mBERT_XLMR_evaludation_results, \"mBERT_XLMR_evaludation_results_rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Perplexity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute perplexity\n",
    "def compute_perplexity_multilingual_batch(texts, model_name):\n",
    "    tokenizer = multilingual_tokenizers[model_name]\n",
    "    model = multilingual_models[model_name].to(device)\n",
    "\n",
    "    # Ensure all inputs are valid strings and replace NaN/None\n",
    "    valid_texts = [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(valid_texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Run the model in batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Compute log-likelihood\n",
    "    log_likelihood = F.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Compute Perplexity for each sentence in batch\n",
    "    perplexities = torch.exp(-log_likelihood.mean(dim=(1, 2))).tolist()\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_perplexity_batch_size = 32\n",
    "\n",
    "for model_name in multilingual_model_names:\n",
    "    print(f\"Computing Perplexity for {model_name}...\")\n",
    "\n",
    "    # Process in batches\n",
    "    perplexity_scores = []\n",
    "    for batch in tqdm(\n",
    "        [mBERT_XLMR_evaludation_results[f\"{model_name}_generated\"][i : i + multilingual_perplexity_batch_size].dropna().tolist()\n",
    "         for i in range(0, len(mBERT_XLMR_evaludation_results), multilingual_perplexity_batch_size)\n",
    "        ],\n",
    "        desc=f\"Perplexity {model_name}\"\n",
    "    ):\n",
    "        perplexity_scores.extend(compute_perplexity_multilingual_batch(batch, model_name))\n",
    "\n",
    "    # Store perplexity scores in DataFrame\n",
    "    mBERT_XLMR_evaludation_results[f\"{model_name}_perplexity\"] = perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Perplexity scores\n",
    "for model_name in multilingual_model_names:\n",
    "    print(f\"{model_name} BLEU Score: {mBERT_XLMR_evaludation_results[f'{model_name}_perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ROUGE scores\n",
    "save_tmp_df(mBERT_XLMR_evaludation_results, \"mBERT_XLMR_evaludation_results_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "save_models_df(mBERT_XLMR_evaludation_results, \"mBERT_XLMR_evaludation_results\")\n",
    "print(\"Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Benchmarking and Analysis\n",
    "Compare the performance of LSTM, mBERT, and XLM-R using BLEU, ROUGE, and Perplexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
