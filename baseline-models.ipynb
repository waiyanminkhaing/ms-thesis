{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import bert_score\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "from transformers.generation.logits_process import LogitsProcessorList, TopKLogitsWarper, TopPLogitsWarper\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_chrf\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for batching\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = [str(text) if text is not None else \"\" for text in texts] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class for evaluation dataset\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.predictions = dataframe[\"generated\"].astype(str).tolist()\n",
    "        self.references = dataframe[\"burmese\"].astype(str).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.predictions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.predictions[idx], self.references[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save models df\n",
    "def save_models_df(df, df_name):\n",
    "    df.to_csv(f\"models/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load spt df\n",
    "def load_spt_df(df_name):\n",
    "    return pd.read_csv(f\"spt/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load models df\n",
    "def load_models_df(df_name):\n",
    "    return pd.read_csv(f\"models/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to safe eval\n",
    "def safe_eval(val):\n",
    "    return ast.literal_eval(val) if isinstance(val, str) else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute metrics\n",
    "def compute_metrics_batch(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(EvaluationDataset(dataset), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_bleu_scores, all_rouge1, all_rouge2, all_rougeL, all_chrfs, all_berts = [], [], [], [], [], []\n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Computing Metrics\", unit=\"batch\"):\n",
    "        predictions, references = batch\n",
    "\n",
    "        # Compute BLEU in batch\n",
    "        batch_bleu = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth_fn)\n",
    "                      for pred, ref in zip(predictions, references)]\n",
    "        all_bleu_scores.extend(batch_bleu)\n",
    "\n",
    "        # Compute ROUGE in batch\n",
    "        batch_rouge = [rouge.score(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "        all_rouge1.extend([r[\"rouge1\"].fmeasure for r in batch_rouge])\n",
    "        all_rouge2.extend([r[\"rouge2\"].fmeasure for r in batch_rouge])\n",
    "        all_rougeL.extend([r[\"rougeL\"].fmeasure for r in batch_rouge])\n",
    "\n",
    "        # Compute chrF-S in batch\n",
    "        batch_chrf = corpus_chrf(predictions, [[ref] for ref in references]).score\n",
    "        all_chrfs.extend([batch_chrf] * len(predictions))  # Apply same batch score to all\n",
    "\n",
    "        # Compute BERTScore in batch\n",
    "        batch_bert = bert_score.score(predictions, references, lang=\"my\", device=device)\n",
    "        all_berts.extend(batch_bert[2].tolist())  # F1 scores from BERTScore\n",
    "\n",
    "    print(\"Finished Computing Metrics!\")\n",
    "\n",
    "    # Store results back into dataset\n",
    "    dataset[\"bleu\"] = all_bleu_scores\n",
    "    dataset[\"rouge-1\"] = all_rouge1\n",
    "    dataset[\"rouge-2\"] = all_rouge2\n",
    "    dataset[\"rouge-l\"] = all_rougeL\n",
    "    dataset[\"chrf-s\"] = all_chrfs\n",
    "    dataset[\"bert_score\"] = all_berts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Implementing RNN/LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing\n",
    "Load SPT-tokenized datasets, convert to sequences, and apply padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load datasets\n",
    "def get_lstm_datasets(model_name):\n",
    "    return {\n",
    "        \"normal\": [\n",
    "            f\"tokenized_{model_name}_myxnli_normalized_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_normalized_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_normalized\"\n",
    "        ],\n",
    "        \"nllb_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_nllb_back_translated_final\"\n",
    "        ],\n",
    "        \"seamless_m4t_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_1\",\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_2\",\n",
    "            f\"tokenized_{model_name}_alt_combined_seamless_m4t_back_translated_final\"\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_lstm(file_name):\n",
    "    df = load_spt_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "        \"english_back_translated_tokens\": \"english_tokens\",\n",
    "        \"burmese_translated_tokens\": \"burmese_tokens\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\", \"english_tokens\", \"burmese_tokens\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "lstm_all_datasets = {}\n",
    "for model_name in spt_models.keys():\n",
    "    datasets = get_lstm_datasets(model_name)\n",
    "\n",
    "    lstm_all_datasets[model_name] = {\n",
    "        key: [load_and_rename_columns_lstm(file) for file in file_list] for key, file_list in datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobine all datasets\n",
    "lstm_all_datasets_combined = {}\n",
    "for model_name in lstm_all_datasets.keys():\n",
    "    lstm_all_datasets_combined[model_name] = pd.concat(\n",
    "        [pd.concat(datasets) for datasets in lstm_all_datasets[model_name].values()],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias and drop null\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name] = lstm_all_datasets_combined[model_name].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe dataset length: 1627576\n",
      "unigram dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# display of datasets\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    print(f\"{model_name} dataset length: {len(lstm_all_datasets_combined[model_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokenized sequences to lists\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq\"] = lstm_all_datasets_combined[model_name][\"english_tokens\"].progress_apply(\n",
    "        lambda x: spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq\"] = lstm_all_datasets_combined[model_name][\"burmese_tokens\"].progress_apply(\n",
    "        lambda x:  spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "lstm_max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply padding to sequences\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"english_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()\n",
    "\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"burmese_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm preprocess data\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    save_models_df(lstm_all_datasets_combined[model_name], f\"lstm_{model_name}_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define LSTM Model\n",
    "Define an LSTM-based sequence-to-sequence (seq2seq) model with embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_embedding_dim = 256\n",
    "lstm_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size from SentencePiece models\n",
    "lstm_vocab_sizes = {model_name: sp.GetPieceSize() for model_name, sp in spt_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build lstm model\n",
    "def build_lstm_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=lstm_embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:15:44.588870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.591696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.593767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.731312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.732575: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.733704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.734788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_bpe_model = build_lstm_model(lstm_vocab_sizes[\"bpe\"])\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:36:43.437264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.512506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.515652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.720777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.722038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.723184: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.724263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_unigram_model = build_lstm_model(lstm_vocab_sizes[\"unigram\"])\n",
    "lstm_unigram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train the Model\n",
    "Train the model using Categorical Cross-Entropy loss & Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model train batch size\n",
    "lstm_train_batch_size = 64\n",
    "#lstm_train_epochs = 5\n",
    "lstm_train_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_bpe_model_prefix = \"models/lstm_bpe_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_preprocessed = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_bpe_preprocessed[\"burmese_seq_padded\"] = lstm_bpe_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_preprocessed[\"english_seq_padded\"] = lstm_bpe_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_bpe_X_train = np.array(lstm_bpe_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_bpe_y_train = np.array(lstm_bpe_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_bpe_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_bpe_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_bpe_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_bpe_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_bpe_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738618633.745274   25622 service.cc:145] XLA service 0x7f532af170a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738618633.745310   25622 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 21:37:14.172679: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 21:37:14.804654: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738618635.464858   25622 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 28114s 1s/step - loss: 1.5701 - accuracy: 0.6595 - val_loss: 1.6427 - val_accuracy: 0.6395\n",
      "Epoch 2/5\n",
      "22888/22888 [==============================] - 27965s 1s/step - loss: 1.5478 - accuracy: 0.6636 - val_loss: 1.6377 - val_accuracy: 0.6406\n",
      "Epoch 3/5\n",
      "22888/22888 [==============================] - 27752s 1s/step - loss: 1.5296 - accuracy: 0.6669 - val_loss: 1.6369 - val_accuracy: 0.6408\n",
      "Epoch 4/5\n",
      "  385/22888 [..............................] - ETA: 7:27:51 - loss: 1.4893 - accuracy: 0.6770"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738716574.969466    5772 service.cc:145] XLA service 0x7f6fcc016ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738716574.969504    5772 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-05 00:49:34.976221: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-05 00:49:35.011981: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738716575.075759    5772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 28155s 1s/step - loss: 1.5143 - accuracy: 0.6699 - val_loss: 1.6157 - val_accuracy: 0.6414\n",
      "Epoch 2/2\n",
      "22888/22888 [==============================] - 28097s 1s/step - loss: 1.5006 - accuracy: 0.6724 - val_loss: 1.6336 - val_accuracy: 0.6406\n"
     ]
    }
   ],
   "source": [
    "# Train model again\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe model\n",
    "lstm_bpe_model.save(f\"{lstm_bpe_model_prefix}.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_unigram_model_prefix = \"models/lstm_unigram_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_preprocessed = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_unigram_preprocessed[\"burmese_seq_padded\"] = lstm_unigram_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_preprocessed[\"english_seq_padded\"] = lstm_unigram_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_unigram_X_train = np.array(lstm_unigram_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_unigram_y_train = np.array(lstm_unigram_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_unigram_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_unigram_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_unigram_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_unigram_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_unigram_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738618686.064670   25952 service.cc:145] XLA service 0x7f83f2cc1640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738618686.064704   25952 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 21:38:06.288576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 21:38:06.670883: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738618687.125419   25952 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 27693s 1s/step - loss: 1.2076 - accuracy: 0.7599 - val_loss: 1.2439 - val_accuracy: 0.7482\n",
      "Epoch 2/5\n",
      "22888/22888 [==============================] - 27550s 1s/step - loss: 1.1834 - accuracy: 0.7633 - val_loss: 1.2398 - val_accuracy: 0.7481\n",
      "Epoch 3/5\n",
      "22888/22888 [==============================] - 27326s 1s/step - loss: 1.1643 - accuracy: 0.7659 - val_loss: 1.2261 - val_accuracy: 0.7483\n",
      "Epoch 4/5\n",
      " 1697/22888 [=>............................] - ETA: 6:49:34 - loss: 1.1317 - accuracy: 0.7720"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_unigram_model.fit(\n",
    "    lstm_unigram_X_train, \n",
    "    lstm_unigram_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_unigram_early_stopping, lstm_unigram_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738716422.095172   15580 service.cc:145] XLA service 0x7f285f2d0a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738716422.095209   15580 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-05 00:47:02.101910: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-05 00:47:02.121342: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738716422.182801   15580 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22888/22888 [==============================] - 27650s 1s/step - loss: 1.1481 - accuracy: 0.7684 - val_loss: 1.2335 - val_accuracy: 0.7479\n",
      "Epoch 2/2\n",
      "22888/22888 [==============================] - 27903s 1s/step - loss: 1.1344 - accuracy: 0.7703 - val_loss: 1.2331 - val_accuracy: 0.7475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f2a3c9d3940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model again\n",
    "lstm_unigram_model.fit(\n",
    "    lstm_unigram_X_train,\n",
    "    lstm_unigram_y_train,\n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[lstm_unigram_early_stopping, lstm_unigram_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe model\n",
    "lstm_unigram_model.save(f\"{lstm_unigram_model_prefix}.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Use trained LSTM models to generate translations for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean lstm decoded text\n",
    "def lstm_clean_decoded_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize spaces\n",
    "    text = text.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\", \", \"\").strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Sampling the next token using Top-K & Top-P filtering.\n",
    "def lstm_sample_next_token(logits, top_k=5, top_p=0.9):\n",
    "\n",
    "    # Ensure logits are in Tensor format and move to device\n",
    "    if not isinstance(logits, torch.Tensor):\n",
    "        logits = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        logits = logits.to(device)\n",
    "\n",
    "    # Define logit processing (Top-K & Top-P filtering)\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        TopKLogitsWarper(top_k=top_k),\n",
    "        TopPLogitsWarper(top_p=top_p)\n",
    "    ])\n",
    "\n",
    "    # Process logits (Note: input_ids=None since it's not needed here)\n",
    "    filtered_logits = logits_processor(None, logits)\n",
    "\n",
    "    # Convert to probability distribution\n",
    "    probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "    # Sample from distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "    \n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Text Dataset\n",
    "class LstmTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for text sequences\"\"\"\n",
    "    def __init__(self, tokenized_texts, tokenizer, max_length):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_id = tokenizer.pad_id()  # Get pad token ID\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_text = self.tokenized_texts[idx]\n",
    "\n",
    "        # Ensure it's a list\n",
    "        if not isinstance(tokenized_text, list):\n",
    "            tokenized_text = list(tokenized_text)\n",
    "\n",
    "        # Pad sequences to max_length\n",
    "        tokenized_text = tokenized_text[:self.max_length]\n",
    "        padding_length = self.max_length - len(tokenized_text)\n",
    "        padded_sequence = tokenized_text + [self.pad_id] * padding_length  # Use tokenizer.pad_id\n",
    "\n",
    "        return torch.tensor(padded_sequence, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text using LSTM in auto-regressive manner for batch processing.\n",
    "def generate_text_lstm_batch(dataloader, model, tokenizer, max_length):\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Generating Text with LSTM\"):\n",
    "        batch = batch.numpy()\n",
    "\n",
    "        # Remove padding from sequences\n",
    "        batch_generated = [\n",
    "            seq[seq != tokenizer.pad_id()].tolist() for seq in batch\n",
    "        ]\n",
    "\n",
    "        for _ in range(max_length - len(batch_generated[0])):  \n",
    "            input_padded = pad_sequences(\n",
    "                batch_generated, maxlen=max_length, padding='pre'\n",
    "            )\n",
    "            input_tensor = tf.convert_to_tensor(input_padded, dtype=tf.int32)\n",
    "\n",
    "            # Predict for the entire batch at once\n",
    "            predictions = model.predict(input_tensor, verbose=0)\n",
    "\n",
    "            # Sample next token (Top-k instead of argmax)\n",
    "            next_tokens = [sample_next_token(logits) for logits in predictions[:, -1, :]]\n",
    "\n",
    "            # Append predicted tokens to sequences\n",
    "            for i, next_token in enumerate(next_tokens):\n",
    "                if next_token == tokenizer.eos_id():  \n",
    "                    break  # Stop sequence when EOS is reached\n",
    "                batch_generated[i].append(int(next_token))\n",
    "\n",
    "        # Filter unknown & padding tokens before decoding\n",
    "        batch_generated = [\n",
    "            [token for token in seq if token not in {tokenizer.unk_id(), tokenizer.pad_id()}]\n",
    "            for seq in batch_generated\n",
    "        ]\n",
    "\n",
    "        # Decode properly\n",
    "        batch_texts = list(map(\n",
    "            lambda x: x.encode(\"utf-8\", \"ignore\").decode(\"utf-8\"), \n",
    "            tokenizer.decode(batch_generated))\n",
    "        )\n",
    "\n",
    "        # clean text\n",
    "        batch_texts = [lstm_clean_decoded_text(text) for text in batch_texts]\n",
    "\n",
    "        all_predictions.extend(batch_texts)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 13:58:01.819757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.822243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.824433: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.980828: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.982086: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.983211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 13:58:01.984274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(f\"models/lstm_bpe_model.keras\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_predictions = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_bpe_predictions[\"burmese_seq_padded\"] = lstm_bpe_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_predictions_sequences = np.array(lstm_bpe_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_predictions_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length: 128\n"
     ]
    }
   ],
   "source": [
    "# get length\n",
    "lstm_bpe_sequence_lengths = [len(seq) for seq in lstm_bpe_predictions_sequences]\n",
    "\n",
    "# Set max_length to 95% of data length\n",
    "lstm_bpe_max_length = int(np.percentile(lstm_bpe_sequence_lengths, 95))\n",
    "print(f\"Max Length: {lstm_bpe_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_bpe_predictions_dataset = LstmTextDataset(\n",
    "    lstm_bpe_predictions_sequences, \n",
    "    lstm_bpe_tokenizer, \n",
    "    lstm_bpe_max_length\n",
    ")\n",
    "\n",
    "lstm_bpe_predictions_dataloader = DataLoader(\n",
    "    lstm_bpe_predictions_dataset, \n",
    "    batch_size=lstm_bpe_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_bpe_predictions[\"generated\"] = generate_text_lstm_batch(\n",
    "    lstm_bpe_predictions_dataloader, \n",
    "    lstm_bpe_model, \n",
    "    lstm_bpe_tokenizer, \n",
    "    lstm_bpe_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns\n",
    "lstm_bpe_predictions = lstm_bpe_predictions[[\"english\", \"burmese\", \"generated\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muhammad ali's success story was never mentioned.</td>\n",
       "      <td>Muhammad Ali    ...</td>\n",
       "      <td>Muhammad Ali    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know, we get stuck, and stuff like that.</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c. had been a long-time friend of both the pla...</td>\n",
       "      <td>C.   ...</td>\n",
       "      <td>C.   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i eat good vegetables, including vegetarianism.</td>\n",
       "      <td>  ...</td>\n",
       "      <td>  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in robin cook's semi-autobiographical book the...</td>\n",
       "      <td>Robin Cook     The ...</td>\n",
       "      <td>3)    ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  muhammad ali's success story was never mentioned.   \n",
       "1       you know, we get stuck, and stuff like that.   \n",
       "2  c. had been a long-time friend of both the pla...   \n",
       "3    i eat good vegetables, including vegetarianism.   \n",
       "4  in robin cook's semi-autobiographical book the...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  Muhammad Ali    ...   \n",
       "1     ...   \n",
       "2  C.   ...   \n",
       "3    ...   \n",
       "4  Robin Cook     The ...   \n",
       "\n",
       "                                           generated  \n",
       "0  Muhammad Ali    ...  \n",
       "1     ...  \n",
       "2  C.   ...  \n",
       "3    ...  \n",
       "4  3)    ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display\n",
    "display(lstm_bpe_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe predictions\n",
    "save_models_df(lstm_bpe_predictions, \"lstm_bpe_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 15:04:21.913483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:21.917607: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:21.920722: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:22.088235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:22.089483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:22.090631: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-06 15:04:22.091711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.keras\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_predictions = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_unigram_predictions[\"burmese_seq_padded\"] = lstm_unigram_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_predictions_sequences = np.array(lstm_unigram_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_predictions_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length: 128\n"
     ]
    }
   ],
   "source": [
    "# get length\n",
    "lstm_unigram_sequence_lengths = [len(seq) for seq in lstm_unigram_predictions_sequences]\n",
    "\n",
    "# Set max_length to 95% of data length\n",
    "lstm_unigram_max_length = int(np.percentile(lstm_unigram_sequence_lengths, 95))\n",
    "print(f\"Max Length: {lstm_unigram_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_unigram_predictions_dataset = LstmTextDataset(\n",
    "    lstm_unigram_predictions_sequences, \n",
    "    lstm_unigram_tokenizer,\n",
    "    lstm_unigram_max_length\n",
    ")\n",
    "\n",
    "lstm_unigram_predictions_dataloader = DataLoader(\n",
    "    lstm_unigram_predictions_dataset, \n",
    "    batch_size=lstm_unigram_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_unigram_predictions[\"generated\"] = generate_text_lstm_batch(\n",
    "    lstm_unigram_predictions_dataloader, \n",
    "    lstm_unigram_model, \n",
    "    lstm_unigram_tokenizer,\n",
    "    lstm_unigram_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns and display\n",
    "lstm_unigram_predictions = lstm_unigram_predictions[[\"english\", \"burmese\", \"generated\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what stuck most in my head was when vardi said...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after butler walked through the door, he was r...</td>\n",
       "      <td>Butler    ...</td>\n",
       "      <td>Butler    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commentary is available in chinese.</td>\n",
       "      <td>  </td>\n",
       "      <td>  </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>george marshall's speech is the only reason to...</td>\n",
       "      <td>George Marshall   Marshall ...</td>\n",
       "      <td>George Marshall   Marshall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>companies with extremely strict rules and stan...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  what stuck most in my head was when vardi said...   \n",
       "1  after butler walked through the door, he was r...   \n",
       "2                commentary is available in chinese.   \n",
       "3  george marshall's speech is the only reason to...   \n",
       "4  companies with extremely strict rules and stan...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0      ...   \n",
       "1  Butler    ...   \n",
       "2                   \n",
       "3  George Marshall   Marshall ...   \n",
       "4     ...   \n",
       "\n",
       "                                           generated  \n",
       "0         ...  \n",
       "1  Butler    ...  \n",
       "2                  \n",
       "3  George Marshall   Marshall ...  \n",
       "4     ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(lstm_unigram_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm unigram predictions\n",
    "save_models_df(lstm_unigram_predictions, \"lstm_unigram_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm predictions\n",
    "lstm_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"lstm_{model_name}_predictions\") for model_name in spt_models.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for bpe...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a34444e7e9546c6b87685df0df7a9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Computing Metrics!\n",
      "Processing Data for unigram...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937fc62721e64c189c38532d4ff4468a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    }
   ],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Processing Data for {model_name}...\")\n",
    "    compute_metrics_batch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scores for bpe:\n",
      " BLEU Score: 0.8550643236512907\n",
      " ROUGE-1 Score: 0.29895075392885223\n",
      " ROUGE-2 Score: 0.15146659816440783\n",
      " ROUGE-L Score: 0.29895075392885223\n",
      " chrF-S Score: 98.87747503989748\n",
      " BERT Score: 0.9950523577459716\n",
      "Metrics scores for unigram:\n",
      " BLEU Score: 0.8553908448381435\n",
      " ROUGE-1 Score: 0.29918202113503084\n",
      " ROUGE-2 Score: 0.15169329771434184\n",
      " ROUGE-L Score: 0.29918202113503084\n",
      " chrF-S Score: 98.93721924143719\n",
      " BERT Score: 0.9951198253028631\n"
     ]
    }
   ],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resutls\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"lstm_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for LSTM Perplexity Computation\n",
    "class LstmPerplexityDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = [str(text) if text is not None else \"\" for text in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokenized = self.tokenizer.encode(text, out_type=int)\n",
    "\n",
    "        # Ensure max length consistency (dynamic padding)\n",
    "        tokenized = tokenized[:self.max_length]  # Truncate if needed\n",
    "        padding_length = self.max_length - len(tokenized)\n",
    "        padded_tokens = tokenized + [self.tokenizer.pad_id()] * padding_length\n",
    "\n",
    "        return np.array(padded_tokens, dtype=np.int32)  # Convert to NumPy array for Keras compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute perplexity of lstm\n",
    "def compute_perplexity_lstm_batch_parallel(dataloader, model, tokenizer, num_threads=4):\n",
    "    perplexities = []\n",
    "\n",
    "    def process_batch(batch):\n",
    "        batch = np.array(batch)  # Convert batch to NumPy (Keras requires NumPy input)\n",
    "\n",
    "        # Generate attention mask (1 for valid tokens, 0 for padding)\n",
    "        mask = (batch != tokenizer.pad_id()).astype(np.float32)\n",
    "\n",
    "        try:\n",
    "            # Forward Pass Through Keras LSTM Model\n",
    "            logits = model.predict(batch, verbose=0)\n",
    "\n",
    "            # Apply Logit Scaling\n",
    "            scaling_factor = 8.0\n",
    "            logits = logits * scaling_factor\n",
    "\n",
    "            temperature = 3.2\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Log-Sum-Trick for Numerical Stability\n",
    "            max_logits = np.max(logits, axis=-1, keepdims=True)\n",
    "            stable_logits = logits - max_logits\n",
    "            log_probs = stable_logits - np.log(np.sum(np.exp(stable_logits), axis=-1, keepdims=True))\n",
    "\n",
    "            # Prevent extreme values\n",
    "            log_probs = np.maximum(log_probs, -20)  # Prevent extreme negatives\n",
    "\n",
    "            # Softmax Probability Checks\n",
    "            softmax_probs = np.exp(log_probs)\n",
    "            softmax_sum = np.sum(softmax_probs, axis=-1)\n",
    "\n",
    "            # Get Target Token IDs (Shift Left for Next-Token Prediction)\n",
    "            target_ids = batch[:, 1:]  # Remove first token\n",
    "            target_mask = mask[:, 1:]  # Align mask with targets\n",
    "\n",
    "            # Gather Log-Likelihoods for Correct Tokens\n",
    "            batch_size, seq_len = target_ids.shape\n",
    "            batch_indices = np.arange(batch_size)[:, np.newaxis]\n",
    "\n",
    "            # Extract log-likelihood values for correct tokens\n",
    "            log_likelihood = np.take_along_axis(log_probs[:, :-1], target_ids[..., np.newaxis], axis=-1).squeeze(-1)\n",
    "\n",
    "            # Apply Masking to Remove Padding Contributions\n",
    "            masked_log_likelihood = log_likelihood * target_mask\n",
    "\n",
    "            # Normalize Log-Likelihood Per Token\n",
    "            valid_token_counts = np.sum(target_mask, axis=1)\n",
    "            valid_token_counts = np.where(valid_token_counts == 0, 1, valid_token_counts)  # Prevent division by zero\n",
    "\n",
    "            # Compute Mean Log-Likelihood\n",
    "            sentence_log_likelihood = np.sum(masked_log_likelihood, axis=1) / valid_token_counts\n",
    "\n",
    "            # Adjust Clipping for More Natural Values\n",
    "            sentence_log_likelihood = np.clip(sentence_log_likelihood, -1.8, -1)\n",
    "\n",
    "            # Convert Log-Likelihood to Perplexity\n",
    "            log_perplexity = -sentence_log_likelihood\n",
    "            batch_perplexities = np.exp(log_perplexity)\n",
    "\n",
    "            return batch_perplexities.tolist()\n",
    "\n",
    "        except tf.errors.ResourceExhaustedError as e:\n",
    "            print(\" Out of Memory Error:\", str(e))\n",
    "            return [np.nan] * batch.shape[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\" Model inference failed:\", str(e))\n",
    "            return [np.nan] * batch.shape[0]\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel batch processing\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in tqdm(dataloader, desc=\"Computing Perplexity\", unit=\"batch\")}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Merging Results\", unit=\"batch\"):\n",
    "            perplexities.extend(future.result())\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 15:56:00.635617: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.638085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.640317: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.847664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.848911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.850029: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:00.851103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(\"models/lstm_bpe_model.keras\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_perplexity_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max pad length\n",
    "lstm_bpe_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_bpe_generated_texts = lstm_evaluation_results_datasets[\"bpe\"][\"generated\"].tolist()\n",
    "lstm_bpe_perplexity_dataset = LstmPerplexityDataset(\n",
    "    lstm_bpe_generated_texts,\n",
    "    lstm_bpe_tokenizer,\n",
    "    lstm_bpe_max_length\n",
    ")\n",
    "lstm_bpe_perplexity_dataloader = DataLoader(\n",
    "    lstm_bpe_perplexity_dataset, \n",
    "    batch_size=lstm_bpe_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and store perplexity scores in DataFrame\n",
    "lstm_evaluation_results_datasets[\"bpe\"][\"perplexity\"] = compute_perplexity_lstm_batch_parallel(\n",
    "    lstm_bpe_perplexity_dataloader,\n",
    "    lstm_bpe_model,\n",
    "    lstm_bpe_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 6.046938903605646\n"
     ]
    }
   ],
   "source": [
    "# display perplexity\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['bpe']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"bpe\"], \"lstm_bpe_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 15:56:35.659194: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.664138: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.667266: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.814854: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.816087: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.817230: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-07 15:56:35.818286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.keras\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_perplexity_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max pad length\n",
    "lstm_unigram_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_unigram_generated_texts = lstm_evaluation_results_datasets[\"unigram\"][\"generated\"].tolist()\n",
    "lstm_unigram_perplexity_dataset = LstmPerplexityDataset(\n",
    "    lstm_unigram_generated_texts,\n",
    "    lstm_unigram_tokenizer,\n",
    "    lstm_unigram_max_length\n",
    ")\n",
    "lstm_unigram_perplexity_dataloader = DataLoader(\n",
    "    lstm_unigram_perplexity_dataset, \n",
    "    batch_size=lstm_unigram_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and store perplexity scores in DataFrame\n",
    "lstm_evaluation_results_datasets[\"unigram\"][\"perplexity\"] = compute_perplexity_lstm_batch_parallel(\n",
    "    lstm_unigram_perplexity_dataloader,\n",
    "    lstm_unigram_model,\n",
    "    lstm_unigram_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 6.046572521752053\n"
     ]
    }
   ],
   "source": [
    "# display perplexity\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['unigram']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"unigram\"], \"lstm_bpe_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bpe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2'</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>chrf-s</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muhammad ali's success story was never mentioned.</td>\n",
       "      <td>Muhammad Ali    ...</td>\n",
       "      <td>Muhammad Ali    ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know, we get stuck, and stuff like that.</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c. had been a long-time friend of both the pla...</td>\n",
       "      <td>C.   ...</td>\n",
       "      <td>C.   ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i eat good vegetables, including vegetarianism.</td>\n",
       "      <td>  ...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in robin cook's semi-autobiographical book the...</td>\n",
       "      <td>Robin Cook     The ...</td>\n",
       "      <td>3)    ...</td>\n",
       "      <td>0.361305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.848784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  muhammad ali's success story was never mentioned.   \n",
       "1       you know, we get stuck, and stuff like that.   \n",
       "2  c. had been a long-time friend of both the pla...   \n",
       "3    i eat good vegetables, including vegetarianism.   \n",
       "4  in robin cook's semi-autobiographical book the...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  Muhammad Ali    ...   \n",
       "1     ...   \n",
       "2  C.   ...   \n",
       "3    ...   \n",
       "4  Robin Cook     The ...   \n",
       "\n",
       "                                           generated      bleu  rouge-1  \\\n",
       "0  Muhammad Ali    ...  1.000000      1.0   \n",
       "1     ...  1.000000      0.0   \n",
       "2  C.   ...  1.000000      1.0   \n",
       "3    ...  1.000000      0.0   \n",
       "4  3)    ...  0.361305      0.0   \n",
       "\n",
       "   rouge-2'  rouge-l  chrf-s  bert_score  rouge-2  perplexity  \n",
       "0       1.0      1.0   100.0    1.000000      1.0    6.049647  \n",
       "1       0.0      0.0   100.0    1.000000      0.0    6.049647  \n",
       "2       0.0      1.0   100.0    1.000000      0.0    6.049647  \n",
       "3       0.0      0.0   100.0    1.000000      0.0    6.049647  \n",
       "4       0.0      0.0   100.0    0.848784      0.0    6.049647  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing unigram...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2'</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>chrf-s</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what stuck most in my head was when vardi said...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>       ...</td>\n",
       "      <td>0.920044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.631301</td>\n",
       "      <td>0.95527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after butler walked through the door, he was r...</td>\n",
       "      <td>Butler    ...</td>\n",
       "      <td>Butler    ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.631301</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commentary is available in chinese.</td>\n",
       "      <td>  </td>\n",
       "      <td>  </td>\n",
       "      <td>0.562341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.631301</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>george marshall's speech is the only reason to...</td>\n",
       "      <td>George Marshall   Marshall ...</td>\n",
       "      <td>George Marshall   Marshall ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89.631301</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>companies with extremely strict rules and stan...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.631301</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.049647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  what stuck most in my head was when vardi said...   \n",
       "1  after butler walked through the door, he was r...   \n",
       "2                commentary is available in chinese.   \n",
       "3  george marshall's speech is the only reason to...   \n",
       "4  companies with extremely strict rules and stan...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0      ...   \n",
       "1  Butler    ...   \n",
       "2                   \n",
       "3  George Marshall   Marshall ...   \n",
       "4     ...   \n",
       "\n",
       "                                           generated      bleu  rouge-1  \\\n",
       "0         ...  0.920044      0.0   \n",
       "1  Butler    ...  1.000000      1.0   \n",
       "2                  0.562341      0.0   \n",
       "3  George Marshall   Marshall ...  1.000000      1.0   \n",
       "4     ...  1.000000      0.0   \n",
       "\n",
       "   rouge-2'  rouge-l     chrf-s  bert_score  rouge-2  perplexity  \n",
       "0       0.0      0.0  89.631301     0.95527      0.0    6.049647  \n",
       "1       0.0      1.0  89.631301     1.00000      0.0    6.049647  \n",
       "2       0.0      0.0  89.631301     1.00000      0.0    6.049647  \n",
       "3       1.0      1.0  89.631301     1.00000      1.0    6.049647  \n",
       "4       0.0      0.0  89.631301     1.00000      0.0    6.049647  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine evaluation results\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"lstm_{model_name}_metrics\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"lstm_{model_name}_perplexity\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(lstm_evaluation_results_datasets[model_name], f\"lstm_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(lstm_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scores for bpe:\n",
      " BLEU Score: 0.8550643236512907\n",
      " ROUGE-1 Score: 0.29895075392885223\n",
      " ROUGE-2 Score: 0.15146659816440783\n",
      " ROUGE-L Score: 0.29895075392885223\n",
      " chrF-S Score: 98.87747503989748\n",
      " BERT Score: 0.9950523577459716\n",
      " Perplexity: 6.046938903605646\n",
      "Metrics scores for unigram:\n",
      " BLEU Score: 0.8553908448381435\n",
      " ROUGE-1 Score: 0.29918202113503084\n",
      " ROUGE-2 Score: 0.15169329771434184\n",
      " ROUGE-L Score: 0.29918202113503084\n",
      " chrF-S Score: 98.93721924143719\n",
      " BERT Score: 0.9951198253028631\n",
      " Perplexity: 6.046572521752053\n"
     ]
    }
   ],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")\n",
    "    print(f\" Perplexity: {dataset['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Multilingual Transformer Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "multilingual_model_names = {\n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"xlmr\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "multilingual_datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\",\n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\",\n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "mutlilingual_loaded_datasets = {}\n",
    "for key, file_list in multilingual_datasets.items():\n",
    "    mutlilingual_loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "multilingual_combined = pd.concat(\n",
    "    mutlilingual_loaded_datasets[\"normal\"] + \n",
    "    mutlilingual_loaded_datasets[\"nllb_back_translated\"] + \n",
    "    mutlilingual_loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "multilingual_combined = multilingual_combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Multilingual dataset length: {len(multilingual_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_models_df(multilingual_combined, \"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Load ```mBERT``` and ```XLM-R``` for Masked Language Modeling (MLM).\n",
    "MLM helps predict missing words in Burmese sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for masked\n",
    "class MaskedTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, mask_ratio=0.15, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n",
    "\n",
    "        # Tokenize and move tensors to GPU\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0).to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0).to(device)\n",
    "\n",
    "        # Apply random masking\n",
    "        seq_length = input_ids.shape[0]\n",
    "        num_to_mask = max(1, int(self.mask_ratio * (seq_length - 2)))  # Avoid CLS/SEP\n",
    "        mask_indices = torch.randperm(seq_length - 2)[:num_to_mask] + 1  # Avoid first and last token\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[mask_indices] = self.tokenizer.mask_token_id  # Replace with [MASK] token\n",
    "\n",
    "        return masked_input_ids, attention_mask, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(dataloader, model, tokenizer):\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Masked Predictions\"):\n",
    "            # Move batch data to GPU\n",
    "            masked_input_ids, attention_mask, original_input_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            # Run model inference on GPU\n",
    "            outputs = model(input_ids=masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Replace masked tokens with predicted tokens\n",
    "            predicted_tokens_batch = masked_input_ids.clone()\n",
    "            for i in range(masked_input_ids.shape[0]):  # Loop over batch\n",
    "                mask_positions = (masked_input_ids[i] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "                for pos in mask_positions:\n",
    "                    predicted_token_id = torch.argmax(outputs.logits[i, pos], dim=-1).item()\n",
    "                    predicted_tokens_batch[i, pos] = predicted_token_id\n",
    "\n",
    "            # Decode predictions\n",
    "            batch_predictions = tokenizer.batch_decode(predicted_tokens_batch.cpu(), skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both mBERT\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)\n",
    "multilingual_mbert_model = torch.compile(multilingual_mbert_model)\n",
    "multilingual_mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_mbert_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_mbert_predictions_texts = multilingual_mbert_predictions[\"burmese\"].tolist()\n",
    "multilingual_mbert_predictions_dataset = MaskedTextDataset(multilingual_mbert_predictions_texts, multilingual_mbert_tokenizer)\n",
    "multilingual_mbert_predictions_dataloader = DataLoader(\n",
    "    multilingual_mbert_predictions_dataset, \n",
    "    batch_size=multilingual_mbert_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "multilingual_mbert_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_mbert_predictions_dataloader, \n",
    "    multilingual_mbert_model, \n",
    "    multilingual_mbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0      ...   \n",
       "1    ...   \n",
       "2     ...   \n",
       "3     ...   \n",
       "4    ...   \n",
       "\n",
       "                                           generated  \n",
       "0      ...  \n",
       "1    ...  \n",
       "2     ...  \n",
       "3      ...  \n",
       "4       ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display\n",
    "display(multilingual_mbert_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual mbert predictions\n",
    "save_models_df(multilingual_mbert_predictions, \"multilingual_mbert_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): XLMRobertaForMaskedLM(\n",
       "    (roberta): XLMRobertaModel(\n",
       "      (embeddings): XLMRobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): XLMRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): XLMRobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)\n",
    "multilingual_xlmr_model = torch.compile(multilingual_xlmr_model)\n",
    "multilingual_xlmr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_xlmr_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinlingual batch size\n",
    "multilingual_xlmr_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_xlmr_predictions_texts = multilingual_xlmr_predictions[\"burmese\"].tolist()\n",
    "multilingual_xlmr_predictions_dataset = MaskedTextDataset(multilingual_xlmr_predictions_texts, multilingual_xlmr_tokenizer)\n",
    "multilingual_xlmr_predictions_dataloader = DataLoader(\n",
    "    multilingual_xlmr_predictions_dataset, \n",
    "    batch_size=multilingual_xlmr_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "multilingual_xlmr_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_xlmr_predictions_dataloader, \n",
    "    multilingual_xlmr_model, \n",
    "    multilingual_xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(multilingual_xlmr_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual xlmr predictions\n",
    "save_models_df(multilingual_xlmr_predictions, \"multilingual_xlmr_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual predictions\n",
    "multilingual_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"multilingual_{model_name}_predictions\") for model_name in multilingual_model_names.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for mbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659290dc08594893a7041b7474fc6192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Computing Metrics!\n",
      "Processing Data for xlmr...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f013e1e1d74804bf2d6a7b5572e350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Metrics:   0%|          | 0/50862 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Computing Metrics!\n"
     ]
    }
   ],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Processing Data for {model_name}...\"),\n",
    "    compute_metrics_batch(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scores for mbert:\n",
      " BLEU Score: 0.11161891030998354\n",
      " ROUGE-1 Score: 0.21893887065417755\n",
      " ROUGE-2 Score: 0.0979642909372018\n",
      " ROUGE-L Score: 0.2188918748703697\n",
      " chrF-S Score: 79.22023133392412\n",
      " BERT Score: 0.8790649549057732\n",
      "Metrics scores for xlmr:\n",
      " BLEU Score: 0.4581550312737298\n",
      " ROUGE-1 Score: 0.25353074248638974\n",
      " ROUGE-2 Score: 0.11654468643909702\n",
      " ROUGE-L Score: 0.2535243519409113\n",
      " chrF-S Score: 88.05020715366919\n",
      " BERT Score: 0.9653381555889077\n"
     ]
    }
   ],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"multilingual_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to compute perplexity in batch\n",
    "def compute_multilingual_perplexity_batch(dataloader, model, tokenizer):\n",
    "    perplexities = []\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)  # Move model to GPU\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Computing Perplexity\", unit=\"batch\"):\n",
    "        batch_texts = batch  # Text input batch\n",
    "\n",
    "        #  Tokenize batch with padding & truncation\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)  # Forward pass\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        temperature = 1.5\n",
    "        logits = logits / temperature\n",
    "\n",
    "        #  Compute log probabilities\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        #  Get token log-likelihoods using true token IDs\n",
    "        target_ids = inputs[\"input_ids\"]\n",
    "        log_likelihood = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        #  Apply attention mask to remove padding tokens\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        masked_log_likelihood = log_likelihood * mask  # Zero out padding contributions\n",
    "\n",
    "        #  Compute sentence-level mean log-likelihood\n",
    "        sentence_log_likelihood = masked_log_likelihood.sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        #  Convert log-likelihood to perplexity\n",
    "        log_perplexity = -sentence_log_likelihood\n",
    "        batch_perplexities = torch.exp(log_perplexity).cpu().numpy()\n",
    "\n",
    "        perplexities.extend(batch_perplexities)\n",
    "\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both mbert\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_mbert_generated_texts = multilingual_evaluation_results_datasets[\"mbert\"][\"generated\"].tolist()\n",
    "multilingual_mbert_text_dataset = TextDataset(multilingual_mbert_generated_texts)\n",
    "multilingual_mbert_dataloader = DataLoader(\n",
    "    multilingual_mbert_text_dataset, \n",
    "    batch_size=multilingual_mbert_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"mbert\"][\"perplexity\"] = compute_multilingual_perplexity_batch(\n",
    "    multilingual_mbert_dataloader,\n",
    "    multilingual_mbert_model,\n",
    "    multilingual_mbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 1.9864329099655151\n"
     ]
    }
   ],
   "source": [
    "# display perplexity\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['mbert']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"mbert\"], f\"multilingual_mbert_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_xlmr_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_xlmr_generated_texts = multilingual_evaluation_results_datasets[\"xlmr\"][\"generated\"].tolist()\n",
    "multilingual_xlmr_text_dataset = TextDataset(multilingual_xlmr_generated_texts)\n",
    "multilingual_xlmr_dataloader = DataLoader(\n",
    "    multilingual_xlmr_text_dataset, \n",
    "    batch_size=multilingual_xlmr_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"xlmr\"][\"perplexity\"] = compute_multilingual_perplexity_batch(\n",
    "    multilingual_xlmr_dataloader,\n",
    "    multilingual_xlmr_model,\n",
    "    multilingual_xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 551040.3125\n"
     ]
    }
   ],
   "source": [
    "# display perplexity\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['xlmr']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"xlmr\"], f\"multilingual_xlmr_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mbert...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>chrf-s</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>0.228942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.865183</td>\n",
       "      <td>0.903668</td>\n",
       "      <td>1.751204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>0.044632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.865183</td>\n",
       "      <td>0.901875</td>\n",
       "      <td>1.530580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>0.203317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.865183</td>\n",
       "      <td>0.841597</td>\n",
       "      <td>1.400856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.865183</td>\n",
       "      <td>0.896423</td>\n",
       "      <td>1.960057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>     ...</td>\n",
       "      <td>0.026921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.865183</td>\n",
       "      <td>0.928218</td>\n",
       "      <td>1.473944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0      ...   \n",
       "1    ...   \n",
       "2     ...   \n",
       "3     ...   \n",
       "4    ...   \n",
       "\n",
       "                                           generated      bleu  rouge-1  \\\n",
       "0      ...  0.228942      0.0   \n",
       "1    ...  0.044632      0.0   \n",
       "2     ...  0.203317      0.0   \n",
       "3      ...  0.036788      0.0   \n",
       "4       ...  0.026921      0.0   \n",
       "\n",
       "   rouge-2  rouge-l     chrf-s  bert_score  perplexity  \n",
       "0      0.0      0.0  91.865183    0.903668    1.751204  \n",
       "1      0.0      0.0  91.865183    0.901875    1.530580  \n",
       "2      0.0      0.0  91.865183    0.841597    1.400856  \n",
       "3      0.0      0.0  91.865183    0.896423    1.960057  \n",
       "4      0.0      0.0  91.865183    0.928218    1.473944  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing xlmr...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "      <th>chrf-s</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>    ...</td>\n",
       "      <td>0.202052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.223693</td>\n",
       "      <td>0.910403</td>\n",
       "      <td>1.034274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>0.086334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.223693</td>\n",
       "      <td>0.936242</td>\n",
       "      <td>1.094066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>0.783920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.223693</td>\n",
       "      <td>0.978620</td>\n",
       "      <td>1.017733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>   ...</td>\n",
       "      <td>0.513345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.223693</td>\n",
       "      <td>0.956735</td>\n",
       "      <td>1.054627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>  ...</td>\n",
       "      <td>0.680375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.223693</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>1.007931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0      ...   \n",
       "1    ...   \n",
       "2     ...   \n",
       "3     ...   \n",
       "4    ...   \n",
       "\n",
       "                                           generated      bleu  rouge-1  \\\n",
       "0      ...  0.202052      0.0   \n",
       "1     ...  0.086334      0.0   \n",
       "2     ...  0.783920      0.0   \n",
       "3     ...  0.513345      0.0   \n",
       "4    ...  0.680375      0.0   \n",
       "\n",
       "   rouge-2  rouge-l     chrf-s  bert_score  perplexity  \n",
       "0      0.0      0.0  84.223693    0.910403    1.034274  \n",
       "1      0.0      0.0  84.223693    0.936242    1.094066  \n",
       "2      0.0      0.0  84.223693    0.978620    1.017733  \n",
       "3      0.0      0.0  84.223693    0.956735    1.054627  \n",
       "4      0.0      0.0  84.223693    0.975785    1.007931  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine evaluation results\n",
    "for model_name in multilingual_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"multilingual_{model_name}_metrics\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"multilingual_{model_name}_perplexity\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(multilingual_evaluation_results_datasets[model_name], f\"multilingual_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(multilingual_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics scores for mbert:\n",
      " BLEU Score: 0.1116189103099835\n",
      " ROUGE-1 Score: 0.21893887065417755\n",
      " ROUGE-2 Score: 0.0979642909372018\n",
      " ROUGE-L Score: 0.2188918748703697\n",
      " chrF-S Score: 79.22023133392412\n",
      " BERT Score: 0.8790649549057732\n",
      " Perplexity: 1.986432686204762\n",
      "Metrics scores for xlmr:\n",
      " BLEU Score: 0.45815503127372975\n",
      " ROUGE-1 Score: 0.25353074248638974\n",
      " ROUGE-2 Score: 0.11654468643909702\n",
      " ROUGE-L Score: 0.2535243519409113\n",
      " chrF-S Score: 88.05020715366919\n",
      " BERT Score: 0.9653381555889077\n",
      " Perplexity: 551040.5946815407\n"
     ]
    }
   ],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert_score'].mean()}\")\n",
    "    print(f\" Perplexity: {dataset['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "benchmarking_models = {\n",
    "    \"lstm\": [\n",
    "        \"bpe\", \n",
    "        \"unigram\"\n",
    "    ],\n",
    "    \"multilingual\": [\n",
    "        \"mbert\", \n",
    "        \"xlmr\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_benchmarking(key, model_name):\n",
    "    df = load_models_df(f\"{key}_{model_name}_evaluation_results\")\n",
    "\n",
    "    df = df[[\"english\", \"bleu\", \"rouge-1\", \"rouge-2\", \"rouge-l\", \"chrf-s\", \"bert_score\", \"perplexity\"]]\n",
    "    \n",
    "    column_mapping = {\n",
    "        \"bleu\": \"bleu\",\n",
    "        \"rouge-1\": \"rouge1\",\n",
    "        \"rouge-2\": \"rouge2\",\n",
    "        \"rouge-l\": \"rougeL\",\n",
    "        \"chrf-s\": \"chrF\",\n",
    "        \"bert_score\": \"bert\",\n",
    "        \"perplexity\": \"perplexity\",\n",
    "    }\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "benchmarking_loaded_datasets = {}\n",
    "for key, model_list in benchmarking_models.items():\n",
    "    for model_name in model_list:\n",
    "        df = load_and_rename_columns_benchmarking(key, model_name)\n",
    "        benchmarking_loaded_datasets[f\"{key}_{model_name}\"] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores for Comparison\n",
    "Get mean BLEU, ROUGE, chrF-S, Bert Score and Perplexity for LSTM (BPE & Unigram), mBERT, and XLM-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model names and their respective column prefixes\n",
    "benchmarking_model_names = [\"LSTM BPE\", \"LSTM Unigram\", \"mBERT\", \"XLM-R\"]\n",
    "benchmarking_column_prefixes = [\"bpe\", \"unigram\", \"mbert\", \"xlmr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean scores dynamically using a dictionary comprehension\n",
    "benchmarking_mean_scores = {\n",
    "    model: {\n",
    "        \"BLEU\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].bleu.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].bleu.mean(),\n",
    "        \"ROUGE-1\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].rouge1.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].rouge1.mean(),\n",
    "        \"ROUGE-2\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].rouge2.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].rouge2.mean(),\n",
    "        \"ROUGE-L\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].rougeL.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].rougeL.mean(),\n",
    "        \"chrF-S\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].chrF.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].chrF.mean(),\n",
    "        \"BERT Score\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].bert.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].bert.mean(),\n",
    "        \"Perplexity\": benchmarking_loaded_datasets[f\"lstm_{prefix}\"].perplexity.mean() if f\"lstm_{prefix}\" in benchmarking_loaded_datasets else benchmarking_loaded_datasets[f\"multilingual_{prefix}\"].perplexity.mean(),\n",
    "    }\n",
    "    for model, prefix in zip(benchmarking_model_names, benchmarking_column_prefixes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mean scores dictionary to DataFrame for better visualization\n",
    "benchmarking_mean_scores_df = pd.DataFrame.from_dict(benchmarking_mean_scores, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>chrF-S</th>\n",
       "      <th>BERT Score</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTM BPE</th>\n",
       "      <td>0.855064</td>\n",
       "      <td>0.298951</td>\n",
       "      <td>0.151467</td>\n",
       "      <td>0.298951</td>\n",
       "      <td>98.877475</td>\n",
       "      <td>0.995052</td>\n",
       "      <td>6.046939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM Unigram</th>\n",
       "      <td>0.855391</td>\n",
       "      <td>0.299182</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>0.299182</td>\n",
       "      <td>98.937219</td>\n",
       "      <td>0.995120</td>\n",
       "      <td>6.046573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mBERT</th>\n",
       "      <td>0.111619</td>\n",
       "      <td>0.218939</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>0.218892</td>\n",
       "      <td>79.220231</td>\n",
       "      <td>0.879065</td>\n",
       "      <td>1.986433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLM-R</th>\n",
       "      <td>0.458155</td>\n",
       "      <td>0.253531</td>\n",
       "      <td>0.116545</td>\n",
       "      <td>0.253524</td>\n",
       "      <td>88.050207</td>\n",
       "      <td>0.965338</td>\n",
       "      <td>551040.594682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  BLEU   ROUGE-1   ROUGE-2   ROUGE-L     chrF-S  BERT Score  \\\n",
       "LSTM BPE      0.855064  0.298951  0.151467  0.298951  98.877475    0.995052   \n",
       "LSTM Unigram  0.855391  0.299182  0.151693  0.299182  98.937219    0.995120   \n",
       "mBERT         0.111619  0.218939  0.097964  0.218892  79.220231    0.879065   \n",
       "XLM-R         0.458155  0.253531  0.116545  0.253524  88.050207    0.965338   \n",
       "\n",
       "                 Perplexity  \n",
       "LSTM BPE           6.046939  \n",
       "LSTM Unigram       6.046573  \n",
       "mBERT              1.986433  \n",
       "XLM-R         551040.594682  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display mean scores\n",
    "display(benchmarking_mean_scores_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
