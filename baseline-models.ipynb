{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n",
      "2025-02-03 08:36:18.426890: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-03 08:36:18.443135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-03 08:36:18.468917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-03 08:36:18.468946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 08:36:18.484364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-03 08:36:19.420598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "=======\n",
      "2025-02-03 15:41:10.248112: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-03 15:41:10.264377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-03 15:41:10.293751: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-03 15:41:10.293789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 15:41:10.308631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-03 15:41:11.187380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import bert_score\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_chrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n",
      "2025-02-03 08:36:25.177779: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:25.225423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:25.227299: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "=======\n",
      "2025-02-03 15:41:16.663371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 15:41:16.711988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 15:41:16.714154: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630eef3e0e3e466f954be9b5c595a370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb69d0c6b7406c8e268998d7ab1618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b59778a199455aaf76f7984a43a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load BLEU and ROUGE metric objects\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for batching\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = [str(text) if text is not None else \"\" for text in texts] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save models df\n",
    "def save_models_df(df, df_name):\n",
    "    df.to_csv(f\"models/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load spt df\n",
    "def load_spt_df(df_name):\n",
    "    return pd.read_csv(f\"spt/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load models df\n",
    "def load_models_df(df_name):\n",
    "    return pd.read_csv(f\"models/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute bleu score\n",
    "def compute_bleu(reference, prediction):\n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference.split()], prediction.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    return ast.literal_eval(val) if isinstance(val, str) else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute BLEU, ROUGE, chrF-S, and BERTScore.\n",
    "def compute_metrics(dataset):\n",
    "    predictions = dataset[\"generated\"].astype(str)  # Convert to string\n",
    "    references = dataset[\"burmese\"].astype(str)  # Convert to string\n",
    "\n",
    "    print(\"Calculating BLEU...\")\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    dataset[\"bleu\"] = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth_fn) for pred, ref in zip(predictions, references)]\n",
    "\n",
    "    print(\"Calculating ROUGE...\")\n",
    "    rouge_scores = [rouge_scorer.score(pred, ref) for pred, ref in zip(predictions, references)]\n",
    "    dataset[\"rouge-1\"] = [r[\"rouge1\"].fmeasure for r in rouge_scores]\n",
    "    dataset[\"rouge-2\"] = [r[\"rouge2\"].fmeasure for r in rouge_scores]\n",
    "    dataset[\"rouge-l\"] = [r[\"rougeL\"].fmeasure for r in rouge_scores]\n",
    "\n",
    "    print(\"Calculating chrF-S...\")\n",
    "    dataset[\"chrf-s\"] = corpus_chrf(\n",
    "        [str(pred) for pred in predictions],  # Convert all predictions to strings\n",
    "        [[str(ref)] for ref in references]    # Convert references to lists of strings\n",
    "    ).score\n",
    "    \n",
    "    print(\"Calculating BERTScore...\")\n",
    "    bert_scores = bert_score.score(predictions, references, lang=\"my\")\n",
    "    dataset[\"bert_score\"] = bert_scores[2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity in batch\n",
    "def compute_perplexity_batch(texts, tokenizer, model):\n",
    "    # Ensure all inputs are valid strings\n",
    "    valid_texts = [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
    "\n",
    "    # Tokenize batch with padding & truncation\n",
    "    inputs = tokenizer(valid_texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Get model outputs\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # Compute log probabilities\n",
    "\n",
    "    # Get token log-likelihoods using true token IDs\n",
    "    target_ids = inputs[\"input_ids\"]  # (batch_size, seq_len)\n",
    "    log_likelihood = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Apply attention mask to remove padding tokens\n",
    "    mask = inputs[\"attention_mask\"]  # (batch_size, seq_len)\n",
    "    masked_log_likelihood = log_likelihood * mask  # Zero out padding contributions\n",
    "\n",
    "    # Compute sentence-level mean log-likelihood (only over valid tokens)\n",
    "    sentence_log_likelihood = masked_log_likelihood.sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "    # Clamp values to avoid numerical instability\n",
    "    sentence_log_likelihood = torch.clamp(sentence_log_likelihood, min=-100, max=100)\n",
    "\n",
    "    # Convert log-likelihood to perplexity\n",
    "    log_perplexity = -sentence_log_likelihood\n",
    "    perplexities = torch.exp(log_perplexity)\n",
    "\n",
    "    # 🔍 Print warning if perplexities contain `inf`\n",
    "    if torch.isinf(perplexities).any():\n",
    "        print(\"\\n [WARNING] Perplexity contains `inf` for some texts!\")\n",
    "        print(f\"  Log-Likelihood Shape: {log_likelihood.shape}\")\n",
    "        print(f\"  Log-Likelihood Mean: {sentence_log_likelihood.mean().item()}\")\n",
    "        print(f\"  Computed Perplexity Values: {perplexities}\")\n",
    "\n",
    "    return perplexities.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing RNN/LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing\n",
    "Load SPT-tokenized datasets, convert to sequences, and apply padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load datasets\n",
    "def get_lstm_datasets(model_name):\n",
    "    return {\n",
    "        \"normal\": [\n",
    "            f\"tokenized_{model_name}_myxnli_normalized_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_normalized_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_normalized\"\n",
    "        ],\n",
    "        \"nllb_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_1\", \n",
    "            f\"tokenized_{model_name}_myxnli_nllb_back_translated_final_2\", \n",
    "            f\"tokenized_{model_name}_alt_combined_nllb_back_translated_final\"\n",
    "        ],\n",
    "        \"seamless_m4t_back_translated\": [\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_1\",\n",
    "            f\"tokenized_{model_name}_myxnli_seamless_m4t_back_translated_final_2\",\n",
    "            f\"tokenized_{model_name}_alt_combined_seamless_m4t_back_translated_final\"\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_lstm(file_name):\n",
    "    df = load_spt_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "        \"english_back_translated_tokens\": \"english_tokens\",\n",
    "        \"burmese_translated_tokens\": \"burmese_tokens\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\", \"english_tokens\", \"burmese_tokens\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "lstm_all_datasets = {}\n",
    "for model_name in spt_models.keys():\n",
    "    datasets = get_lstm_datasets(model_name)\n",
    "\n",
    "    lstm_all_datasets[model_name] = {\n",
    "        key: [load_and_rename_columns_lstm(file) for file in file_list] for key, file_list in datasets.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cobine all datasets\n",
    "lstm_all_datasets_combined = {}\n",
    "for model_name in lstm_all_datasets.keys():\n",
    "    lstm_all_datasets_combined[model_name] = pd.concat(\n",
    "        [pd.concat(datasets) for datasets in lstm_all_datasets[model_name].values()],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias and drop null\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name] = lstm_all_datasets_combined[model_name].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe dataset length: 1627576\n",
      "unigram dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# display of datasets\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    print(f\"{model_name} dataset length: {len(lstm_all_datasets_combined[model_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Apply padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9a8580ffe4e71bf432aabe37f3e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0dfee7ebcc4bce8f4310359d590ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821c5216a41c4a1091a5814be6f84558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bdd8a4c9564f75a3c4e3d9e587ee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1627576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert tokenized sequences to lists\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq\"] = lstm_all_datasets_combined[model_name][\"english_tokens\"].progress_apply(\n",
    "        lambda x: spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq\"] = lstm_all_datasets_combined[model_name][\"burmese_tokens\"].progress_apply(\n",
    "        lambda x:  spt_models[model_name].EncodeAsIds(str(x)) if isinstance(x, str) else []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "lstm_max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply padding to sequences\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    lstm_all_datasets_combined[model_name][\"english_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"english_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()\n",
    "\n",
    "    lstm_all_datasets_combined[model_name][\"burmese_seq_padded\"] = pad_sequences(\n",
    "        lstm_all_datasets_combined[model_name][\"burmese_seq\"], maxlen=lstm_max_seq_length, padding=\"post\"\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm preprocess data\n",
    "for model_name in lstm_all_datasets_combined.keys():\n",
    "    save_models_df(lstm_all_datasets_combined[model_name], f\"lstm_{model_name}_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model\n",
    "Define an LSTM-based sequence-to-sequence (seq2seq) model with embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lstm_embedding_dim = 256\n",
    "lstm_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size from SentencePiece models\n",
    "lstm_vocab_sizes = {model_name: sp.GetPieceSize() for model_name, sp in spt_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build lstm model\n",
    "def build_lstm_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=lstm_embedding_dim, mask_zero=True),\n",
    "        Bidirectional(LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        LSTM(lstm_hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:15:44.588870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.591696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.593767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.731312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.732575: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.733704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:15:44.734788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_bpe_model = build_lstm_model(lstm_vocab_sizes[\"bpe\"])\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 08:36:43.437264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.512506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.515652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.720777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.722038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.723184: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-03 08:36:43.724263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 256)         8192000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, None, 1024)        3149824   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 32000)       16416000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30905600 (117.90 MB)\n",
      "Trainable params: 30905600 (117.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build lstm bpe model\n",
    "lstm_unigram_model = build_lstm_model(lstm_vocab_sizes[\"unigram\"])\n",
    "lstm_unigram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Train the model using Categorical Cross-Entropy loss & Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model train batch size\n",
    "lstm_train_batch_size = 64\n",
    "lstm_train_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_bpe_model_prefix = \"models/lstm_bpe_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_preprocessed = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_bpe_preprocessed[\"burmese_seq_padded\"] = lstm_bpe_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_preprocessed[\"english_seq_padded\"] = lstm_bpe_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_bpe_X_train = np.array(lstm_bpe_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_bpe_y_train = np.array(lstm_bpe_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_bpe_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_bpe_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_bpe_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_bpe_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_bpe_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_bpe_model.load_weights(f\"{lstm_bpe_model_prefix}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738571273.538957     644 service.cc:145] XLA service 0x7f13d31873b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738571273.538996     644 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 08:27:53.545684: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 08:27:53.565445: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738571273.632815     644 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4/22888 [..............................] - ETA: 7:51:19 - loss: 1.5636 - accuracy: 0.6750  "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_bpe_model.fit(\n",
    "    lstm_bpe_X_train, \n",
    "    lstm_bpe_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_bpe_early_stopping, lstm_bpe_checkpoint]\n",
    ")\n",
    "\n",
    "# save lstm bpe model\n",
    "lstm_bpe_model.save(f\"{lstm_bpe_model_prefix}.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "lstm_bpe_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prefix\n",
    "lstm_unigram_model_prefix = \"models/lstm_unigram_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_preprocessed = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list\n",
    "lstm_unigram_preprocessed[\"burmese_seq_padded\"] = lstm_unigram_preprocessed[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_preprocessed[\"english_seq_padded\"] = lstm_unigram_preprocessed[\"english_seq_padded\"].apply(safe_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1627576, 128)\n",
      "y_train shape: (1627576, 128)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "lstm_unigram_X_train = np.array(lstm_unigram_preprocessed[\"burmese_seq_padded\"].tolist(), dtype=np.int32)\n",
    "lstm_unigram_y_train = np.array(lstm_unigram_preprocessed[\"english_seq_padded\"].tolist(), dtype=np.int32)\n",
    "\n",
    "print(f\"X_train shape: {lstm_unigram_X_train.shape}\")\n",
    "print(f\"y_train shape: {lstm_unigram_y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Early Stopping + Model Checkpoint\n",
    "lstm_unigram_early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_unigram_checkpoint = ModelCheckpoint(\n",
    "    f\"{lstm_unigram_model_prefix}.keras\", \n",
    "    save_best_only=True, \n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_unigram_model.load_weights(f\"{lstm_unigram_model_prefix}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738572496.380013   32555 service.cc:145] XLA service 0x7fa8b8006b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738572496.380050   32555 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-03 08:48:16.386695: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-03 08:48:16.406514: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1738572496.469539   32555 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45/22888 [..............................] - ETA: 7:53:16 - loss: 1.2307 - accuracy: 0.7582  "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lstm_unigram_model.fit(\n",
    "    lstm_unigram_X_train, \n",
    "    lstm_unigram_y_train, \n",
    "    batch_size=lstm_train_batch_size,\n",
    "    epochs=lstm_train_epochs, \n",
    "    validation_split=0.1, \n",
    "    callbacks=[lstm_unigram_early_stopping, lstm_unigram_checkpoint]\n",
    ")\n",
    "\n",
    "# save lstm bpe model\n",
    "lstm_unigram_model.save(lstm_unigram_model_prefix, save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "lstm_unigram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Use trained LSTM models to generate translations for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Text Dataset\n",
    "class LstmTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for text sequences\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=50):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokenized_text = self.tokenizer.encode(text, out_type=int)\n",
    "\n",
    "        # Pad sequences to max_length\n",
    "        tokenized_text = tokenized_text[:self.max_length]\n",
    "        padding_length = self.max_length - len(tokenized_text)\n",
    "        padded_sequence = tokenized_text + [0] * padding_length  # Padding with 0\n",
    "\n",
    "        return torch.tensor(padded_sequence, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text using LSTM in auto-regressive manner for batch processing.\n",
    "def generate_text_lstm_batch(dataloader, model, tokenizer, max_length=50):\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Generating Text with LSTM\"):\n",
    "        batch = batch.numpy()  # Convert to NumPy for TensorFlow processing\n",
    "        batch_generated = []\n",
    "\n",
    "        for seq in batch:\n",
    "            generated_sequence = seq.tolist()\n",
    "\n",
    "            for _ in range(max_length - len(seq)):\n",
    "                input_padded = pad_sequences(\n",
    "                    [generated_sequence], maxlen=max_length, padding='pre'\n",
    "                )\n",
    "                input_tensor = tf.convert_to_tensor(input_padded, dtype=tf.int32)\n",
    "\n",
    "                predictions = model.predict(input_tensor, verbose=0)\n",
    "                next_token = np.argmax(predictions[0], axis=-1)\n",
    "\n",
    "                if next_token == tokenizer.eos_id():\n",
    "                    break  # Stop at EOS token\n",
    "\n",
    "                generated_sequence.append(next_token)\n",
    "\n",
    "            batch_generated.append(tokenizer.decode(generated_sequence))\n",
    "\n",
    "        all_predictions.extend(batch_generated)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(f\"models/lstm_bpe_model.h5\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe dataset\n",
    "lstm_bpe_predictions = load_models_df(\"lstm_bpe_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_bpe_predictions[\"burmese_seq_padded\"] = lstm_bpe_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_bpe_predictions_sequences = np.array(lstm_bpe_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_bpe_predictions_dataset = LstmTextDataset(lstm_bpe_predictions_sequences, lstm_bpe_tokenizer)\n",
    "lstm_bpe_predictions_dataloader = DataLoader(lstm_bpe_predictions_dataset, batch_size=lstm_bpe_predictions_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_bpe_predictions[\"generated\"] = generate_text_lstm_batch(lstm_bpe_predictions_dataloader, lstm_bpe_model, lstm_bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns and display\n",
    "lstm_bpe_predictions = lstm_bpe_predictions[lstm_bpe_predictions[\"english\", \"burmese\", \"generated\"]]\n",
    "display(lstm_bpe_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm bpe predictions\n",
    "save_models_df(lstm_bpe_predictions, \"lstm_bpe_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.h5\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram dataset\n",
    "lstm_unigram_predictions = load_models_df(\"lstm_unigram_preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from string to list and NumPy arrays\n",
    "lstm_unigram_predictions[\"burmese_seq_padded\"] = lstm_unigram_predictions[\"burmese_seq_padded\"].apply(safe_eval)\n",
    "lstm_unigram_predictions_sequences = np.array(lstm_unigram_predictions[\"burmese_seq_padded\"].tolist(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "lstm_unigram_predictions_dataset = LstmTextDataset(lstm_unigram_predictions_sequences, lstm_unigram_tokenizer)\n",
    "lstm_unigram_predictions_dataloader = DataLoader(lstm_unigram_predictions_dataset, batch_size=lstm_unigram_predictions_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "lstm_unigram_predictions[\"generated\"] = generate_text_lstm_batch(lstm_unigram_predictions_dataloader, lstm_unigram_model, lstm_unigram_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns and display\n",
    "lstm_unigram_predictions = lstm_unigram_predictions[lstm_unigram_predictions[\"english\", \"burmese\", \"generated\"]]\n",
    "display(lstm_unigram_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstm unigram predictions\n",
    "save_models_df(lstm_unigram_predictions, \"lstm_unigram_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm predictions\n",
    "lstm_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"lstm_{model_name}_predictions\") for model_name in spt_models.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    compute_metrics(dataset)\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resutls\n",
    "for model_name, dataset in lstm_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"lstm_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_bpe_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm bpe model and tokenizer\n",
    "lstm_bpe_model = load_model(f\"models/lstm_bpe_model.h5\")\n",
    "lstm_bpe_tokenizer = spt_models[\"bpe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_bpe_generated_texts = lstm_evaluation_results_datasets[\"bpe\"][\"generated\"].tolist()\n",
    "lstm_bpe_text_dataset = TextDataset(lstm_bpe_generated_texts)\n",
    "lstm_bpe_dataloader = DataLoader(\n",
    "    lstm_bpe_text_dataset, \n",
    "    batch_size=lstm_bpe_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "lstm_bpe_perplexity_scores = []\n",
    "for batch in tqdm(lstm_bpe_dataloader, desc=\"Computing Perplexity for LSTM BPE\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, lstm_bpe_tokenizer, lstm_bpe_model)\n",
    "    lstm_bpe_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "lstm_evaluation_results_datasets[\"bpe\"][\"perplexity\"] = lstm_bpe_perplexity_scores\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['bpe']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"bpe\"], f\"lstm_bpe_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "lstm_unigram_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lstm unigram model and tokenizer\n",
    "lstm_unigram_model = load_model(f\"models/lstm_unigram_model.h5\")\n",
    "lstm_unigram_tokenizer = spt_models[\"unigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "lstm_unigram_generated_texts = lstm_evaluation_results_datasets[\"unigram\"][\"generated\"].tolist()\n",
    "lstm_unigram_text_dataset = TextDataset(lstm_unigram_generated_texts)\n",
    "lstm_unigram_dataloader = DataLoader(\n",
    "    lstm_unigram_text_dataset, \n",
    "    batch_size=lstm_unigram_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "lstm_unigram_perplexity_scores = []\n",
    "for batch in tqdm(lstm_unigram_dataloader, desc=\"Computing Perplexity for LSTM Unigram\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, lstm_unigram_tokenizer, lstm_unigram_model)\n",
    "    lstm_unigram_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "lstm_evaluation_results_datasets[\"unigram\"][\"perplexity\"] = lstm_unigram_perplexity_scores\n",
    "print(f\"Perplexity Score: {lstm_evaluation_results_datasets['unigram']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(lstm_evaluation_results_datasets[\"unigram\"], f\"lstm_unigram_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in lstm_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"lstm_{model_name}_metrics\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    lstm_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"lstm_{model_name}_perplexity\")\n",
    "    lstm_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(lstm_evaluation_results_datasets[model_name], f\"lstm_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(lstm_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Implementing Multilingual Transformer Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "multilingual_model_names = {\n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"xlmr\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "multilingual_datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\",\n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\",\n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english_back_translated\": \"english\",\n",
    "        \"burmese_translated\": \"burmese\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"burmese\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "mutlilingual_loaded_datasets = {}\n",
    "for key, file_list in multilingual_datasets.items():\n",
    "    mutlilingual_loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "multilingual_combined = pd.concat(\n",
    "    mutlilingual_loaded_datasets[\"normal\"] + \n",
    "    mutlilingual_loaded_datasets[\"nllb_back_translated\"] + \n",
    "    mutlilingual_loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "multilingual_combined = multilingual_combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Multilingual dataset length: {len(multilingual_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_models_df(multilingual_combined, \"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions\n",
    "Load ```mBERT``` and ```XLM-R``` for Masked Language Modeling (MLM).\n",
    "MLM helps predict missing words in Burmese sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for masked\n",
    "class MaskedTextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, mask_ratio=0.15, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n",
    "\n",
    "        # Tokenize and move tensors to GPU\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0).to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0).to(device)\n",
    "\n",
    "        # Apply random masking\n",
    "        seq_length = input_ids.shape[0]\n",
    "        num_to_mask = max(1, int(self.mask_ratio * (seq_length - 2)))  # Avoid CLS/SEP\n",
    "        mask_indices = torch.randperm(seq_length - 2)[:num_to_mask] + 1  # Avoid first and last token\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[mask_indices] = self.tokenizer.mask_token_id  # Replace with [MASK] token\n",
    "\n",
    "        return masked_input_ids, attention_mask, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(dataloader, model, tokenizer):\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Masked Predictions\"):\n",
    "            # Move batch data to GPU\n",
    "            masked_input_ids, attention_mask, original_input_ids = [x.to(device) for x in batch]\n",
    "\n",
    "            # Run model inference on GPU\n",
    "            outputs = model(input_ids=masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Replace masked tokens with predicted tokens\n",
    "            predicted_tokens_batch = masked_input_ids.clone()\n",
    "            for i in range(masked_input_ids.shape[0]):  # Loop over batch\n",
    "                mask_positions = (masked_input_ids[i] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "                for pos in mask_positions:\n",
    "                    predicted_token_id = torch.argmax(outputs.logits[i, pos], dim=-1).item()\n",
    "                    predicted_tokens_batch[i, pos] = predicted_token_id\n",
    "\n",
    "            # Decode predictions\n",
    "            batch_predictions = tokenizer.batch_decode(predicted_tokens_batch.cpu(), skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both mBERT\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)\n",
    "multilingual_mbert_model = torch.compile(multilingual_mbert_model)\n",
    "multilingual_mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_mbert_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_mbert_predictions_texts = multilingual_mbert_predictions[\"burmese\"].tolist()\n",
    "multilingual_mbert_predictions_dataset = MaskedTextDataset(multilingual_mbert_predictions_texts, multilingual_mbert_tokenizer)\n",
    "multilingual_mbert_predictions_dataloader = DataLoader(\n",
    "    multilingual_mbert_predictions_dataset, \n",
    "    batch_size=multilingual_mbert_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35939d50c1ae4413b2a520d4c04ad813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Masked Predictions:   0%|          | 0/203447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run text generation\n",
    "multilingual_mbert_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_mbert_predictions_dataloader, \n",
    "    multilingual_mbert_model, \n",
    "    multilingual_mbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "      <td>စွန့်စားချင်သီတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...</td>\n",
       "      <td>သူမက စီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတဲ့ ၊ိမ်းကောင်း...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...</td>\n",
       "      <td>ထိုစုံပွဲသည် ဇင်ဘာတွေ ၏ အကြိမ်များကို ပြန်လည်ရ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...</td>\n",
       "      <td>အခွက်ထမ်းများထံ ၎ င်း ၏ သတိပြချက်များကို ရှင်း...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...   \n",
       "1  စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...   \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...   \n",
       "3  ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...   \n",
       "4  အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...   \n",
       "\n",
       "                                           generated  \n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...  \n",
       "1  စွန့်စားချင်သီတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...  \n",
       "2  သူမက စီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတဲ့ ၊ိမ်းကောင်း...  \n",
       "3  ထိုစုံပွဲသည် ဇင်ဘာတွေ ၏ အကြိမ်များကို ပြန်လည်ရ...  \n",
       "4  အခွက်ထမ်းများထံ ၎ င်း ၏ သတိပြချက်များကို ရှင်း...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display\n",
    "display(multilingual_mbert_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual mbert predictions\n",
    "save_models_df(multilingual_mbert_predictions, \"multilingual_mbert_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): XLMRobertaForMaskedLM(\n",
       "    (roberta): XLMRobertaModel(\n",
       "      (embeddings): XLMRobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): XLMRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x XLMRobertaLayer(\n",
       "            (attention): XLMRobertaAttention(\n",
       "              (self): XLMRobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): XLMRobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): XLMRobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): XLMRobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): XLMRobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)\n",
    "multilingual_xlmr_model = torch.compile(multilingual_xlmr_model)\n",
    "multilingual_xlmr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual dataset\n",
    "multilingual_xlmr_predictions = load_models_df(\"multilingual_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinlingual batch size\n",
    "multilingual_xlmr_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "multilingual_xlmr_predictions_texts = multilingual_xlmr_predictions[\"burmese\"].tolist()\n",
    "multilingual_xlmr_predictions_dataset = MaskedTextDataset(multilingual_xlmr_predictions_texts, multilingual_xlmr_tokenizer)\n",
    "multilingual_xlmr_predictions_dataloader = DataLoader(\n",
    "    multilingual_xlmr_predictions_dataset, \n",
    "    batch_size=multilingual_xlmr_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "multilingual_xlmr_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    multilingual_xlmr_predictions_dataloader, \n",
    "    multilingual_xlmr_model, \n",
    "    multilingual_xlmr_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(multilingual_xlmr_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save multilingual xlmr predictions\n",
    "save_models_df(multilingual_xlmr_predictions, \"multilingual_xlmr_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilingual predictions\n",
    "multilingual_evaluation_results_datasets = {\n",
    "    model_name: load_models_df(f\"multilingual_{model_name}_predictions\") for model_name in multilingual_model_names.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   english  \\\n",
      "69601    mr qarase drove to the estate, but was told by...   \n",
      "126205   program coordinators estimate that tutors volu...   \n",
      "432052   \"i maintain my demands and the deadline still ...   \n",
      "517986                                        - i 'm not .   \n",
      "572578                                        - i 'm not .   \n",
      "712910                                          they're...   \n",
      "824587   expedition 10 is scheduled to conduct its seco...   \n",
      "1080759  fiji's great council of chiefs called for calm...   \n",
      "1599299  and studs and leather is basically heaven's on...   \n",
      "\n",
      "                                                   burmese generated  bleu  \\\n",
      "69601                                                   \\n       NaN   0.0   \n",
      "126205                                                  \\n       NaN   0.0   \n",
      "432052                                                  \\n       NaN   0.0   \n",
      "517986                                                 ...       NaN   0.0   \n",
      "572578                                                 ...       NaN   0.0   \n",
      "712910                                           သူတို့...       NaN   0.0   \n",
      "824587                                                  \\n       NaN   0.0   \n",
      "1080759                                                 \\n       NaN   0.0   \n",
      "1599299                                                 \\n       NaN   0.0   \n",
      "\n",
      "         rouge-1  rouge-2  \n",
      "69601        0.0      0.0  \n",
      "126205       0.0      0.0  \n",
      "432052       0.0      0.0  \n",
      "517986       0.0      0.0  \n",
      "572578       0.0      0.0  \n",
      "712910       0.0      0.0  \n",
      "824587       0.0      0.0  \n",
      "1080759      0.0      0.0  \n",
      "1599299      0.0      0.0  \n"
     ]
    }
   ],
   "source": [
    "tmp = multilingual_evaluation_results_datasets[\"xlmr\"]\n",
    "null_rows = tmp[tmp.isnull().any(axis=1)]\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU...\n"
     ]
    }
   ],
   "source": [
    "# compute metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    compute_metrics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display metrics\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    print(f\"Metrics scores for {model_name}:\")\n",
    "    print(f\" BLEU Score: {dataset['bleu'].mean()}\")\n",
    "    print(f\" ROUGE-1 Score: {dataset['rouge-1'].mean()}\")\n",
    "    print(f\" ROUGE-2 Score: {dataset['rouge-2'].mean()}\")\n",
    "    print(f\" ROUGE-L Score: {dataset['rouge-l'].mean()}\")\n",
    "    print(f\" chrF-S Score: {dataset['chrf-s'].mean()}\")\n",
    "    print(f\" BERT Score: {dataset['bert'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "for model_name, dataset in multilingual_evaluation_results_datasets.items():\n",
    "    save_tmp_df(dataset, f\"multilingual_{model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_mbert_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizers & models for both mbert\n",
    "multilingual_mbert_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"mbert\"])\n",
    "multilingual_mbert_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"mbert\"]).to(device)\n",
    "multilingual_mbert_model = torch.compile(multilingual_mbert_model)\n",
    "multilingual_mbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_mbert_generated_texts = multilingual_evaluation_results_datasets[\"mbert\"][\"generated\"].tolist()\n",
    "multilingual_mbert_text_dataset = TextDataset(multilingual_mbert_generated_texts)\n",
    "multilingual_mbert_dataloader = DataLoader(\n",
    "    multilingual_mbert_text_dataset, \n",
    "    batch_size=multilingual_mbert_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eaf3beb2914831b514b50606344ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Perplexity for mBert:   0%|          | 0/203447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 15:42:02.372000 13899 site-packages/torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "# Compute perplexity in batches\n",
    "multilingual_mbert_perplexity_scores = []\n",
    "for batch in tqdm(multilingual_mbert_dataloader, desc=\"Computing Perplexity for mBert\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, multilingual_mbert_tokenizer, multilingual_mbert_model)\n",
    "    multilingual_mbert_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 2.2175958210491413\n"
     ]
    }
   ],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"mbert\"][\"perplexity\"] = multilingual_mbert_perplexity_scores\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['mbert']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"mbert\"], f\"multilingual_mbert_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "multilingual_xlmr_perplexity_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for both xlmr\n",
    "multilingual_xlmr_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_names[\"xlmr\"])\n",
    "multilingual_xlmr_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_names[\"xlmr\"]).to(device)\n",
    "multilingual_xlmr_model = torch.compile(multilingual_xlmr_model)\n",
    "multilingual_xlmr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and DataLoader\n",
    "multilingual_xlmr_generated_texts = multilingual_evaluation_results_datasets[\"xlmr\"][\"generated\"].tolist()\n",
    "multilingual_xlmr_text_dataset = TextDataset(multilingual_xlmr_generated_texts)\n",
    "multilingual_xlmr_dataloader = DataLoader(\n",
    "    multilingual_xlmr_text_dataset, \n",
    "    batch_size=multilingual_xlmr_perplexity_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity in batches\n",
    "multilingual_xlmr_perplexity_scores = []\n",
    "for batch in tqdm(multilingual_xlmr_dataloader, desc=\"Computing Perplexity for XLM-R\"):\n",
    "    batch_perplexities = compute_perplexity_batch(batch, multilingual_xlmr_tokenizer, multilingual_xlmr_model)\n",
    "    multilingual_xlmr_perplexity_scores.extend(batch_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store perplexity scores in DataFrame and display\n",
    "multilingual_evaluation_results_datasets[\"xlmr\"][\"perplexity\"] = multilingual_xlmr_perplexity_scores\n",
    "print(f\"Perplexity Score: {multilingual_evaluation_results_datasets['xlmr']['perplexity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save perplexity\n",
    "save_tmp_df(multilingual_evaluation_results_datasets[\"xlmr\"], f\"multilingual_xlmr_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in multilingual_evaluation_results_datasets.keys():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"multilingual_{model_name}_metrics\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bleu\"] = metrics[\"bleu\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    multilingual_evaluation_results_datasets[model_name][\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"multilingual_{model_name}_perplexity\")\n",
    "    multilingual_evaluation_results_datasets[model_name][\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_models_df(multilingual_evaluation_results_datasets[model_name], f\"multilingual_{model_name}_evaluation_results\")\n",
    "\n",
    "    display(multilingual_evaluation_results_datasets[model_name].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, and Perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "benchmarking_models = {\n",
    "    \"lstm\": [\n",
    "        \"bpe\", \n",
    "        \"unigram\"\n",
    "    ],\n",
    "    \"multilingual\": [\n",
    "        \"mbert\", \n",
    "        \"xlmr\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_benchmarking(key, model_name):\n",
    "    df = load_models_df(f\"{key}_{model_name}_evaluation_results\")\n",
    "\n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"english\", \"bleu\", \"rouge\", \"perplexity\"]]\n",
    "\n",
    "    column_mapping = {\n",
    "        \"bleu\": f\"bleu_{model_name}\",\n",
    "        \"rouge\": f\"rouge_{model_name}\",\n",
    "        \"perplexity\": f\"perplexity_{model_name}\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "benchmarking_loaded_datasets = []\n",
    "for key, file_list in benchmarking_models.items():\n",
    "    df_list= [load_and_rename_columns_multilingual(file) for file in file_list]\n",
    "    benchmarking_loaded_datasets.append(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on 'english' using an outer join\n",
    "benchmarking_results = benchmarking_loaded_datasets[0]\n",
    "for df in benchmarking_loaded_datasets[1:]:\n",
    "    benchmarking_results = benchmarking_results.merge(df, on=\"english\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display merge dataset\n",
    "display(benchmarking_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Scores for Comparison\n",
    "Get mean BLEU, ROUGE, and Perplexity for LSTM (BPE & Unigram), mBERT, and XLM-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model names and their respective column prefixes\n",
    "benchmarking_model_names = [\"LSTM BPE\", \"LSTM Unigram\", \"mBERT\", \"XLM-R\"]\n",
    "benchmarking_column_prefixes = [\"bpe\", \"unigram\", \"mBERT\", \"XLM-R\"]\n",
    "\n",
    "# Compute mean scores dynamically using a dictionary comprehension\n",
    "benchmarking_mean_scores = {\n",
    "    model: {\n",
    "        \"BLEU\": df[f\"bleu_{prefix}\"].mean(),\n",
    "        \"ROUGE\": df[f\"rouge_{prefix}\"].mean(),\n",
    "        \"Perplexity\": df[f\"perplexity_{prefix}\"].mean(),\n",
    "    }\n",
    "    for model, prefix in zip(benchmarking_model_names, benchmarking_column_prefixes)\n",
    "}\n",
    "\n",
    "# Display mean scores\n",
    "pprint.pprint(benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Benchmarking Results\n",
    "Plot BLEU, ROUGE, and Perplexity scores for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each metric\n",
    "benchmarking_metrics = [\"BLEU\", \"ROUGE\", \"Perplexity\"]\n",
    "for metric in benchmarking_metrics:\n",
    "    values = [benchmarking_mean_scores[\"LSTM BPE\"][metric], \n",
    "              benchmarking_mean_scores[\"LSTM Unigram\"][metric], \n",
    "              benchmarking_mean_scores[\"mBERT\"][metric], \n",
    "              benchmarking_mean_scores[\"XLM-R\"][metric]]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(benchmarking_model_names, values, color=[\"blue\", \"green\", \"orange\", \"red\"])\n",
    "    plt.title(f\"{metric} Score Comparison\")\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Benchmarking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "save_models_df(benchmarking_results, \"benchmarking_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
