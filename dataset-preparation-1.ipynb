{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, SeamlessM4TForTextToText, AutoProcessor\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save gen df\n",
    "def save_gen_df(df, df_name):\n",
    "    df.to_csv(f\"gen/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save tmp df\n",
    "def save_tmp_df(df, df_name):\n",
    "    df.to_csv(f\"tmp/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load generated df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tmp df\n",
    "def load_tmp_df(df_name):\n",
    "    return pd.read_csv(f\"tmp/{df_name}.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading\n",
    "\n",
    "This step involves loading the datasets `myXNLI` and `ALT Corpus` into pandas DataFrames. \n",
    "The English and Burmese datasets from the ALT Corpus are combined to create a bilingual parallel corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myXNLI dataset loaded successfully with 392702 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load myXNLI dataset\n",
    "myxnli_path = './data/myXNLI.train.tsv'  # Path to the file\n",
    "myxnli_data = pd.read_csv(myxnli_path, sep='\\t', header=0)\n",
    "print(f\"myXNLI dataset loaded successfully with {len(myxnli_data)} records.\")\n",
    "display(myxnli_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT English dataset loaded successfully with 19908 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT English data\n",
    "alt_en_path = './data/ALT_data_en.txt'  # Path to the English ALT corpus\n",
    "alt_en_data = pd.read_csv(alt_en_path, sep='\\t', header=None, names=[\"ID\", \"English_Sentence\"])\n",
    "print(f\"ALT English dataset loaded successfully with {len(alt_en_data)} records.\")\n",
    "display(alt_en_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT Burmese dataset loaded successfully with 19265 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load ALT Burmese data\n",
    "alt_my_path = './data/ALT_data_my.txt'  # Path to the Burmese ALT corpus\n",
    "alt_my_data = pd.read_csv(alt_my_path, sep='\\t', header=None, names=[\"ID\", \"Burmese_Sentence\"])\n",
    "print(f\"ALT Burmese dataset loaded successfully with {len(alt_my_data)} records.\")\n",
    "display(alt_my_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT combined dataset created successfully with 19173 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 31-5 in Pool C of...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 31-5 in Pool C of...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 16-5 at half time but were matched b...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine ALT datasets (if IDs match)\n",
    "alt_combined = pd.merge(alt_en_data, alt_my_data, on=\"ID\")\n",
    "print(f\"ALT combined dataset created successfully with {len(alt_combined)} records.\")\n",
    "display(alt_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n",
    "This step focuses on cleaning the datasets to prepare them for further processing. The cleaning operations include:\n",
    "1. Removing duplicate entries.\n",
    "2. Handling missing values.\n",
    "3. Removing non-standard characters or symbols unrelated to the Burmese or English language.\n",
    "4. Ensuring consistent formatting.\n",
    "\n",
    "The cleaned datasets will be ready for normalization and tokenization in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning myXNLI dataset...\n",
      "myXNLI dataset cleaned successfully.\n",
      "Original Records: 392702.\n",
      "Remaining records: 392682.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  One of our number will carry out your instruct...   \n",
       "3     fiction  entailment  How do you know? All this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning myXNLI dataset\n",
    "print(\"Cleaning myXNLI dataset...\")\n",
    "myxnli_cleaned = myxnli_data.drop_duplicates()  # Remove duplicates\n",
    "myxnli_cleaned = myxnli_cleaned.dropna()  # Remove rows with missing values\n",
    "#myxnli_cleaned = myxnli_cleaned.replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"myXNLI dataset cleaned successfully.\")\n",
    "print(f\"Original Records: {len(myxnli_data)}.\")\n",
    "print(f\"Remaining records: {len(myxnli_cleaned)}.\")\n",
    "display(myxnli_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned myXNLI dataset\n",
    "save_gen_df(myxnli_cleaned, \"myxnli_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT English dataset...\n",
      "ALT English dataset cleaned successfully.\n",
      "Original records: 19908.\n",
      "Remaining records: 19908.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...\n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...\n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...\n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...\n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT English data\n",
    "print(\"Cleaning ALT English dataset...\")\n",
    "alt_en_cleaned = alt_en_data.drop_duplicates()  # Remove duplicates\n",
    "alt_en_cleaned = alt_en_cleaned.dropna()  # Remove rows with missing values\n",
    "alt_en_cleaned[\"English_Sentence\"] = alt_en_cleaned[\"English_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT English dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_en_data)}.\")\n",
    "print(f\"Remaining records: {len(alt_en_cleaned)}.\")\n",
    "display(alt_en_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ALT Burmese dataset...\n",
      "ALT Burmese dataset cleaned successfully.\n",
      "Original records: 19265\n",
      "Remaining records: 19258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   Burmese_Sentence\n",
       "0  SNT.80188.1  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...\n",
       "1  SNT.80188.2  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...\n",
       "2  SNT.80188.3  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...\n",
       "3  SNT.80188.4  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...\n",
       "4  SNT.80188.5  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning ALT Burmese data\n",
    "print(\"Cleaning ALT Burmese dataset...\")\n",
    "alt_my_cleaned = alt_my_data.drop_duplicates()  # Remove duplicates\n",
    "alt_my_cleaned = alt_my_cleaned.dropna()  # Remove rows with missing values\n",
    "#alt_my_cleaned[\"Burmese_Sentence\"] = alt_my_cleaned[\"Burmese_Sentence\"].replace(r'[^\\w\\s]', '', regex=True)  # Remove non-standard characters\n",
    "print(f\"ALT Burmese dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_my_data)}\")\n",
    "print(f\"Remaining records: {len(alt_my_cleaned)}\")\n",
    "display(alt_my_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning combined ALT dataset...\n",
      "Combined ALT dataset cleaned successfully.\n",
      "Original records: 19173\n",
      "Remaining records: 19166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>Italy have defeated Portugal 315 in Pool C of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>Italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  Italy have defeated Portugal 315 in Pool C of ...   \n",
       "1  SNT.80188.2  Andrea Masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  Despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  Portugal never gave up and David Penalva score...   \n",
       "4  SNT.80188.5  Italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine cleaned ALT datasets\n",
    "print(\"Cleaning combined ALT dataset...\")\n",
    "alt_combined_cleaned = pd.merge(alt_en_cleaned, alt_my_cleaned, on=\"ID\")\n",
    "print(f\"Combined ALT dataset cleaned successfully.\")\n",
    "print(f\"Original records: {len(alt_combined)}\")\n",
    "print(f\"Remaining records: {len(alt_combined_cleaned)}\")\n",
    "display(alt_combined_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned combined ALT dataset\n",
    "save_gen_df(alt_combined_cleaned, \"alt_combined_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Normalization\n",
    "This step normalizes the text data to ensure consistency across datasets. The normalization process includes:\n",
    "1. Applying Unicode normalization to handle encoding inconsistencies.\n",
    "2. Standardizing text formatting by converting all text to lowercase and standardizing punctuation.\n",
    "3. Normalizing diacritical marks and stacked consonants in the Burmese text to improve text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    # Apply Unicode normalization\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Convert to lowercase\n",
    "    normalized_text = normalized_text.lower()\n",
    "    # Standardize punctuation (e.g., replace unusual punctuation marks)\n",
    "    normalized_text = normalized_text.replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize Burmese text (handles diacritical marks and stacked consonants)\n",
    "def normalize_burmese(text):\n",
    "    if pd.isnull(text):\n",
    "        return text  # Skip null values\n",
    "    normalized_text = unicodedata.normalize('NFKC', text)\n",
    "    # Additional Burmese-specific normalization can be added here if needed\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing myXNLI dataset...\n",
      "myXNLI dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1_en</th>\n",
       "      <th>sentence2_en</th>\n",
       "      <th>sentence1_my</th>\n",
       "      <th>sentence2_my</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>conceptually cream skimming has two basic dime...</td>\n",
       "      <td>product and geography are what make cream skim...</td>\n",
       "      <td>သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...</td>\n",
       "      <td>ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>you lose the things to the following level if ...</td>\n",
       "      <td>ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...</td>\n",
       "      <td>လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>one of our number will carry out your instruct...</td>\n",
       "      <td>a member of my team will execute your orders w...</td>\n",
       "      <td>ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...</td>\n",
       "      <td>ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>how do you know? all this is their information...</td>\n",
       "      <td>this information belongs to them.</td>\n",
       "      <td>သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...</td>\n",
       "      <td>ဒီအချက်အလက်က သူတို့ပိုင်တယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>the tennis shoes have a range of prices.</td>\n",
       "      <td>ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...</td>\n",
       "      <td>တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre       label                                       sentence1_en  \\\n",
       "0  government     neutral  conceptually cream skimming has two basic dime...   \n",
       "1   telephone  entailment  you know during the season and i guess at at y...   \n",
       "2     fiction  entailment  one of our number will carry out your instruct...   \n",
       "3     fiction  entailment  how do you know? all this is their information...   \n",
       "4   telephone     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                        sentence2_en  \\\n",
       "0  product and geography are what make cream skim...   \n",
       "1  you lose the things to the following level if ...   \n",
       "2  a member of my team will execute your orders w...   \n",
       "3                  this information belongs to them.   \n",
       "4           the tennis shoes have a range of prices.   \n",
       "\n",
       "                                        sentence1_my  \\\n",
       "0  သဘောတရားအရ ခရင်မ်စိမ်ခြင်းတွင် အခြေခံအတိုင်းအတ...   \n",
       "1  ရာသီအတွင်း မင်းသိတယ်၊ မင်းရဲ့အဆင့်ကို ငါ ခန့်မ...   \n",
       "2  ကျွန်ုပ်တို့၏နံပါတ်တစ်ခုသည် သင့်ညွှန်ကြားချက်မ...   \n",
       "3  သင်ဘယ်လိုသိသလဲ? ဒါတွေအားလုံးဟာ သူတို့ရဲ့ အချက်...   \n",
       "4  ဟုတ်တယ် ငါမင်းကိုပြောပြမယ် ဒီတင်းနစ်ဖိနပ်တချို...   \n",
       "\n",
       "                                        sentence2_my  \n",
       "0  ထုတ်ကုန်နှင့် ပထဝီဝင်အနေအထားသည် ခရင်မ် skimmin...  \n",
       "1  လူတွေပြန်ခေါ်ရင် အောက်ပါအဆင့်အထိ ဆုံးရှုံးသွား...  \n",
       "2  ကျွန်ုပ်၏အဖွဲ့သားတစ်ဦးသည် သင်၏အမိန့်စာများကို ...  \n",
       "3                       ဒီအချက်အလက်က သူတို့ပိုင်တယ်။  \n",
       "4    တင်းနစ်ဖိနပ်များသည် ဈေးနှုန်းအမျိုးမျိုးရှိသည်။  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize myXNLI cleaned dataset\n",
    "print(\"Normalizing myXNLI dataset...\")\n",
    "myxnli_normalized = load_gen_df(\"myxnli_cleaned\")\n",
    "\n",
    "# Normalize English columns\n",
    "myxnli_normalized[\"sentence1_en\"] = myxnli_normalized[\"sentence1_en\"].apply(normalize_text)\n",
    "myxnli_normalized[\"sentence2_en\"] = myxnli_normalized[\"sentence2_en\"].apply(normalize_text)\n",
    "\n",
    "# Normalize Burmese columns\n",
    "myxnli_normalized[\"sentence1_my\"] = myxnli_normalized[\"sentence1_my\"].apply(normalize_burmese)\n",
    "myxnli_normalized[\"sentence2_my\"] = myxnli_normalized[\"sentence2_my\"].apply(normalize_burmese)\n",
    "\n",
    "print(f\"myXNLI dataset normalized successfully.\")\n",
    "display(myxnli_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized myXNLI dataset\n",
    "save_gen_df(myxnli_normalized, \"myxnli_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing combined ALT dataset...\n",
      "Combined ALT dataset normalized successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Burmese_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNT.80188.1</td>\n",
       "      <td>italy have defeated portugal 315 in pool c of ...</td>\n",
       "      <td>ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNT.80188.2</td>\n",
       "      <td>andrea masi opened the scoring in the fourth m...</td>\n",
       "      <td>အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNT.80188.3</td>\n",
       "      <td>despite controlling the game for much of the f...</td>\n",
       "      <td>ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNT.80188.4</td>\n",
       "      <td>portugal never gave up and david penalva score...</td>\n",
       "      <td>ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNT.80188.5</td>\n",
       "      <td>italy led 165 at half time but were matched by...</td>\n",
       "      <td>အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                   English_Sentence  \\\n",
       "0  SNT.80188.1  italy have defeated portugal 315 in pool c of ...   \n",
       "1  SNT.80188.2  andrea masi opened the scoring in the fourth m...   \n",
       "2  SNT.80188.3  despite controlling the game for much of the f...   \n",
       "3  SNT.80188.4  portugal never gave up and david penalva score...   \n",
       "4  SNT.80188.5  italy led 165 at half time but were matched by...   \n",
       "\n",
       "                                    Burmese_Sentence  \n",
       "0  ပြင်သစ်နိုင်ငံ ပါရီမြို့ ပါ့ဒက်စ် ပရင့်စက် ၌ ၂...  \n",
       "1  အန်ဒရီယာ မာစီ သည် အီတလီ အတွက် စမ်းသပ်မှု တစ်ခု...  \n",
       "2  ပထမ တစ်ဝက် ၏ တော်တော်များများ အတွက် ကစားပွဲ ကိ...  \n",
       "3  ပေါ်တူဂီ သည် ဘယ်သောအခါမှ စွန့်လွှတ်မှု မရှိခဲ့...  \n",
       "4  အီတလီ သည် ပထမပိုင်း ၌ ၁၆-၅ ဖြင့် ဦးဆောင်ခဲ့ သေ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize combined ALT cleaned dataset\n",
    "print(\"Normalizing combined ALT dataset...\")\n",
    "alt_combined_normalized = load_gen_df(\"alt_combined_cleaned\")\n",
    "alt_combined_normalized[\"English_Sentence\"] = alt_combined_normalized[\"English_Sentence\"].apply(normalize_text)\n",
    "alt_combined_normalized[\"Burmese_Sentence\"] = alt_combined_normalized[\"Burmese_Sentence\"].apply(normalize_burmese)\n",
    "print(f\"Combined ALT dataset normalized successfully.\")\n",
    "display(alt_combined_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized ALT dataset\n",
    "save_gen_df(alt_combined_normalized, \"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Back-Translation Augmentation\n",
    "This step applies back-translation augmentation to the `myXNLI` and `ALT Combined` datasets. Models (`facebook/nllb-200-distilled-600M` and `facebook/hf-seamless-m4t-large`) are used to generate synthetic data. \n",
    "Results are stored in additional columns for evaluation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized myXNLI and Combined ALT  dataset\n",
    "myxnli_back_translated = load_gen_df(\"myxnli_normalized\")\n",
    "alt_combined_back_translated = load_gen_df(\"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/nllb-200-distilled-600M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLLB model and tokenizer\n",
    "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name).to(device)\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using NLLB\n",
    "def back_translate_nllb(text, src_lang=\"eng_Latn\", tgt_lang=\"mya_Mymr\"):\n",
    "    try:\n",
    "        # Forward translation: English -> Burmese\n",
    "        inputs = nllb_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        translated_tokens = nllb_model.generate(\n",
    "            **inputs, forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "        )\n",
    "        translated_text = nllb_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Backward translation: Burmese -> English\n",
    "        back_inputs = nllb_tokenizer(translated_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        back_translated_tokens = nllb_model.generate(\n",
    "            **back_inputs, forced_bos_token_id=nllb_tokenizer.convert_tokens_to_ids(src_lang)\n",
    "        )\n",
    "        back_translated_text = nllb_tokenizer.batch_decode(back_translated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return translated_text, back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### myXNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to myXNLI dataset (sentences 1)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e98fdc35f54c80b7ddb0a121f77ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the sentences 1 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 1)...\")\n",
    "myxnli_nllb_back_translated_start_index_1 = 0\n",
    "\n",
    "with open(\"tmp/myxnli_nllb_back_translated_1.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_nllb_back_translated_start_index_1 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_nllb_back_translated_start_index_1:].iterrows(), \n",
    "                           total=len(myxnli_back_translated) - myxnli_nllb_back_translated_start_index_1):\n",
    "        original = row[\"sentence1_en\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 1)\n",
    "tmp_myxnli_nllb_back_translated_1 = load_tmp_df(\"myxnli_nllb_back_translated_1\")\n",
    "myxnli_back_translated[\"nllb_translated_s1\"] = tmp_myxnli_nllb_back_translated_1[\"translated\"]\n",
    "myxnli_back_translated[\"nllb_back_translated_s1\"] = tmp_myxnli_nllb_back_translated_1[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to myXNLI dataset (sentences 2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22945169d5d4407286b986520569a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/295188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the sentences 2 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 2)...\")\n",
    "myxnli_nllb_back_translated_start_index_2 = 97494\n",
    "with open(\"tmp/myxnli_nllb_back_translated_2.csv\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_nllb_back_translated_start_index_2 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_nllb_back_translated_start_index_2:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_nllb_back_translated_start_index_2):\n",
    "        original = row[\"sentence2_en\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\") if original is not None else None\n",
    "        translated = translated.replace('\"', \"'\") if translated is not None else None\n",
    "        back_translated = back_translated.replace('\"', \"'\") if back_translated is not None else None\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 2)\n",
    "tmp_myxnli_nllb_back_translated_2 = load_tmp_df(\"myxnli_nllb_back_translated_2\")\n",
    "myxnli_back_translated[\"nllb_translated_s2\"] = tmp_myxnli_nllb_back_translated_2[\"translated\"]\n",
    "myxnli_back_translated[\"nllb_back_translated_s2\"] = tmp_myxnli_nllb_back_translated_2[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to alt combined dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbbbd77dd214d19aa9f2416c8d5d00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the alt combined dataset\n",
    "print(\"Applying back-translation to alt combined dataset...\")\n",
    "alt_combined_nllb_back_translated_start_index = 0\n",
    "with open(\"tmp/alt_combined_nllb_back_translated.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if alt_combined_nllb_back_translated_start_index == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(alt_combined_back_translated.iloc[alt_combined_nllb_back_translated_start_index:].iterrows(),\n",
    "                       total=len(alt_combined_back_translated) - alt_combined_nllb_back_translated_start_index):\n",
    "        original = row[\"English_Sentence\"]\n",
    "        translated, back_translated = back_translate_nllb(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated combined ALT dataset\n",
    "tmp_alt_combined_nllb_back_translated = load_tmp_df(\"alt_combined_nllb_back_translated\")\n",
    "alt_combined_back_translated[\"nllb_translated\"] = tmp_alt_combined_nllb_back_translated[\"translated\"]\n",
    "alt_combined_back_translated[\"nllb_back_translated\"] = tmp_alt_combined_nllb_back_translated[\"back_translated\"]\n",
    "display(alt_combined_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facebook/hf-seamless-m4t-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seamless m4t model and processor\n",
    "seamless_m4t_model_name = \"facebook/hf-seamless-m4t-large\"\n",
    "seamless_m4t_model = SeamlessM4TForTextToText.from_pretrained(seamless_m4t_model_name).to(device)\n",
    "seamless_m4t_processor = AutoProcessor.from_pretrained(seamless_m4t_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for back-translation using seamless_m4t\n",
    "def back_translate_seamless_m4t(text, src_lang=\"eng\", tgt_lang=\"mya\"):\n",
    "    try:\n",
    "        # Forward translation: English -> Burmese\n",
    "        text_inputs = seamless_m4t_processor(text, src_lang=src_lang, return_tensors=\"pt\", padding=True).to(device)\n",
    "        output_tokens = seamless_m4t_model.generate(**text_inputs, tgt_lang=tgt_lang)\n",
    "        translated_text = seamless_m4t_processor.decode(output_tokens[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Backward translation: Burmese -> English\n",
    "        back_text_inputs = seamless_m4t_processor(translated_text, src_lang=tgt_lang, return_tensors=\"pt\", padding=True).to(device)\n",
    "        back_output_tokens = seamless_m4t_model.generate(**back_text_inputs, tgt_lang=src_lang)\n",
    "        back_translated_text = seamless_m4t_processor.decode(back_output_tokens[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        return translated_text, back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### myXNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the sentences 1 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 1)...\")\n",
    "myxnli_seamless_m4t_back_translated_start_index_1 = 0\n",
    "with open(\"tmp/myxnli_seamless_m4t_back_translated_1.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_seamless_m4t_back_translated_start_index_1 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_seamless_m4t_back_translated_start_index_1:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_seamless_m4t_back_translated_start_index_1):\n",
    "        original = row[\"sentence1_en\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 1)\n",
    "tmp_myxnli_seamless_m4t_back_translated_1 = load_tmp_df(\"myxnli_seamless_m4t_back_translated_1\")\n",
    "myxnli_back_translated[\"seamless_translated_s1\"] = tmp_myxnli_seamless_m4t_back_translated_1[\"translated\"]\n",
    "myxnli_back_translated[\"seamless_back_translated_s1\"] = tmp_myxnli_seamless_m4t_back_translated_1[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply back-translation to the sentences 2 in myXNLI\n",
    "print(\"Applying back-translation to myXNLI dataset (sentences 2)...\")\n",
    "myxnli_seamless_m4t_back_translated_start_index_2 = 0\n",
    "with open(\"tmp/myxnli_seamless_m4t_back_translated_2.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if myxnli_seamless_m4t_back_translated_start_index_2 == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(myxnli_back_translated.iloc[myxnli_seamless_m4t_back_translated_start_index_2:].iterrows(), \n",
    "                       total=len(myxnli_back_translated) - myxnli_seamless_m4t_back_translated_start_index_2):\n",
    "        original = row[\"sentence2_en\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated myXNLI dataset (sentences 2)\n",
    "tmp_myxnli_seamless_m4t_back_translated_2 = load_tmp_df(\"myxnli_seamless_m4t_back_translated_2\")\n",
    "myxnli_back_translated[\"seamless_translated_s2\"] = tmp_myxnli_seamless_m4t_back_translated_2[\"translated\"]\n",
    "myxnli_back_translated[\"seamless_back_translated_s2\"] = tmp_myxnli_seamless_m4t_back_translated_2[\"back_translated\"]\n",
    "display(myxnli_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ALT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying back-translation to alt combined dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227bfb5e4f7d442894176b8ca21afd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply back-translation to the alt combined dataset\n",
    "print(\"Applying back-translation to alt combined dataset...\")\n",
    "alt_combined_seamless_m4t_back_translated_start_index = 0\n",
    "with open(\"tmp/alt_combined_seamless_m4t_back_translated.csv\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "    if alt_combined_seamless_m4t_back_translated_start_index == 0:\n",
    "        # Write CSV header\n",
    "        f.write(\"original,translated,back_translated\\n\")\n",
    "    \n",
    "    # Process rows and write results\n",
    "    for index, row in tqdm(alt_combined_back_translated.iloc[alt_combined_seamless_m4t_back_translated_start_index:].iterrows(), \n",
    "                       total=len(alt_combined_back_translated) - alt_combined_seamless_m4t_back_translated_start_index):\n",
    "        original = row[\"English_Sentence\"]\n",
    "        translated, back_translated = back_translate_seamless_m4t(original)\n",
    "        \n",
    "        # Replace double quotes with single quotes\n",
    "        original = original.replace('\"', \"'\")\n",
    "        translated = translated.replace('\"', \"'\")\n",
    "        back_translated = back_translated.replace('\"', \"'\")\n",
    "        \n",
    "        # Wrap text in double quotes if it contains a comma\n",
    "        original = f'\"{original}\"' if ',' in original else original\n",
    "        translated = f'\"{translated}\"' if ',' in translated else translated\n",
    "        back_translated = f'\"{back_translated}\"' if ',' in back_translated else back_translated\n",
    "        \n",
    "        # Write to CSV\n",
    "        if translated and back_translated:\n",
    "            f.write(f\"{original},{translated},{back_translated}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{original},,\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set and display back-translated combined ALT dataset\n",
    "tmp_alt_combined_seamless_m4t_back_translated = load_tmp_df(\"alt_combined_seamless_m4t_back_translated\")\n",
    "alt_combined_back_translated[\"seamless_translated\"] = tmp_alt_combined_seamless_m4t_back_translated[\"translated\"]\n",
    "alt_combined_back_translated[\"seamless_back_translated\"] = tmp_alt_combined_seamless_m4t_back_translated[\"back_translated\"]\n",
    "display(alt_combined_back_translated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Back-Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back-translated myXNLI dataset and combined ALT dataset\n",
    "save_gen_df(myxnli_back_translated, \"myxnli_back_translated\")\n",
    "save_gen_df(alt_combined_back_translated, \"alt_combined_back_translated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pseudo-Parallel Corpus Creation\n",
    "\n",
    "This step involves aligning monolingual English and Burmese text from the datasets to create pseudo-parallel corpora by using models (`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`). \n",
    "Semantic similarity methods are used to identify pairs of sentences with similar meanings. \n",
    "The resulting aligned corpus enhances the dataset and is valuable for low-resource language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalized myXNLI and Combined ALT  dataset\n",
    "myxnli_corpus = load_gen_df(\"myxnli_normalized\")\n",
    "alt_combined_corpus = load_gen_df(\"alt_combined_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract monolingual text\n",
    "corpus_english_sentences = list(myxnli_corpus[\"sentence1_en\"]) + list(myxnli_corpus[\"sentence2_en\"]) + list(alt_combined_corpus[\"English_Sentence\"])\n",
    "corpus_burmese_sentences = list(myxnli_corpus[\"sentence1_my\"]) + list(myxnli_corpus[\"sentence2_my\"]) + list(alt_combined_corpus[\"Burmese_Sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained multilingual embedding model\n",
    "minilm_embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "minilm_embedding_model = SentenceTransformer(minilm_embedding_model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for embeddings and checkpoints\n",
    "minilm_corpus_english_embedding_file = \"gen/minilm_corpus_english_embeddings.npy\"\n",
    "minilm_corpus_burmese_embedding_file = \"gen/minilm_corpus_burmese_embeddings.npy\"\n",
    "minilm_corpus_embedding_checkpoint_file = \"tmp/minilm_corpus_embedding_checkpoint.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing progress if available\n",
    "if os.path.exists(\"gen/minilm_pseudo_parallel_corpus.csv\"):\n",
    "    progress_df = load_gen_df(\"minilm_pseudo_parallel_corpus\")\n",
    "    minilm_corpus_aligned_pairs = progress_df.to_dict(orient=\"records\")\n",
    "else:\n",
    "    minilm_corpus_aligned_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings and checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# Initialize progress\n",
    "if os.path.exists(minilm_corpus_english_embedding_file) and os.path.exists(minilm_corpus_burmese_embedding_file) and os.path.exists(minilm_corpus_embedding_checkpoint_file):\n",
    "    print(\"Loading existing embeddings and checkpoint...\")\n",
    "    minilm_corpus_english_embeddings = np.load(minilm_corpus_english_embedding_file)\n",
    "    minilm_corpus_burmese_embeddings = np.load(minilm_corpus_burmese_embedding_file)\n",
    "    with open(minilm_corpus_embedding_checkpoint_file, \"r\") as f:\n",
    "        minilm_corpus_embeddings_start_index = int(f.read().strip())\n",
    "else:\n",
    "    print(\"Starting fresh embedding computation...\")\n",
    "    minilm_corpus_english_embeddings = None\n",
    "    minilm_corpus_burmese_embeddings = None\n",
    "    minilm_corpus_embeddings_start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c2c44df3cc43f29f7a75cac4e07f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding computation completed.\n"
     ]
    }
   ],
   "source": [
    "# Process sentences in batches and save incrementally\n",
    "minilm_corpus_embeddings_batch_size = 100\n",
    "for i in tqdm(range(minilm_corpus_embeddings_start_index, len(corpus_english_sentences), minilm_corpus_embeddings_batch_size)):\n",
    "    \n",
    "    # Get current batch of sentences\n",
    "    english_batch = corpus_english_sentences[i:i+minilm_corpus_embeddings_batch_size]\n",
    "    burmese_batch = corpus_burmese_sentences[i:i+minilm_corpus_embeddings_batch_size]\n",
    "    \n",
    "    # Compute embeddings\n",
    "    english_batch_embeddings = minilm_embedding_model.encode(english_batch)\n",
    "    burmese_batch_embeddings = minilm_embedding_model.encode(burmese_batch)\n",
    "    \n",
    "    # Merge with existing embeddings\n",
    "    if minilm_corpus_english_embeddings is None:\n",
    "        minilm_corpus_english_embeddings = english_batch_embeddings\n",
    "        minilm_corpus_burmese_embeddings = burmese_batch_embeddings\n",
    "    else:\n",
    "        minilm_corpus_english_embeddings = np.vstack([minilm_corpus_english_embeddings, english_batch_embeddings])\n",
    "        minilm_corpus_burmese_embeddings = np.vstack([minilm_corpus_burmese_embeddings, burmese_batch_embeddings])\n",
    "\n",
    "    # Save progress\n",
    "    np.save(minilm_corpus_english_embedding_file, minilm_corpus_english_embeddings)\n",
    "    np.save(minilm_corpus_burmese_embedding_file, minilm_corpus_burmese_embeddings)\n",
    "    with open(minilm_corpus_embedding_checkpoint_file, \"w\") as f:\n",
    "        f.write(str(i + minilm_corpus_embeddings_batch_size))\n",
    "\n",
    "print(\"Embedding computation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Similarity and Filter High-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk and max\n",
    "minilm_corpus_chunk = 2\n",
    "minilm_corpus_max = 10000 * minilm_corpus_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "minilm_corpus_checkpoint_file = f\"tmp/minilm_pseudo_parallel_last_index_{minilm_corpus_chunk}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing progress if available\n",
    "if os.path.exists(f\"tmp/minilm_pseudo_parallel_corpus_{minilm_corpus_chunk}.csv\"):\n",
    "    corpus_progress_df = load_tmp_df(f\"minilm_pseudo_parallel_corpus_{minilm_corpus_chunk}\")\n",
    "    minilm_corpus_aligned_pairs = corpus_progress_df.to_dict(orient=\"records\")\n",
    "else:\n",
    "    minilm_corpus_aligned_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index: 10000\n"
     ]
    }
   ],
   "source": [
    "# Determine the starting index\n",
    "if os.path.exists(minilm_corpus_checkpoint_file):\n",
    "    with open(minilm_corpus_checkpoint_file, \"r\") as f:\n",
    "        minilm_corpus_start_index = int(f.read().strip())\n",
    "else:\n",
    "    minilm_corpus_start_index = minilm_corpus_max - 10000\n",
    "\n",
    "print(f\"Start Index: {minilm_corpus_start_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b5f7ee60049f79238336e94430d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e4244e6c548409b76ef8f5a4d25f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batch 10000:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98485367479c42888a818d2b480d5b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10000:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23fbcb220a94470815718b78fc1ed2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10001:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5c41ea789441a6b12775bb1b834802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10002:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440de5c67d52465a9dc9abd3fb4f9250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10003:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed48e6839e744e7b7a91e503753553d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10004:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418844271e9f4c929122050ccfef257f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10005:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90272eddfca491687fb199e5702c328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10006:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6b5878a2cd46eb951867ae2de965ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10007:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827fdf55fe2a4b90bc6e23c85b2902fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10008:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2e446dd286468288d74264cc8553c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10009:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b56521c11ee4850864e4a075d6a96bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10010:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b6c154f9944353b7027e7f1d9441ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10011:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c8ce2015a04d92bc68d84040f5a7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10012:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614c24b2d8fc473e823fa009fe3d94ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10013:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0b5ae12ed44c7fba3b57b43347f796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10014:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e450e86e8e64dc482fcd11caff0959f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10015:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eac03dea86473485ecc5dabe2e5baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10016:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6702e7f8ad09489abc2be499d90f16e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10017:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af0306e0eab4c7e957a37587ff98ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10018:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13457a2d1fa046baa361b89f915b9555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10019:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018dfaec3a154dea9cf3f7524e9d967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10020:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d900f0e5ed96487a885d5f205bec8dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10021:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61631b453cac4d00af44510a4597441e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10022:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a57e8b24e645b282a1882767de93b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10023:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f217255dc14306b1c2d81df560b48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10024:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a648a7fbd435bbc3fa9838344235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10025:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555733e5f6ea49ad995ad510488e0742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10026:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb182f589b884ff69698fabf8726ab95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10027:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b200c5ea405b4f62aac9be5bc20e5940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10028:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5283012f3fa4e29b4fb899696ef250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10029:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e0a798fd0642a187a442ec1901a1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10030:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6d3e2de3df4f82adf0f066e8fa46a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10031:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45d5e526304f0ab0d999b4743dec25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10032:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8b7d113d984b12ab25c4b42d194035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10033:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a309f33877e474d9460fe09f7a1e115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10034:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80e8db5e3ce45929aa7ef656a7fbcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10035:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576c073fa5ba4dab90d0821013318992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10036:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5ac46b3c6b4a578fd53f2bf7fcc430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10037:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e56f0f3f244180810a161574a91149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10038:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7c7094b80c41b2ab84584035ef4726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10039:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571d8ad55e4246728269028d8930d290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10040:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce4a92c7f894da5985a08200576a9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10041:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13a001ee7f14bf29ef2deeabf3eb2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10042:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27b619c55654e01b1d8626a333cb27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10043:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436a120665f04c908d7027f815b10047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10044:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9398c7da9240cca766fdd99f1f58f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10045:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b5139a7f8640a28bdec4e50ebb6d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10046:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01deb80fa49488ca1d030c44d5f935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10047:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d204275bb8554869b1d5adc19f7944eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10048:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8955de1e4adf4142bda761c218080b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10049:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041cb408be0a43d8b556f56c7e1a3761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10050:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715305d10f4d48d890f5defad9fdede2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10051:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53f7bdc112342f2b3f2e560f4114266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10052:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab7261cbc4423db412de381629bcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10053:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271a525e96e949ef8d4a0750ed054f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10054:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46deb82741d47cfaa613ea22b1c0aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10055:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7cf47bee8c4240b8c436762d3aa0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10056:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024a46c49ed2442695424120200b125c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10057:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050431df311e49988f48bedbb85c222a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10058:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bcc83e54104ccf8ef68f02bf622d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10059:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9db88233830420b87a70fe813a71b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10060:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35129f2b55c47f6a0f558c6f7bcb287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10061:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dc7a41f504480793148f7695635a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10062:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd8055e11a644a1a389335fca7fac2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10063:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba960e7439d34e768a63078b5190dcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10064:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f86564a79ae45d493e7ca946c81856e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10065:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9f0fe35a3141c6ae57d34b74eac6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10066:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b1d846fa994d69b5633279d7032b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10067:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416f3365840b49fdbdab5ed49ee12f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10068:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb08ed8ef6b4fa1ab8a453ff629d7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning sentences for index 10069:   0%|          | 0/804530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Align sentences based on similarity threshold\n",
    "minilm_threshold = 0.8\n",
    "minilm_corpus_batch_size = 1000\n",
    "\n",
    "# Process sentences in batches\n",
    "for batch_start in tqdm(range(minilm_corpus_start_index, minilm_corpus_max, minilm_corpus_batch_size)):\n",
    "    batch_end = min(batch_start + minilm_corpus_batch_size, len(corpus_english_sentences))  # Ensure no overflow\n",
    "    english_batch = corpus_english_sentences[batch_start:batch_end]\n",
    "\n",
    "    for i, english_sentence in tqdm(enumerate(english_batch), desc=f\"Processing batch {batch_start}\", total=len(english_batch), leave=False):\n",
    "        current_index = batch_start + i  # Absolute index in the full corpus\n",
    "        for j, burmese_sentence in tqdm(enumerate(corpus_burmese_sentences), desc=f\"Aligning sentences for index {current_index}\", total=len(corpus_burmese_sentences), leave=False, mininterval=1.0):\n",
    "            # Compute similarity score\n",
    "            similarity_score = util.cos_sim(minilm_corpus_english_embeddings[current_index], minilm_corpus_burmese_embeddings[j]).item()\n",
    "            if similarity_score > minilm_threshold:\n",
    "                minilm_corpus_aligned_pairs.append({\n",
    "                    \"english\": english_sentence,\n",
    "                    \"burmese\": burmese_sentence,\n",
    "                    \"similarity_score\": similarity_score\n",
    "                })\n",
    "\n",
    "    # Save progress incrementally after each batch\n",
    "    progress_df = pd.DataFrame(minilm_corpus_aligned_pairs)\n",
    "    save_tmp_df(progress_df, f\"minilm_pseudo_parallel_corpus_{minilm_corpus_chunk}\")\n",
    "\n",
    "    # Update checkpoint file after each batch\n",
    "    with open(minilm_corpus_checkpoint_file, \"w\") as f:\n",
    "        f.write(str(batch_end))\n",
    "\n",
    "print(\"Pseudo-parallel corpus creation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
