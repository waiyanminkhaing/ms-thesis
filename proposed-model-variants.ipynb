{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded from S3 successfully!\n"
     ]
    }
   ],
   "source": [
    "# S3 Setup\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"ms-thesis-sagemaker\"  # Replace with your S3 bucket\n",
    "s3_file_path = \"mbert_bpe_hf_dataset.zip\"  # Replace with the file name in S3\n",
    "local_zip_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/mbert_bpe_hf_dataset.zip\"  # Where to save in SageMaker\n",
    "\n",
    "# Download the ZIP file from S3\n",
    "s3.download_file(bucket_name, s3_file_path, local_zip_path)\n",
    "print(\"ZIP file downloaded from S3 successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "extract_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/\"  # Where to extract\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"ZIP file extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file deleted to free space.\n"
     ]
    }
   ],
   "source": [
    "os.remove(local_zip_path)\n",
    "print(\"ZIP file deleted to free space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to zip and upload\n",
    "def zip_and_upload_to_s3(source_folder):\n",
    "    bucket_name = \"ms-thesis-sagemaker\"\n",
    "\n",
    "    zip_file = f\"{source_folder}.zip\"\n",
    "    s3_key = f\"uploads/{zip_file}\"\n",
    "\n",
    "    # Create a zip archive\n",
    "    shutil.make_archive(zip_file.replace(\".zip\", \"\"), 'zip', source_folder)\n",
    "    print(f\"Zipped {source_folder} -> {zip_file}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.upload_file(zip_file, bucket_name, s3_key)\n",
    "    print(f\"Uploaded to S3: s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "    return f\"s3://{bucket_name}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip logs\n",
    "zip_and_upload_to_s3(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "zip_and_upload_to_s3(\"model-variants/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "zip_and_upload_to_s3(\"model-variants/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from utils.dataframe import (\n",
    "    load_gen_df, save_tmp_df, load_tmp_df,\n",
    "    save_model_variants_df, load_model_variants_df,\n",
    "    save_model_variants_hf, load_model_variants_hf,\n",
    "    convert_to_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    generate_masked_predictions_hf, generate_mt5_predictions,\n",
    "    compute_metrics_hf,\n",
    "    convert_to_mean_scores_df,\n",
    "    get_fine_tuned_model, get_embedded_fine_tuned_model, get_distilled_fine_tuned_model,\n",
    "    compute_multilingual_masked_perplexity_single, compute_multilingual_mt5_perplexity_single,\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "train_model_names = {\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"mT5\": \"google/mt5-small\",\n",
    "    \"XLM-R\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizers\n",
    "train_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(train_model_names[\"mBERT\"]),\n",
    "    \"mT5\": AutoTokenizer.from_pretrained(train_model_names[\"mT5\"], use_fast=False, legacy=True),\n",
    "    \"XLM-R\": AutoTokenizer.from_pretrained(train_model_names[\"XLM-R\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agrs = {\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 1000,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"disable_tqdm\": False,\n",
    "    \"label_names\": [\"labels\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, model_name, is_student):\n",
    "    \"\"\"\n",
    "    Applies LoRA for efficient fine-tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select correct LoRA target layers\n",
    "    if \"t5\" in model_name.lower():\n",
    "        target_modules = [\"q\", \"v\"]  # LoRA for T5/mT5\n",
    "    else:\n",
    "        target_modules = [\"query\", \"value\"]  # LoRA for BERT\n",
    "\n",
    "    # Define LoRA Configuration\n",
    "    if is_student:\n",
    "        lora_config = LoraConfig(\n",
    "            r=4,                    # Rank of LoRA matrices\n",
    "            lora_alpha=8,           # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.05,      # Prevents overfitting\n",
    "            bias=\"none\"\n",
    "        )\n",
    "    else:\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,                    # Rank of LoRA matrices\n",
    "            lora_alpha=16,          # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.1,       # Prevents overfitting\n",
    "            bias=\"none\"\n",
    "        )\n",
    "\n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Move model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"LoRA applied to {model_name} (Target Modules: {target_modules})\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Preprocessing\n",
    "Datasets used for training:\n",
    "- myXNLI & ALT Corpus (normalized)\n",
    "- Back-translated datasets (NLLB, Seamless M4T)\n",
    "- Pseudo-parallel datasets (MiniLM, LaBSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english\": \"source\",\n",
    "        \"burmese\": \"target\",\n",
    "        \"english_back_translated\": \"source\",\n",
    "        \"burmese_translated\": \"target\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"source\", \"target\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\", \n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\", \n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "loaded_datasets = {}\n",
    "for key, file_list in datasets.items():\n",
    "    loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "combined = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined = combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archaeologists think that a fire broke out in ...</td>\n",
       "      <td>ရှေးဟောင်းသုတေသီတွေက Knossos မှာ မီးလောင်တာ BC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are political meetings in every neighbor...</td>\n",
       "      <td>ရပ်ကွက်တိုင်းမှာ နိုင်ငံရေး အစည်းအဝေးတွေရှိတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the lawyer said that in article 712 (1) gao wa...</td>\n",
       "      <td>ရှေ့နေက ပုဒ်မ ၇၁၂ (၁) မှာ Gao ကို ငွေကြေးဆိုင်...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>things can get confusing when talking about do...</td>\n",
       "      <td>Dordogne အကြောင်းပြောသောအခါ၊ ဝေးကွာသောနေရာများ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>making financial management a top priority acr...</td>\n",
       "      <td>ဘဏ္ဍာရေး စီမံခန့်ခွဲမှုကို ပြည်ထောင်စု အစိုးရတ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  archaeologists think that a fire broke out in ...   \n",
       "1  there are political meetings in every neighbor...   \n",
       "2  the lawyer said that in article 712 (1) gao wa...   \n",
       "3  things can get confusing when talking about do...   \n",
       "4  making financial management a top priority acr...   \n",
       "\n",
       "                                              target  \n",
       "0  ရှေးဟောင်းသုတေသီတွေက Knossos မှာ မီးလောင်တာ BC...  \n",
       "1    ရပ်ကွက်တိုင်းမှာ နိုင်ငံရေး အစည်းအဝေးတွေရှိတယ်။  \n",
       "2  ရှေ့နေက ပုဒ်မ ၇၁၂ (၁) မှာ Gao ကို ငွေကြေးဆိုင်...  \n",
       "3  Dordogne အကြောင်းပြောသောအခါ၊ ဝေးကွာသောနေရာများ...  \n",
       "4  ဘဏ္ဍာရေး စီမံခန့်ခွဲမှုကို ပြည်ထောင်စု အစိုးရတ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Combined dataset length: {len(combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined, \"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, tokenizer, spt_tokenizer, model_name):\n",
    "    \"\"\"\n",
    "    Tokenizes Burmese text using the selected SentencePiece model before applying Transformer tokenization.\n",
    "    \"\"\"\n",
    "    # Apply SentencePiece Tokenization for Burmese target text\n",
    "    spt_burmese = [\" \".join(spt_tokenizer.encode_as_pieces(text)) for text in examples[\"target\"]]\n",
    "    examples[\"target\"] = spt_burmese  # Overwrite with tokenized text\n",
    "\n",
    "    if \"t5\" in model_name.lower():\n",
    "        # mT5/T5 (Text-to-Text) - Tokenize source & target separately\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Tokenize target`\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],  \n",
    "            padding=\"max_length\",  \n",
    "            truncation=True,  \n",
    "            max_length=512,\n",
    "            return_special_tokens_mask=True  # Helps handle special tokens\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        model_inputs[\"decoder_input_ids\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    # BERT-based models (Masked/Causal LM)\n",
    "    inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        examples[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Assign labels for causal LM (BERT-like models)\n",
    "    inputs[\"labels\"] = deepcopy(inputs[\"input_ids\"])\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize for each model and spt\n",
    "for model_name, tokenizer in train_tokenizers.items():\n",
    "    for spt_name, spt_tokenizer in spt_models.items():\n",
    "        dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        dataset = convert_to_hf(dataset)\n",
    "\n",
    "        # apply tokenize\n",
    "        dataset = dataset.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            desc=f\"Tokenizing dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save_model_variants_hf(dataset, f\"{model_name.lower()}_{spt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-Tuning Transformer Models for Burmese\n",
    "This notebook fine-tunes three transformer models:\n",
    "- mBERT (best perplexity, but weak BLEU/ROUGE)\n",
    "- mT5 (best for generation, but requires more data)\n",
    "- XLM-R (good BLEU/ROUGE, but poor perplexity)\n",
    "\n",
    "Apply:\n",
    "- Sentence-Piece Tokenization for Burmese segmentation\n",
    "- LoRA for efficient fine-tuning\n",
    "- Prefix-Tuning for lightweight adaptations\n",
    "- Mixed Precision Training for speed improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "train_models = {\n",
    "    \"mBERT\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"mBERT\"], num_labels=1).to(device),\n",
    "    \"mT5\": AutoModelForSeq2SeqLM.from_pretrained(train_model_names[\"mT5\"]).to(device),\n",
    "    \"XLM-R\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"XLM-R\"], num_labels=1).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized dataset\n",
    "tokenized_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_hf(f\"{model_name.lower()}_{spt_name}\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, spt_name, batch_size):\n",
    "    \"\"\"\n",
    "    Fine-tunes the model with LoRA on the specified SentencePiece tokenization (SPT).\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning {model_name} using SPT-{spt_name.upper()}...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = train_tokenizers[model_name]\n",
    "    model = train_models[model_name]\n",
    "\n",
    "    # Move model to GPU before applying LoRA\n",
    "    model.to(device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    model = apply_lora(model, model_name, False)\n",
    "\n",
    "    # Tokenize dataset & split into training and validation sets\n",
    "    tokenized_dataset = tokenized_datasets[model_name][spt_name]\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    train_data = split_dataset[\"train\"]\n",
    "    val_data = split_dataset[\"test\"]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    trained_model_name = f\"{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/{trained_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=3e-5,\n",
    "        logging_dir= f\"./logs/{trained_model_name}\",\n",
    "        **train_agrs\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{trained_model_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model `{model_name}` fine-tuned and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning mBERT using SPT-BPE...\n",
      "LoRA applied to mBERT (Target Modules: ['query', 'value'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85892' max='406895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 85892/406895 8:08:21 < 30:25:11, 2.93 it/s, Epoch 1.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mBERT\", \"bpe\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mBERT\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning mT5 using SPT-BPE...\n",
      "LoRA applied to mT5 (Target Modules: ['q', 'v'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='813790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    53/813790 00:15 < 68:48:19, 3.29 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mT5\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mT5\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"XLM-R\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_fine_tuned_model(model_name, spt_name):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, device)\n",
    "\n",
    "    # Load dataset \n",
    "    tokenized_dataset = load_model_variants_hf(f\"{model_name}_{spt_name}\")\n",
    "\n",
    "    # remove comment for debug\n",
    "    #tokenized_dataset = tokenized_dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    if \"t5\" in model_name.lower():\n",
    "        tokenized_dataset = generate_mt5_predictions(model, tokenizer, device)\n",
    "    else:\n",
    "        tokenized_dataset = generate_masked_predictions_hf(tokenized_dataset, model, tokenizer, device)\n",
    "\n",
    "    # remove columns\n",
    "    tokenized_dataset = tokenized_dataset.select_columns([\"source\", \"target\", \"generated\"])\n",
    "\n",
    "    # Display results\n",
    "    display(tokenized_dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_df(tokenized_dataset, f\"model-variants/{model_name}_{spt_name}_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf(metrics_dataset)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {model_name} with {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {metrics_dataset['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics_dataset['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics_dataset['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics_dataset['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics_dataset['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics_dataset['bert_score'].mean()}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{model_name}_{spt_name}_trained_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with BPE\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with Unigram\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with BPE\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with Unigram\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with BPE\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with Unigram\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity for fine-tuned model using HF Dataset\n",
    "def compute_perplexity_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset as Hugging Face Dataset\n",
    "    perplexity = load_model_variants_hf(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute perplexity using HF Dataset\n",
    "    if \"t5\" in model_name.lower():\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_mt5_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "    else:\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_masked_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "\n",
    "    # Display Perplexity Score\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_trained_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "\n",
    "        evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "        # load metrics and set\n",
    "        metrics = load_tmp_df(f\"{model_name}_{spt_name}_trained_metrics\")\n",
    "        evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "        evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "        evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "        evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "        evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "        evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "        # load perplexity and set\n",
    "        perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "        evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "        save_model_variants_df(evaluation_results, f\"{model_name}_{spt_name}_trained_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trained_benchmarking_datasets = {}\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        df = load_model_variants_df(f\"{model_name}_{spt_name}_trained_evaluation_results\")\n",
    "        trained_benchmarking_datasets[f\"{model_name} {spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "trained_benchmarking_mean_scores = convert_to_mean_scores_df(trained_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(trained_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(trained_benchmarking_mean_scores, \"trained_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enhance Burmese Contextual Representations\n",
    "- Use LASER, mUSE, and FastText for cross-lingual and morphology-aware training.\n",
    "- Fine-tune mBERT, XLM-R on Burmese dataset after adding contextual embedded.\n",
    "- Train models again using combined embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model names\n",
    "embedding_model_names = [\"mBERT\", \"XLM-R\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tune mBERT & XLM-R with Combined embedded\n",
    "def fine_tune_with_multilingual_embedded(model_name, spt_name, batch_size):\n",
    "    print(f\"Fine-tuning {model_name}-{spt_name.upper()} with multilingual embedded...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    model = apply_lora(model, model_name, True)\n",
    "\n",
    "    # Tokenize dataset & split into training and validation sets\n",
    "    tokenized_dataset = load_model_variants_hf(f\"{model_name.lower()}_{spt_name}\")\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "    train_data = split_dataset[\"train\"]\n",
    "    val_data = split_dataset[\"test\"]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    embedded_model_name = f\"Embedded_{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/{embedded_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        logging_dir=f\"./logs/{embedded_model_name}\"\n",
    "        **train_agrs\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # Train the Model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{embedded_model_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Fine-tuned {model_name}_{spt_name.upper()} with multilingual embedded saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_with_multilingual_embedded(\"mBERT\", \"bpe\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_with_multilingual_embedded(\"mBERT\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_with_multilingual_embedded(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_with_multilingual_embedded(\"XLM-R\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for embedded fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_embedded_fine_tuned_model(model_name, spt_name, device)\n",
    "\n",
    "    # Load dataset \n",
    "    tokenized_dataset = load_model_variants_hf(f\"{model_name}_{spt_name}\")\n",
    "\n",
    "    # remove comment for debug\n",
    "    #tokenized_dataset = tokenized_dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    tokenized_dataset = generate_masked_predictions_hf(tokenized_dataset, model, tokenizer, device)\n",
    "\n",
    "    # remove columns\n",
    "    tokenized_dataset = tokenized_dataset.select_columns([\"source\", \"target\", \"generated\"])\n",
    "\n",
    "    # Display results\n",
    "    display(tokenized_dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_df(tokenized_dataset, f\"model-variants/{model_name}_{spt_name}_embedded_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE\n",
    "generate_predictions_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram\n",
    "generate_predictions_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE\n",
    "generate_predictions_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram\n",
    "generate_predictions_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for embedded Fine-Tuned Model using HF Dataset\n",
    "def compute_metric_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_df(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf(metrics_dataset)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {model_name} with {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {metrics_dataset['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics_dataset['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics_dataset['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics_dataset['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics_dataset['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics_dataset['bert_score'].mean()}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{model_name}_{spt_name}_embedded_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with BPE\n",
    "compute_metric_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with Unigram\n",
    "compute_metric_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with BPE\n",
    "compute_metric_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with Unigram\n",
    "compute_metric_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity for embedded fine-tuned model using HF Dataset\n",
    "def compute_perplexity_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset as Hugging Face Dataset\n",
    "    perplexity = load_model_variants_hf(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_embedded_fine_tuned_model(model_name, spt_name, device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute perplexity using HF Dataset\n",
    "    perplexity = perplexity.map(lambda example: {\n",
    "        \"perplexity\": compute_multilingual_masked_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "    })\n",
    "\n",
    "    # Display Perplexity Score\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_embedded_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in embedding_model_names:\n",
    "    for spt_name in spt_models.keys():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "\n",
    "        evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "        # load metrics and set\n",
    "        metrics = load_tmp_df(f\"{model_name}_{spt_name}_metrics\")\n",
    "        evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "        evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "        evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "        evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "        evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "        evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "        # load perplexity and set\n",
    "        perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "        evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "        save_model_variants_df(evaluation_results, f\"{model_name}_{spt_name}_embedded_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "embedded_benchmarking_datasets = {}\n",
    "for model_name in embedding_model_names:\n",
    "    for spt_name in spt_models.keys():\n",
    "        df = load_model_variants_df(f\"{model_name}_{spt_name}_trained_evaluation_results\")\n",
    "        embedded_benchmarking_datasets[f\"{model_name} {spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "embedded_benchmarking_mean_scores = convert_to_mean_scores_df(embedded_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(embedded_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(embedded_benchmarking_mean_scores, \"embedded_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize Model Efficiency with Lightweight Transformers\n",
    "- Optimizes mBERT, XLM-R, mT5-Small (BPE & Unigram).\n",
    "- Trains TinyBERT, DistilBERT with Knowledge Distillation.\n",
    "- Evaluates BLEU, ROUGE, chrF-S and Perplexity after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Student Models (TinyBERT & DistilBERT)\n",
    "distill_model_names = {\n",
    "    \"TinyBERT\": \"huawei-noah/TinyBERT_General_6L_768D\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Student Models with Knowledge Distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, alpha=0.5, temperature=2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.teacher_model.eval()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Ensure labels exist in inputs\n",
    "        if \"labels\" not in inputs:\n",
    "            raise ValueError(\"Missing 'labels' in input dictionary.\")\n",
    "\n",
    "        labels = inputs[\"labels\"].view(-1)\n",
    "\n",
    "        # Compute teacher logits without gradient computation\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Compute CrossEntropy Loss (ignoring padding tokens)\n",
    "        loss_ce = nn.CrossEntropyLoss(ignore_index=-100)(\n",
    "            student_logits.view(-1, student_logits.size(-1)), labels\n",
    "        )\n",
    "\n",
    "        # Compute KL Divergence Loss for Knowledge Distillation\n",
    "        loss_kl = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            torch.nn.functional.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "            torch.nn.functional.softmax(teacher_logits / self.temperature, dim=-1),\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # Final loss: Combination of CE loss and KL loss\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "\n",
    "        return (loss, student_outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train fine tuned model with knowledge distillation\n",
    "def train_distilled_model(teacher_model_name, teacher_spt_name, student_model_name, batch_size):\n",
    "    print(f\"Training {student_model_name} using {teacher_model_name}_{teacher_spt_name.upper()} as a teacher...\")\n",
    "\n",
    "    # Select Correct Model Type and teacher model\n",
    "    if \"t5\" in teacher_model_name.lower():\n",
    "        teacher_model, tokenizer = get_fine_tuned_model(teacher_model_name, teacher_spt_name, device)\n",
    "        student_model = AutoModelForSeq2SeqLM.from_pretrained(distill_model_names[student_model_name]).to(device)\n",
    "    else:\n",
    "        teacher_model, tokenizer = get_embedded_fine_tuned_model(teacher_model_name, teacher_spt_name, device)\n",
    "        student_model = AutoModelForMaskedLM.from_pretrained(distill_model_names[student_model_name]).to(device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    student_model = apply_lora(student_model, teacher_model_name, True)\n",
    "\n",
    "    # Tokenize dataset & split into training and validation sets\n",
    "    tokenized_dataset = tokenized_datasets[teacher_model_name][teacher_spt_name]\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    train_data = split_dataset[\"train\"]\n",
    "    val_data = split_dataset[\"test\"]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    distilled_model_name = f\"{teacher_model_name}_{teacher_spt_name.upper()}_{student_model_name}\"\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/Distilled_{distilled_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        logging_dir=f\"./logs/{distilled_model_name}\"\n",
    "        **train_agrs\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = DistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        model=student_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{distilled_model_name}\"\n",
    "    student_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Training {student_model_name} using {teacher_model_name}_{teacher_spt_name.upper()} as a teacher is finished and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mBERT\", \"bpe\", \"TinyBERT\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mBERT\", \"unigram\", \"TinyBERT\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mBERT\", \"bpe\", \"DistilBERT\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mBERT\", \"unigram\", \"DistilBERT\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"XLM-R\", \"bpe\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"XLM-R\", \"unigram\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"XLM-R\", \"bpe\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"XLM-R\", \"unigram\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mT5\", \"bpe\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mT5\", \"unigram\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mT5\", \"bpe\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mT5\", \"unigram\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_distilled_fine_tuned_model(model_name, spt_name, distill_model_name):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_distilled_fine_tuned_model(model_name, spt_name, distill_model_name, device)\n",
    "\n",
    "    # Load dataset \n",
    "    tokenized_dataset = load_model_variants_hf(f\"{model_name}_{spt_name}\")\n",
    "\n",
    "    # remove comment for debug\n",
    "    #tokenized_dataset = tokenized_dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    if \"t5\" in model_name.lower():\n",
    "        tokenized_dataset = generate_mt5_predictions(model, tokenizer, device)\n",
    "    else:\n",
    "        tokenized_dataset = generate_masked_predictions_hf(tokenized_dataset, model, tokenizer, device)\n",
    "\n",
    "    # remove columns\n",
    "    tokenized_dataset = tokenized_dataset.select_columns([\"source\", \"target\", \"generated\"])\n",
    "\n",
    "    # Display results\n",
    "    display(tokenized_dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_df(tokenized_dataset, f\"model-variants/{model_name}_{spt_name}_{distill_model_name}_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"XLM-R\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram and TinyBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram and DistillBERT\n",
    "generate_predictions_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Distilled Fine-Tuned Model using HF Dataset\n",
    "def compute_metric_distilled_fine_tuned_model(model_name, spt_name, distill_model_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()} and {distill_model_name}...\")\n",
    "    metrics_dataset = compute_metrics_hf(metrics_dataset)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {model_name} with {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {metrics_dataset['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics_dataset['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics_dataset['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics_dataset['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics_dataset['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics_dataset['bert_score'].mean()}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{model_name}_{spt_name}_{distill_model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"XLM-R\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"XLM-R\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity for distilled fine-tuned model using HF Dataset\n",
    "def compute_perplexity_distilled_fine_tuned_model(model_name, spt_name, distill_model_name):\n",
    "    # Load dataset as Hugging Face Dataset\n",
    "    perplexity = load_model_variants_hf(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_distilled_fine_tuned_model(model_name, spt_name, distill_model_name, device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute perplexity using HF Dataset\n",
    "    if \"t5\" in model_name.lower():\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_mt5_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "    else:\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_masked_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "\n",
    "    # Display Perplexity Score\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_{distill_model_name}_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mBERT\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mBERT\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"XLM-R\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"XLM-R\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"XLM-R\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mT5\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_fine_tuned_model(\"mT5\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        for distill_model_name in distill_model_names.keys():\n",
    "            print(f\"Processing {model_name} with {spt_name} and {distill_model_name}...\")\n",
    "\n",
    "            evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "            # load metrics and set\n",
    "            metrics = load_tmp_df(f\"{model_name}_{spt_name}_metrics\")\n",
    "            evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "            evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "            evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "            evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "            evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "            evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "            # load perplexity and set\n",
    "            perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "            evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "            save_model_variants_df(evaluation_results, f\"{model_name}_{spt_name}_{distill_model_name}_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "distilled_benchmarking_datasets = {}\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        for distill_model_name in distill_model_names.keys():\n",
    "            df = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_evaluation_results\")\n",
    "            distilled_benchmarking_datasets[f\"{model_name} {spt_name.upper()} {distill_model_name}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "distilled_benchmarking_mean_scores = convert_to_mean_scores_df(distilled_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(distilled_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(distilled_benchmarking_mean_scores, \"distilled_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
