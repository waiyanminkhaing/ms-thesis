{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded from S3 successfully!\n"
     ]
    }
   ],
   "source": [
    "# S3 Setup\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"ms-thesis-sagemaker\"  # Replace with your S3 bucket\n",
    "s3_file_path = \"mbert_bpe_hf_dataset.zip\"  # Replace with the file name in S3\n",
    "local_zip_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/mbert_bpe_hf_dataset.zip\"  # Where to save in SageMaker\n",
    "\n",
    "# Download the ZIP file from S3\n",
    "s3.download_file(bucket_name, s3_file_path, local_zip_path)\n",
    "print(\"ZIP file downloaded from S3 successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "extract_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/\"  # Where to extract\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"ZIP file extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file deleted to free space.\n"
     ]
    }
   ],
   "source": [
    "os.remove(local_zip_path)\n",
    "print(\"ZIP file deleted to free space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from utils.dataframe import (\n",
    "    load_gen_df, save_tmp_df, load_tmp_df,\n",
    "    save_model_variants_df, load_model_variants_df,\n",
    "    save_model_variants_arrow, load_model_variants_arrow\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.custom_class import MaskedTextDataset, EvaluationDataset, TextDataset\n",
    "from utils.common import (\n",
    "    generate_masked_predictions_batch,\n",
    "    generate_mt5_predictions_batch,\n",
    "    compute_metrics_batch,\n",
    "    compute_multilingual_masked_perplexity_batch,\n",
    "    compute_multilingual_mt5_perplexity_batch,\n",
    "    convert_to_mean_scores_df\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import prune\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM,\n",
    "    Trainer, TrainingArguments, LongformerConfig, LongformerModel,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# from optimum.intel.openvino import OVModelForMaskedLM, OVModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "train_model_names = {\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"mT5\": \"google/mt5-small\",\n",
    "    \"XLM-R\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizers\n",
    "train_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(train_model_names[\"mBERT\"]),\n",
    "    \"mT5\": AutoTokenizer.from_pretrained(train_model_names[\"mT5\"], use_fast=False, legacy=True),\n",
    "    \"XLM-R\": AutoTokenizer.from_pretrained(train_model_names[\"XLM-R\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agrs = {\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_dir\": \"./logs\",\n",
    "    \"logging_steps\": 1000,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"disable_tqdm\": False,\n",
    "    \"label_names\": [\"labels\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, model_name, is_student):\n",
    "    \"\"\"\n",
    "    Applies LoRA for efficient fine-tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select correct LoRA target layers\n",
    "    if \"t5\" in model_name.lower():\n",
    "        target_modules = [\"q\", \"v\"]  # LoRA for T5/mT5\n",
    "    else:\n",
    "        target_modules = [\"query\", \"value\"]  # LoRA for BERT\n",
    "\n",
    "    # Define LoRA Configuration\n",
    "    if is_student:\n",
    "        lora_config = LoraConfig(\n",
    "            r=4,                    # Rank of LoRA matrices\n",
    "            lora_alpha=8,           # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.05,      # Prevents overfitting\n",
    "            bias=\"none\"\n",
    "        )\n",
    "    else:\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,                    # Rank of LoRA matrices\n",
    "            lora_alpha=16,          # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.1,       # Prevents overfitting\n",
    "            bias=\"none\"\n",
    "        )\n",
    "\n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Move model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"LoRA applied to {model_name} (Target Modules: {target_modules})\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Preprocessing\n",
    "Datasets used for training:\n",
    "- myXNLI & ALT Corpus (normalized)\n",
    "- Back-translated datasets (NLLB, Seamless M4T)\n",
    "- Pseudo-parallel datasets (MiniLM, LaBSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english\": \"source\",\n",
    "        \"burmese\": \"target\",\n",
    "        \"english_back_translated\": \"source\",\n",
    "        \"burmese_translated\": \"target\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"source\", \"target\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\", \n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\", \n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "loaded_datasets = {}\n",
    "for key, file_list in datasets.items():\n",
    "    loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "combined = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined = combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archaeologists think that a fire broke out in ...</td>\n",
       "      <td>ရှေးဟောင်းသုတေသီတွေက Knossos မှာ မီးလောင်တာ BC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are political meetings in every neighbor...</td>\n",
       "      <td>ရပ်ကွက်တိုင်းမှာ နိုင်ငံရေး အစည်းအဝေးတွေရှိတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the lawyer said that in article 712 (1) gao wa...</td>\n",
       "      <td>ရှေ့နေက ပုဒ်မ ၇၁၂ (၁) မှာ Gao ကို ငွေကြေးဆိုင်...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>things can get confusing when talking about do...</td>\n",
       "      <td>Dordogne အကြောင်းပြောသောအခါ၊ ဝေးကွာသောနေရာများ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>making financial management a top priority acr...</td>\n",
       "      <td>ဘဏ္ဍာရေး စီမံခန့်ခွဲမှုကို ပြည်ထောင်စု အစိုးရတ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  archaeologists think that a fire broke out in ...   \n",
       "1  there are political meetings in every neighbor...   \n",
       "2  the lawyer said that in article 712 (1) gao wa...   \n",
       "3  things can get confusing when talking about do...   \n",
       "4  making financial management a top priority acr...   \n",
       "\n",
       "                                              target  \n",
       "0  ရှေးဟောင်းသုတေသီတွေက Knossos မှာ မီးလောင်တာ BC...  \n",
       "1    ရပ်ကွက်တိုင်းမှာ နိုင်ငံရေး အစည်းအဝေးတွေရှိတယ်။  \n",
       "2  ရှေ့နေက ပုဒ်မ ၇၁၂ (၁) မှာ Gao ကို ငွေကြေးဆိုင်...  \n",
       "3  Dordogne အကြောင်းပြောသောအခါ၊ ဝေးကွာသောနေရာများ...  \n",
       "4  ဘဏ္ဍာရေး စီမံခန့်ခွဲမှုကို ပြည်ထောင်စု အစိုးရတ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Combined dataset length: {len(combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined, \"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, tokenizer, spt_tokenizer, model_name):\n",
    "    \"\"\"\n",
    "    Tokenizes Burmese text using the selected SentencePiece model before applying Transformer tokenization.\n",
    "    \"\"\"\n",
    "    # Apply SentencePiece Tokenization for Burmese target text\n",
    "    spt_burmese = [\" \".join(spt_tokenizer.encode_as_pieces(text)) for text in examples[\"target\"]]\n",
    "    examples[\"target\"] = spt_burmese  # Overwrite with tokenized text\n",
    "\n",
    "    if \"t5\" in model_name.lower():\n",
    "        # mT5/T5 (Text-to-Text) - Tokenize source & target separately\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Tokenize target`\n",
    "        labels = tokenizer(\n",
    "            examples[\"target\"],  \n",
    "            padding=\"max_length\",  \n",
    "            truncation=True,  \n",
    "            max_length=512,\n",
    "            return_special_tokens_mask=True  # Helps handle special tokens\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        model_inputs[\"decoder_input_ids\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    # BERT-based models (Masked/Causal LM)\n",
    "    inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        examples[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Assign labels for causal LM (BERT-like models)\n",
    "    inputs[\"labels\"] = deepcopy(inputs[\"input_ids\"])\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize for each model and spt\n",
    "for model_name, tokenizer in train_tokenizers.items():\n",
    "    for spt_name, spt_tokenizer in spt_models.items():\n",
    "        dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "        # apply tokenize\n",
    "        dataset = dataset.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            desc=f\"Tokenizing dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save_model_variants_arrow(dataset, f\"{model_name.lower()}_{spt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-Tuning Transformer Models for Burmese\n",
    "This notebook fine-tunes three transformer models:\n",
    "- mBERT (best perplexity, but weak BLEU/ROUGE)\n",
    "- mT5 (best for generation, but requires more data)\n",
    "- XLM-R (good BLEU/ROUGE, but poor perplexity)\n",
    "\n",
    "Apply:\n",
    "- Sentence-Piece Tokenization for Burmese segmentation\n",
    "- LoRA for efficient fine-tuning\n",
    "- Prefix-Tuning for lightweight adaptations\n",
    "- Mixed Precision Training for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get fine tuned model\n",
    "def get_fine_tuned_model(model_name, spt_name):\n",
    "    model_path = f\"model-variants/models/{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if \"t5\" in model_name.lower():\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained().to(device)\n",
    "    else:\n",
    "        model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "train_models = {\n",
    "    \"mBERT\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"mBERT\"], num_labels=1).to(device),\n",
    "    \"mT5\": AutoModelForSeq2SeqLM.from_pretrained(train_model_names[\"mT5\"]).to(device),\n",
    "    \"XLM-R\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"XLM-R\"], num_labels=1).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized dataset\n",
    "tokenized_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_arrow(f\"{model_name.lower()}_{spt_name}\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, spt_name, batch_size):\n",
    "    \"\"\"\n",
    "    Fine-tunes the model with LoRA on the specified SentencePiece tokenization (SPT).\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning {model_name} using SPT-{spt_name.upper()}...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = train_tokenizers[model_name]\n",
    "    model = train_models[model_name]\n",
    "\n",
    "    # Move model to GPU before applying LoRA\n",
    "    model.to(device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    model = apply_lora(model, model_name, False)\n",
    "\n",
    "    # Tokenize dataset & split into training and validation sets\n",
    "    tokenized_dataset = tokenized_datasets[model_name][spt_name]\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    train_data = split_dataset[\"train\"]\n",
    "    val_data = split_dataset[\"test\"]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/{model_name}_{spt_name.upper()}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=3e-5,\n",
    "        **train_agrs\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{model_name}_{spt_name.upper()}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model `{model_name}` fine-tuned and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning mBERT using SPT-BPE...\n",
      "LoRA applied to mBERT (Target Modules: ['query', 'value'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85892' max='406895' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 85892/406895 8:08:21 < 30:25:11, 2.93 it/s, Epoch 1.06/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mBERT\", \"bpe\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mBERT\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning mT5 using SPT-BPE...\n",
      "LoRA applied to mT5 (Target Modules: ['q', 'v'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='813790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    53/813790 00:15 < 68:48:19, 3.29 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mT5\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mT5\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"XLM-R\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate predictions of fine tuned model\n",
    "def generate_predictions_fine_tuned_model(model_name, spt_name, batch_size):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name)\n",
    "\n",
    "    # load dataset\n",
    "    predictions = load_model_variants_df(\"combined\")\n",
    "\n",
    "    # Convert to DataLoader\n",
    "    predictions_texts = predictions[\"target\"].tolist()\n",
    "    predictions_dataset = MaskedTextDataset(predictions_texts, tokenizer)\n",
    "    predictions_dataloader = DataLoader(\n",
    "        predictions_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Run text generation\n",
    "    if \"t5\" in model_name.lower():\n",
    "        predictions[\"generated\"] = generate_mt5_predictions_batch(\n",
    "            predictions_dataloader, \n",
    "            model, \n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "    else:\n",
    "        predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "            predictions_dataloader, \n",
    "            model, \n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "\n",
    "    # display\n",
    "    display(predictions.head())\n",
    "\n",
    "    # save trained mbert predictions\n",
    "    save_model_variants_df(predictions, f\"{model_name}_{spt_name}_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute metric for fine tuned model\n",
    "def compute_metric_fine_tuned_model(model_name, spt_name, batch_size):\n",
    "    # load dataset\n",
    "    metrics = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()}...\"),\n",
    "    compute_metrics_batch(metrics, batch_size)\n",
    "\n",
    "    # display\n",
    "    print(f\"Metrics scores for mBERT with BPE:\")\n",
    "    print(f\"BLEU Score: {metrics['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics['bert_score'].mean()}\")\n",
    "\n",
    "    # save results\n",
    "    save_tmp_df(metrics, f\"{model_name}_{spt_name}_trained_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with BPE\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with Unigram\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with BPE\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with Unigram\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with BPE\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with Unigram\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute perplexity for fine tuned model\n",
    "def compute_perplexity_fine_tuned_model(model_name, spt_name, batch_size):\n",
    "    # load dataset\n",
    "    perplexity = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name)\n",
    "\n",
    "    # Prepare dataset and DataLoader\n",
    "    generated_texts = perplexity[\"generated\"].tolist()\n",
    "    text_dataset = TextDataset(generated_texts)\n",
    "    dataloader = DataLoader(\n",
    "        text_dataset, \n",
    "        batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # compute and store perplexity scores in DataFrame\n",
    "    if \"t5\" in model_name.lower():\n",
    "        perplexity[\"perplexity\"] = compute_multilingual_mt5_perplexity_batch(\n",
    "            dataloader,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "    else:\n",
    "        perplexity[\"perplexity\"] = compute_multilingual_masked_perplexity_batch(\n",
    "            dataloader,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "\n",
    "    # display perplexity\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # save perplexity\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_trained_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"bpe\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"unigram\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "\n",
    "        distilled_evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "        # load metrics and set\n",
    "        metrics = load_tmp_df(f\"{model_name}_{spt_name}_metrics\")\n",
    "        distilled_evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "        distilled_evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "        distilled_evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "        distilled_evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "        distilled_evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "        distilled_evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "        # load perplexity and set\n",
    "        perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "        distilled_evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "        save_model_variants_df(distilled_evaluation_results, f\"{model_name}_{spt_name}_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trained_benchmarking_datasets = {}\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        df = load_model_variants_df(f\"{model_name}_{spt_name}_evaluation_results\")\n",
    "        trained_benchmarking_datasets[f\"{model_name} {spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "trained_benchmarking_mean_scores = convert_to_mean_scores_df(trained_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(trained_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(trained_benchmarking_mean_scores, \"trained_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimize Model Efficiency with Lightweight Transformers\n",
    "- Optimizes mBERT, XLM-R, mT5-Small (BPE & Unigram).\n",
    "- Trains TinyBERT, DistilBERT with Knowledge Distillation.\n",
    "- Evaluates BLEU, ROUGE, chrF-S and Perplexity after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Student Models (TinyBERT & DistilBERT)\n",
    "distill_model_names = {\n",
    "    \"TinyBERT\": \"huawei-noah/TinyBERT_General_6L_768D\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get distilled model\n",
    "def get_distilled_model(model_name, spt_name, distill_model_name):\n",
    "    model_path = f\"model-variants/models/{model_name}_{spt_name.upper()}_{distill_model_name}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if \"t5\" in model_name.lower():\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained().to(device)\n",
    "    else:\n",
    "        model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Student Models with Knowledge Distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, alpha=0.5, temperature=2.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.teacher_model.eval()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Ensure labels exist in inputs\n",
    "        if \"labels\" not in inputs:\n",
    "            raise ValueError(\"Missing 'labels' in input dictionary.\")\n",
    "\n",
    "        labels = inputs[\"labels\"].view(-1)\n",
    "\n",
    "        # Compute teacher logits without gradient computation\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Compute CrossEntropy Loss (ignoring padding tokens)\n",
    "        loss_ce = nn.CrossEntropyLoss(ignore_index=-100)(\n",
    "            student_logits.view(-1, student_logits.size(-1)), labels\n",
    "        )\n",
    "\n",
    "        # Compute KL Divergence Loss for Knowledge Distillation\n",
    "        loss_kl = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            torch.nn.functional.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "            torch.nn.functional.softmax(teacher_logits / self.temperature, dim=-1),\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # Final loss: Combination of CE loss and KL loss\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "\n",
    "        return (loss, student_outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train fine tuned model with knowledge distillation\n",
    "def train_distilled_model(teacher_model_name, teacher_spt_name, student_model_name, batch_size):\n",
    "    print(f\"Training {student_model_name} using {teacher_model_name}_{teacher_spt_name.upper()} as a teacher...\")\n",
    "\n",
    "    # get teacher model and tokenizer\n",
    "    teacher_model, tokenizer = get_fine_tuned_model(teacher_model_name, teacher_spt_name)\n",
    "\n",
    "    # Select Correct Model Type\n",
    "    if \"t5\" in teacher_model_name.lower():\n",
    "        student_model = AutoModelForSeq2SeqLM.from_pretrained(distill_model_names[student_model_name]).to(device)\n",
    "    else:\n",
    "        student_model = AutoModelForMaskedLM.from_pretrained(distill_model_names[student_model_name]).to(device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    student_model = apply_lora(student_model, teacher_model_name, True)\n",
    "\n",
    "    # Tokenize dataset & split into training and validation sets\n",
    "    tokenized_dataset = tokenized_datasets[teacher_model_name][teacher_spt_name]\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    train_data = split_dataset[\"train\"]\n",
    "    val_data = split_dataset[\"test\"]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/{teacher_model_name}_{teacher_spt_name.upper()}_{student_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        **train_agrs\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = DistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        model=student_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{teacher_model_name}_{teacher_spt_name.upper()}_{student_model_name}\"\n",
    "    student_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Training {student_model_name} using {teacher_model_name}_{teacher_spt_name.upper()} as a teacher is finished and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mBERT\", \"bpe\", \"TinyBERT\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mBERT\", \"unigram\", \"TinyBERT\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mBERT\", \"bpe\", \"DistilBERT\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mBERT\", \"unigram\", \"DistilBERT\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"XLM-R\", \"bpe\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"XLM-R\", \"unigram\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"XLM-R\", \"bpe\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"XLM-R\", \"unigram\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mT5\", \"bpe\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mT5\", \"unigram\", \"TinyBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with BPE\n",
    "train_distilled_model(\"mT5\", \"bpe\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Knowledge Distillation TinyBERT with Unigram\n",
    "train_distilled_model(\"mT5\", \"unigram\", \"DistilBERT\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate predictions of distilled model\n",
    "def generate_predictions_distilled_model(model_name, spt_name, distilled_model_name, batch_size):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_distilled_model(model_name, spt_name, distilled_model_name)\n",
    "\n",
    "    # load dataset\n",
    "    predictions = load_model_variants_df(\"combined\")\n",
    "\n",
    "    # Convert to DataLoader\n",
    "    predictions_texts = predictions[\"target\"].tolist()\n",
    "    predictions_dataset = MaskedTextDataset(predictions_texts, tokenizer)\n",
    "    predictions_dataloader = DataLoader(\n",
    "        predictions_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Run text generation\n",
    "    if \"t5\" in model_name.lower():\n",
    "        predictions[\"generated\"] = generate_mt5_predictions_batch(\n",
    "            predictions_dataloader, \n",
    "            model, \n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "    else:\n",
    "        predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "            predictions_dataloader, \n",
    "            model, \n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "\n",
    "    # display\n",
    "    display(predictions.head())\n",
    "\n",
    "    # save trained mbert predictions\n",
    "    save_model_variants_df(predictions, f\"{model_name}_{spt_name}_{distilled_model_name}_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE and TinyBERT\n",
    "generate_predictions_distilled_model(\"mBERT\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram and TinyBERT\n",
    "generate_predictions_distilled_model(\"mBERT\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE and DistillBERT\n",
    "generate_predictions_distilled_model(\"mBERT\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram and DistillBERT\n",
    "generate_predictions_distilled_model(\"mBERT\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and TinyBERT\n",
    "generate_predictions_distilled_model(\"XLM-R\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram and TinyBERT\n",
    "generate_predictions_distilled_model(\"XLM-R\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and DistillBERT\n",
    "generate_predictions_distilled_model(\"XLM-R\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE and DistillBERT\n",
    "generate_predictions_distilled_model(\"XLM-R\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE and TinyBERT\n",
    "generate_predictions_distilled_model(\"mT5\", \"bpe\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram and TinyBERT\n",
    "generate_predictions_distilled_model(\"mT5\", \"unigram\", \"TinyBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE and DistillBERT\n",
    "generate_predictions_distilled_model(\"mT5\", \"bpe\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram and DistillBERT\n",
    "generate_predictions_distilled_model(\"mT5\", \"unigram\", \"DistillBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute metric for distilled model\n",
    "def compute_metric_distilled_model(model_name, spt_name, distill_model_name, batch_size):\n",
    "    # load dataset\n",
    "    metrics = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "    # compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()} and {distill_model_name}...\"),\n",
    "    compute_metrics_batch(metrics, batch_size)\n",
    "\n",
    "    # display\n",
    "    print(f\"Metrics scores for mBERT with BPE:\")\n",
    "    print(f\"BLEU Score: {metrics['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics['bert_score'].mean()}\")\n",
    "\n",
    "    # save results\n",
    "    save_tmp_df(metrics, f\"{model_name}_{spt_name}_{distill_model_name}_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"mBERT\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"mBERT\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"mBERT\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"mBERT\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"XLM-R\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"XLM-R\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"XLM-R\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"XLM-R\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"mT5\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"mT5\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with BPE\n",
    "compute_metric_distilled_model(\"mT5\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for distilled model with Unigram\n",
    "compute_metric_distilled_model(\"mT5\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute perplexity for distilled model\n",
    "def compute_perplexity_distilled_model(model_name, spt_name, distill_model_name, batch_size):\n",
    "    # load dataset\n",
    "    perplexity = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_distilled_model(model_name, spt_name, distill_model_name)\n",
    "\n",
    "    # Prepare dataset and DataLoader\n",
    "    generated_texts = perplexity[\"generated\"].tolist()\n",
    "    text_dataset = TextDataset(generated_texts)\n",
    "    dataloader = DataLoader(\n",
    "        text_dataset, \n",
    "        batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # compute and store perplexity scores in DataFrame\n",
    "    if \"t5\" in model_name.lower():\n",
    "        perplexity[\"perplexity\"] = compute_multilingual_mt5_perplexity_batch(\n",
    "            dataloader,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "    else:\n",
    "        perplexity[\"perplexity\"] = compute_multilingual_masked_perplexity_batch(\n",
    "            dataloader,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device\n",
    "        )\n",
    "\n",
    "    # display perplexity\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # save perplexity\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_{distill_model_name}_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"mBERT\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"mBERT\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"mBERT\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"mBERT\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"XLM-R\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"XLM-R\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"XLM-R\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"XLM-R\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"mT5\", \"bpe\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"mT5\", \"unigram\", \"TinyBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with bpe\n",
    "compute_perplexity_distilled_model(\"mT5\", \"bpe\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with unigram\n",
    "compute_perplexity_distilled_model(\"mT5\", \"unigram\", \"DistillBERT\", 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        for distill_model_name in distill_model_names.keys():\n",
    "            print(f\"Processing {model_name} with {spt_name} and {distill_model_name}...\")\n",
    "\n",
    "            distilled_evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_predictions\")\n",
    "\n",
    "            # load metrics and set\n",
    "            metrics = load_tmp_df(f\"{model_name}_{spt_name}_metrics\")\n",
    "            distilled_evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "            distilled_evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "            distilled_evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "            distilled_evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "            distilled_evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "            distilled_evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "            # load perplexity and set\n",
    "            perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "            distilled_evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "            save_model_variants_df(distilled_evaluation_results, f\"{model_name}_{spt_name}_{distill_model_name}_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "distilled_benchmarking_datasets = {}\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        for distill_model_name in distill_model_names.keys():\n",
    "            df = load_model_variants_df(f\"{model_name}_{spt_name}_{distill_model_name}_evaluation_results\")\n",
    "            distilled_benchmarking_datasets[f\"{model_name} {spt_name.upper()} {distill_model_name}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "distilled_benchmarking_mean_scores = convert_to_mean_scores_df(distilled_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(distilled_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(distilled_benchmarking_mean_scores, \"distilled_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
