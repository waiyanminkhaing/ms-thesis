{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model-variants df\n",
    "def save_model_variants_df(df, df_name):\n",
    "    df.to_csv(f\"model-variants/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_variants_df_arrow(df, df_name): \n",
    "    df.save_to_disk(f\"model-variants/{df_name}_hf_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model-variants df\n",
    "def load_model_variants_df(df_name):\n",
    "    return pd.read_csv(f\"model-variants/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model-variants df with arrow\n",
    "def load_model_variants_df_arrow(df_name):\n",
    "    return load_dataset(f\"model-variants/{df_name}_hf_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformer Models for Burmese\n",
    "This notebook fine-tunes three transformer models:\n",
    "- mBERT (Multilingual BERT)\n",
    "- mT5 (Multilingual T5)\n",
    "- XLM-RoBERTa\n",
    "\n",
    "Apply:\n",
    "- Sentence-Piece Tokenization for Burmese segmentation\n",
    "- LoRA for efficient fine-tuning\n",
    "- Prefix-Tuning for lightweight adaptations\n",
    "- Mixed Precision Training for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "train_model_names = {\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"mT5\": \"google/mt5-small\",\n",
    "    \"XLM-R\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizers\n",
    "train_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(train_model_names[\"mBERT\"]),\n",
    "    \"mT5\": AutoTokenizer.from_pretrained(train_model_names[\"mT5\"], use_fast=False, legacy=True),\n",
    "    \"XLM-R\": AutoTokenizer.from_pretrained(train_model_names[\"XLM-R\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "train_models = {\n",
    "    \"mBERT\": AutoModelForSequenceClassification.from_pretrained(train_model_names[\"mBERT\"], num_labels=1).to(device),\n",
    "    \"mT5\": AutoModelForSeq2SeqLM.from_pretrained(train_model_names[\"mT5\"]).to(device),\n",
    "    \"XLM-R\": AutoModelForSequenceClassification.from_pretrained(train_model_names[\"XLM-R\"], num_labels=1).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Datasets used for training:\n",
    "- myXNLI & ALT Corpus (normalized)\n",
    "- Back-translated datasets (NLLB, Seamless M4T)\n",
    "- Pseudo-parallel datasets (MiniLM, LaBSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english\": \"source\",\n",
    "        \"burmese\": \"target\",\n",
    "        \"english_back_translated\": \"source\",\n",
    "        \"burmese_translated\": \"target\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"source\", \"target\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\", \n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\", \n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "loaded_datasets = {}\n",
    "for key, file_list in datasets.items():\n",
    "    loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "combined = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined = combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>economic literature is still being developed, ...</td>\n",
       "      <td>အချိန်မမီ သေဆုံးမှု အန္တရာယ် လျှော့ချမှုကို တန...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they also have other group excursions not listed.</td>\n",
       "      <td>၎င်းတို့တွင် စာရင်းမသွင်းထားသော အခြားအဖွဲ့လိုက...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think it's yours, but it's actually mine.</td>\n",
       "      <td>ဒါက သင့်ရဲ့လို့ ထင်ပေမဲ့ တကယ်က ကျွန်မပါ။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all counties benefit from the state income tax...</td>\n",
       "      <td>ခရိုင်အားလုံးဟာ အခွန်ထမ်းတွေရဲ့ ပြည်နယ် ဝင်ငွေ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the deep breathing barik did helped him to cal...</td>\n",
       "      <td>Barik အသက်ပြင်းပြင်းရှူခြင်းက သူ့ကို စိတ်တည်ငြ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  economic literature is still being developed, ...   \n",
       "1  they also have other group excursions not listed.   \n",
       "2        i think it's yours, but it's actually mine.   \n",
       "3  all counties benefit from the state income tax...   \n",
       "4  the deep breathing barik did helped him to cal...   \n",
       "\n",
       "                                              target  \n",
       "0  အချိန်မမီ သေဆုံးမှု အန္တရာယ် လျှော့ချမှုကို တန...  \n",
       "1  ၎င်းတို့တွင် စာရင်းမသွင်းထားသော အခြားအဖွဲ့လိုက...  \n",
       "2           ဒါက သင့်ရဲ့လို့ ထင်ပေမဲ့ တကယ်က ကျွန်မပါ။  \n",
       "3  ခရိုင်အားလုံးဟာ အခွန်ထမ်းတွေရဲ့ ပြည်နယ် ဝင်ငွေ...  \n",
       "4  Barik အသက်ပြင်းပြင်းရှူခြင်းက သူ့ကို စိတ်တည်ငြ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Records: 1627576.\n",
      "Remaining records: 1614484.\n"
     ]
    }
   ],
   "source": [
    "# cleaning combined dataset\n",
    "print(f\"Original Records: {len(combined)}.\")\n",
    "combined = combined.drop_duplicates()  # Remove duplicates\n",
    "combined = combined.dropna()  # Remove rows with missing values\n",
    "print(f\"Remaining records: {len(combined)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined, \"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to tokenize Burmese text using the selected SentencePiece model before applying Transformer tokenization.\n",
    "def tokenize(examples, tokenizer, spt_tokenizer, model_name):\n",
    "    spt_burmese = [spt_tokenizer.encode_as_pieces(text) for text in examples[\"target\"]]\n",
    "    examples[\"target\"] = [\" \".join(tokens) for tokens in spt_burmese]\n",
    "\n",
    "    if \"t5\" in model_name:  # mT5: text-to-text format\n",
    "        return tokenizer(\n",
    "            examples[\"source\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "        )\n",
    "    \n",
    "    # BERT-based models: Masked/Causal LM\n",
    "    return tokenizer(\n",
    "        examples[\"source\"],\n",
    "        examples[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12956461b484393bab1819dc1524132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for mBERT with bpe (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5f31325fc7486eb23cefefed61c25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67488e49df458cab45651f7cc0752b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for mBERT with unigram (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7887e5a1ea8d44bcaf2a6a0df7ec1720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dc5ff0a7f84a2486b5f5a60d1456dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for mT5 with bpe (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc2250bff3e4ba8b7d46593c331ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f233a14f244171b1cb33b3a521fd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for mT5 with unigram (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe96dd595db4e95a7aa6caf40dec44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75658988e7d4e7f8fa0b0383c0fe136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for XLM-R with bpe (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa6ce6773d14fa487107ed580b8f745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1283a83fbf5648e59611302558d007a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for XLM-R with unigram (num_proc=10):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979cbce780b54e37ae0cef4af3de1466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/1614484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize for each model and spt\n",
    "for model_name, tokenizer in train_tokenizers.items():\n",
    "    for spt_name, spt_tokenizer in spt_models.items():\n",
    "        dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "        # apply tokenize\n",
    "        dataset = dataset.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            desc=f\"Tokenizing dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save_model_variants_df_arrow(dataset, f\"{model_name.lower()}_{spt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine-tuning for:\n",
    "- mBERT (best perplexity, but weak BLEU/ROUGE)\n",
    "- mT5 (best for generation, but requires more data)\n",
    "- XLM-R (good BLEU/ROUGE, but poor perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_df_arrow(f\"{model_name}_{spt_name}\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 3,\n",
    "    \"fp16\": True,  # Mixed Precision Training\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,  # Lower loss is better\n",
    "    \"logging_dir\": \"./logs\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"optim\": \"adamw_torch_fused\",  # Optimized for GPU\n",
    "    \"use_cpu\": False if torch.cuda.is_available() else True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model):\n",
    "    \"\"\"\n",
    "    LoRA reduces memory and computational costs.\n",
    "    It fine-tunes only attention layers instead of the whole model.\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, spt_name):\n",
    "    print(f\"Fine-tuning {model_name} on using SPT-{spt_name.upper()}...\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = train_tokenizers[model_name]\n",
    "    model = train_models[model_name]\n",
    "\n",
    "    # Apply LoRA for efficient fine-tuning\n",
    "    model = apply_lora(model)\n",
    "\n",
    "    # tokenize dataset\n",
    "    tokenized_dataset = tokenized_datasets[model_name][spt_name]\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./model-variants/results/{model_name}_SPT-{spt_name.upper()}\",\n",
    "        **train_args\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stops training if no improvement for 3 epochs\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    name = f\"./model-variants/models/{model_name}_SPT-{spt_name.upper()}\"\n",
    "    model.save_pretrained(name)\n",
    "    tokenizer.save_pretrained(name)\n",
    "\n",
    "    print(f\"✅ Model {model_name} fine-tuned using SPT-{spt_name.upper()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(dataloader, model, tokenizer):\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Masked Predictions\"):\n",
    "            # Move batch data to GPU\n",
    "            masked_input_ids, attention_mask, _ = [x.to(device) for x in batch]\n",
    "\n",
    "            # Run model inference on GPU\n",
    "            outputs = model(input_ids=masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Replace masked tokens with predicted tokens\n",
    "            predicted_tokens_batch = masked_input_ids.clone()\n",
    "            for i in range(masked_input_ids.shape[0]):  # Loop over batch\n",
    "                mask_positions = (masked_input_ids[i] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "                for pos in mask_positions:\n",
    "                    predicted_token_id = torch.argmax(outputs.logits[i, pos], dim=-1).item()\n",
    "                    predicted_tokens_batch[i, pos] = predicted_token_id\n",
    "\n",
    "            # Decode predictions\n",
    "            batch_predictions = tokenizer.batch_decode(predicted_tokens_batch.cpu(), skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate predictions\n",
    "def generate_predictions_batch(dataloader, model, tokenizer, spt_processor, model_name):\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Generating Predictions\", unit=\"batch\"):\n",
    "        spt_encoded_batch = [\" \".join(spt_processor.encode_as_pieces(text)) for text in batch]\n",
    "        inputs = tokenizer(spt_encoded_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if \"t5\" in model_name.lower():  # ✅ Use `generate()` for mT5\n",
    "                output_tokens = model.generate(**inputs, max_length=128)\n",
    "                decoded_output = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in output_tokens]\n",
    "            else:  # ✅ Use fill-mask approach for mBERT & XLM-R\n",
    "                mask_token = tokenizer.mask_token\n",
    "                masked_inputs = [text.replace(\"[MASK]\", mask_token) for text in spt_encoded_batch]\n",
    "                inputs = tokenizer(masked_inputs, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                outputs = model(**inputs)\n",
    "                predictions_indices = torch.argmax(outputs.logits, dim=-1)\n",
    "                decoded_output = tokenizer.batch_decode(predictions_indices, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(decoded_output)\n",
    "\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
