{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece evaluate sacrebleu bert-score peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 19:43:00.858188: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-08 19:43:03.877668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-08 19:43:04.326385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-08 19:43:04.328190: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-08 19:43:05.131920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-08 19:43:18.637186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Transformers is only compatible with Keras 2, but you have explicitly set `TF_USE_LEGACY_KERAS` to `0`. This may result in unexpected behaviour or errors if Keras 3 objects are passed to Transformers models.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_from_disk, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset as tDataset\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoModelForMaskedLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 18:52:13.478109: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-08 18:52:25.442651: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-08 18:52:25.445253: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-08 18:52:25.449630: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'compute_capability': (7, 5), 'device_name': 'Tesla T4'}\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# for mac\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(\"GPU details: \", details)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# set GPU device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window / Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 19:43:56.112952: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-08 19:44:07.939907: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-08 19:44:07.942270: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# for window\n",
    "print(\"Tensorflow GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using PyTorch device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for batching\n",
    "class TextDataset(tDataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = [str(text) if text is not None else \"\" for text in texts] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model-variants df\n",
    "def save_model_variants_df(df, df_name):\n",
    "    df.to_csv(f\"model-variants/{df_name}.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_variants_df_arrow(df, df_name): \n",
    "    df.save_to_disk(f\"model-variants/{df_name}_hf_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load gen df\n",
    "def load_gen_df(df_name):\n",
    "    return pd.read_csv(f\"gen/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model-variants df\n",
    "def load_model_variants_df(df_name):\n",
    "    return pd.read_csv(f\"model-variants/{df_name}.csv\", header=0, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model-variants df with arrow\n",
    "def load_model_variants_df_arrow(df_name):\n",
    "    return load_from_disk(f\"model-variants/{df_name}_hf_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformer Models for Burmese\n",
    "This notebook fine-tunes three transformer models:\n",
    "- mBERT (Multilingual BERT)\n",
    "- mT5 (Multilingual T5)\n",
    "- XLM-RoBERTa\n",
    "\n",
    "Apply:\n",
    "- Sentence-Piece Tokenization for Burmese segmentation\n",
    "- LoRA for efficient fine-tuning\n",
    "- Prefix-Tuning for lightweight adaptations\n",
    "- Mixed Precision Training for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = {\n",
    "    \"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    #\"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "train_model_names = {\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    #\"mT5\": \"google/mt5-small\",\n",
    "    #\"XLM-R\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866a5b9117294de9ae1c105e180894e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8733b1e3f4471faf8f6e052ee378e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f33c8ea3b458da7f65a42990183f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828194242df042a1b8ef4397462b0a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train tokenizers\n",
    "train_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(train_model_names[\"mBERT\"]),\n",
    "    #\"mT5\": AutoTokenizer.from_pretrained(train_model_names[\"mT5\"], use_fast=False, legacy=True),\n",
    "    #\"XLM-R\": AutoTokenizer.from_pretrained(train_model_names[\"XLM-R\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing\n",
    "Datasets used for training:\n",
    "- myXNLI & ALT Corpus (normalized)\n",
    "- Back-translated datasets (NLLB, Seamless M4T)\n",
    "- Pseudo-parallel datasets (MiniLM, LaBSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english\": \"source\",\n",
    "        \"burmese\": \"target\",\n",
    "        \"english_back_translated\": \"source\",\n",
    "        \"burmese_translated\": \"target\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"source\", \"target\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\", \n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\", \n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "loaded_datasets = {}\n",
    "for key, file_list in datasets.items():\n",
    "    loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "combined = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined = combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archaeologists think that a fire broke out in ...</td>\n",
       "      <td>·Äõ·Äæ·Ä±·Ä∏·Äü·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äû·ÄØ·Äê·Ä±·Äû·ÄÆ·Äê·ÄΩ·Ä±·ÄÄ Knossos ·Äô·Äæ·Ä¨ ·Äô·ÄÆ·Ä∏·Äú·Ä±·Ä¨·ÄÑ·Ä∫·Äê·Ä¨ BC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are political meetings in every neighbor...</td>\n",
       "      <td>·Äõ·Äï·Ä∫·ÄÄ·ÄΩ·ÄÄ·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Ä¨ ·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÑ·Ä∂·Äõ·Ä±·Ä∏ ·Ä°·ÄÖ·Ää·Ä∫·Ä∏·Ä°·Äù·Ä±·Ä∏·Äê·ÄΩ·Ä±·Äõ·Äæ·Ä≠·Äê·Äö·Ä∫·Åã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the lawyer said that in article 712 (1) gao wa...</td>\n",
       "      <td>·Äõ·Äæ·Ä±·Ä∑·Äî·Ä±·ÄÄ ·Äï·ÄØ·Äí·Ä∫·Äô ·Åá·ÅÅ·ÅÇ (·ÅÅ) ·Äô·Äæ·Ä¨ Gao ·ÄÄ·Ä≠·ÄØ ·ÄÑ·ÄΩ·Ä±·ÄÄ·Äº·Ä±·Ä∏·ÄÜ·Ä≠·ÄØ·ÄÑ·Ä∫...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>things can get confusing when talking about do...</td>\n",
       "      <td>Dordogne ·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·Ä±·Ä¨·Äû·Ä±·Ä¨·Ä°·ÄÅ·Ä´·Åä ·Äù·Ä±·Ä∏·ÄÄ·ÄΩ·Ä¨·Äû·Ä±·Ä¨·Äî·Ä±·Äõ·Ä¨·Äô·Äª·Ä¨·Ä∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>making financial management a top priority acr...</td>\n",
       "      <td>·Äò·Äè·Äπ·Äç·Ä¨·Äõ·Ä±·Ä∏ ·ÄÖ·ÄÆ·Äô·Ä∂·ÄÅ·Äî·Ä∑·Ä∫·ÄÅ·ÄΩ·Ä≤·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Ää·Ä∫·Äë·Ä±·Ä¨·ÄÑ·Ä∫·ÄÖ·ÄØ ·Ä°·ÄÖ·Ä≠·ÄØ·Ä∏·Äõ·Äê...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  archaeologists think that a fire broke out in ...   \n",
       "1  there are political meetings in every neighbor...   \n",
       "2  the lawyer said that in article 712 (1) gao wa...   \n",
       "3  things can get confusing when talking about do...   \n",
       "4  making financial management a top priority acr...   \n",
       "\n",
       "                                              target  \n",
       "0  ·Äõ·Äæ·Ä±·Ä∏·Äü·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äû·ÄØ·Äê·Ä±·Äû·ÄÆ·Äê·ÄΩ·Ä±·ÄÄ Knossos ·Äô·Äæ·Ä¨ ·Äô·ÄÆ·Ä∏·Äú·Ä±·Ä¨·ÄÑ·Ä∫·Äê·Ä¨ BC...  \n",
       "1    ·Äõ·Äï·Ä∫·ÄÄ·ÄΩ·ÄÄ·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Ä¨ ·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·ÄÑ·Ä∂·Äõ·Ä±·Ä∏ ·Ä°·ÄÖ·Ää·Ä∫·Ä∏·Ä°·Äù·Ä±·Ä∏·Äê·ÄΩ·Ä±·Äõ·Äæ·Ä≠·Äê·Äö·Ä∫·Åã  \n",
       "2  ·Äõ·Äæ·Ä±·Ä∑·Äî·Ä±·ÄÄ ·Äï·ÄØ·Äí·Ä∫·Äô ·Åá·ÅÅ·ÅÇ (·ÅÅ) ·Äô·Äæ·Ä¨ Gao ·ÄÄ·Ä≠·ÄØ ·ÄÑ·ÄΩ·Ä±·ÄÄ·Äº·Ä±·Ä∏·ÄÜ·Ä≠·ÄØ·ÄÑ·Ä∫...  \n",
       "3  Dordogne ·Ä°·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äï·Äº·Ä±·Ä¨·Äû·Ä±·Ä¨·Ä°·ÄÅ·Ä´·Åä ·Äù·Ä±·Ä∏·ÄÄ·ÄΩ·Ä¨·Äû·Ä±·Ä¨·Äî·Ä±·Äõ·Ä¨·Äô·Äª·Ä¨·Ä∏...  \n",
       "4  ·Äò·Äè·Äπ·Äç·Ä¨·Äõ·Ä±·Ä∏ ·ÄÖ·ÄÆ·Äô·Ä∂·ÄÅ·Äî·Ä∑·Ä∫·ÄÅ·ÄΩ·Ä≤·Äô·Äæ·ÄØ·ÄÄ·Ä≠·ÄØ ·Äï·Äº·Ää·Ä∫·Äë·Ä±·Ä¨·ÄÑ·Ä∫·ÄÖ·ÄØ ·Ä°·ÄÖ·Ä≠·ÄØ·Ä∏·Äõ·Äê...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Combined dataset length: {len(combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined, \"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to tokenize Burmese text using the selected SentencePiece model before applying Transformer tokenization.\n",
    "def tokenize(examples, tokenizer, spt_tokenizer, model_name):\n",
    "    spt_burmese = [spt_tokenizer.encode_as_pieces(text) for text in examples[\"target\"]]\n",
    "    examples[\"target\"] = [\" \".join(tokens) for tokens in spt_burmese]\n",
    "\n",
    "    if \"t5\" in model_name:  # mT5: text-to-text format\n",
    "        return tokenizer(\n",
    "            examples[\"source\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "        )\n",
    "    \n",
    "    # BERT-based models: Masked/Causal LM\n",
    "    return tokenizer(\n",
    "        examples[\"source\"],\n",
    "        examples[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda33e440a9242bbbd0cf8879cf52dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for mT5 with bpe (num_proc=10):   0%|          | 0/1627576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize for each model and spt\n",
    "for model_name, tokenizer in train_tokenizers.items():\n",
    "    for spt_name, spt_tokenizer in spt_models.items():\n",
    "        dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "        # apply tokenize\n",
    "        dataset = dataset.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            desc=f\"Tokenizing dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save_model_variants_df_arrow(dataset, f\"{model_name.lower()}_{spt_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine-tuning for:\n",
    "- mBERT (best perplexity, but weak BLEU/ROUGE)\n",
    "- mT5 (best for generation, but requires more data)\n",
    "- XLM-R (good BLEU/ROUGE, but poor perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "train_models = {\n",
    "    \"mBERT\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"mBERT\"], num_labels=1).to(device),\n",
    "    #\"mT5\": AutoModelForSeq2SeqLM.from_pretrained(train_model_names[\"mT5\"]).to(device),\n",
    "    #\"XLM-R\": AutoModelForSequenceClassification.from_pretrained(train_model_names[\"XLM-R\"], num_labels=1).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_df_arrow(f\"{model_name.lower()}_{spt_name}\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 3,\n",
    "    \"fp16\": True,  # Mixed Precision Training\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,  # Lower loss is better\n",
    "    \"logging_dir\": \"./logs\",\n",
    "    \"logging_steps\": 100,\n",
    "    \"optim\": \"adamw_torch_fused\",  # Optimized for GPU\n",
    "    \"use_cpu\": False if torch.cuda.is_available() else True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model):\n",
    "    \"\"\"\n",
    "    LoRA reduces memory and computational costs.\n",
    "    It fine-tunes only attention layers instead of the whole model.\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fine tune model\n",
    "def fine_tune_model(model_name, spt_name):\n",
    "    print(f\"Fine-tuning {model_name} on using SPT-{spt_name.upper()}...\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = train_tokenizers[model_name]\n",
    "    model = train_models[model_name]\n",
    "\n",
    "    # Apply LoRA for efficient fine-tuning\n",
    "    model = apply_lora(model)\n",
    "\n",
    "    # tokenize dataset\n",
    "    tokenized_dataset = tokenized_datasets[model_name][spt_name]\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"model-variants/results/{model_name}_SPT-{spt_name.upper()}\",\n",
    "        **train_args\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stops training if no improvement for 3 epochs\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    name = f\"model-variants/models/{model_name}_SPT-{spt_name.upper()}\"\n",
    "    model.save_pretrained(name)\n",
    "    tokenizer.save_pretrained(name)\n",
    "\n",
    "    print(f\"Model {model_name} fine-tuned using SPT-{spt_name.upper()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add 'labels' in dataset\n",
    "def add_labels(example):\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30374229dcd240058b09fafe153fe72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/1627576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets[\"mBERT\"][\"bpe\"] = tokenized_datasets[\"mBERT\"][\"bpe\"].map(\n",
    "    add_labels, \n",
    "    batched=True,\n",
    "    desc=f\"Tokenizing dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning mBERT on using SPT-BPE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_7397/621823064.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.655, 'grad_norm': 8.106327056884766, 'learning_rate': 6e-06, 'epoch': 0.0009830570123914337}\n"
     ]
    }
   ],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT and XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class for masked\n",
    "class MaskedTextDataset(tDataset):\n",
    "    def __init__(self, texts, tokenizer, mask_ratio=0.15, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n",
    "\n",
    "        # Tokenize and move tensors to GPU\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0).to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0).to(device)\n",
    "\n",
    "        # Apply random masking\n",
    "        seq_length = input_ids.shape[0]\n",
    "        num_to_mask = max(1, int(self.mask_ratio * (seq_length - 2)))  # Avoid CLS/SEP\n",
    "        mask_indices = torch.randperm(seq_length - 2)[:num_to_mask] + 1  # Avoid first and last token\n",
    "\n",
    "        masked_input_ids = input_ids.clone()\n",
    "        masked_input_ids[mask_indices] = self.tokenizer.mask_token_id  # Replace with [MASK] token\n",
    "\n",
    "        return masked_input_ids, attention_mask, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate masked predictions\n",
    "def generate_masked_predictions_batch(dataloader, model, tokenizer):\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Masked Predictions\"):\n",
    "            # Move batch data to GPU\n",
    "            masked_input_ids, attention_mask, _ = [x.to(device) for x in batch]\n",
    "\n",
    "            # Run model inference on GPU\n",
    "            outputs = model(input_ids=masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Replace masked tokens with predicted tokens\n",
    "            predicted_tokens_batch = masked_input_ids.clone()\n",
    "            for i in range(masked_input_ids.shape[0]):  # Loop over batch\n",
    "                mask_positions = (masked_input_ids[i] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "                for pos in mask_positions:\n",
    "                    predicted_token_id = torch.argmax(outputs.logits[i, pos], dim=-1).item()\n",
    "                    predicted_tokens_batch[i, pos] = predicted_token_id\n",
    "\n",
    "            # Decode predictions\n",
    "            batch_predictions = tokenizer.batch_decode(predicted_tokens_batch.cpu(), skip_special_tokens=True)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mBERT with BPE\n",
    "mbert_bpe_trained_path = \"model-variants/models/mBERT_SPT-BPE\"\n",
    "mbert_bpe_trained_tokenizer = AutoTokenizer.from_pretrained(mbert_bpe_trained_path)\n",
    "mbert_bpe_trained_model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "mbert_bpe_trained_model = torch.compile(mbert_bpe_trained_model)\n",
    "mbert_bpe_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mbert_bpe_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "mbert_bpe_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "mbert_bpe_trained_predictions_texts = mbert_bpe_trained_predictions[\"target\"].tolist()\n",
    "mbert_bpe_trained_predictions_dataset = MaskedTextDataset(mbert_bpe_trained_predictions_texts, mbert_bpe_trained_tokenizer)\n",
    "mbert_bpe_trained_predictions_dataloader = DataLoader(\n",
    "    mbert_bpe_trained_predictions_dataset, \n",
    "    batch_size=mbert_bpe_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "mbert_bpe_trained_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    mbert_bpe_trained_predictions_dataloader, \n",
    "    mbert_bpe_trained_model, \n",
    "    mbert_bpe_trained_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(mbert_bpe_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained mbert predictions\n",
    "save_model_variants_df(mbert_bpe_trained_predictions, \"mbert_bpe_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mBERT with BPE\n",
    "mbert_unigram_trained_path = \"model-variants/models/mBERT_SPT-UNIGRAM\"\n",
    "mbert_unigram_trained_tokenizer = AutoTokenizer.from_pretrained(mbert_unigram_trained_path)\n",
    "mbert_unigram_trained_model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "mbert_unigram_trained_model = torch.compile(mbert_unigram_trained_model)\n",
    "mbert_unigram_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mbert_unigram_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "mbert_unigram_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "mbert_unigram_trained_predictions_texts = mbert_unigram_trained_predictions[\"target\"].tolist()\n",
    "mbert_unigram_trained_predictions_dataset = MaskedTextDataset(mbert_unigram_trained_predictions_texts, mbert_unigram_trained_tokenizer)\n",
    "mbert_unigram_trained_predictions_dataloader = DataLoader(\n",
    "    mbert_unigram_trained_predictions_dataset, \n",
    "    batch_size=mbert_unigram_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "mbert_unigram_trained_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    mbert_unigram_trained_predictions_dataloader, \n",
    "    mbert_unigram_trained_model, \n",
    "    mbert_unigram_trained_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(mbert_unigram_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained mbert predictions\n",
    "save_model_variants_df(mbert_unigram_trained_predictions, \"mbert_unigram_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mBERT with BPE\n",
    "xlmr_bpe_trained_path = \"model-variants/models/XLM-R_SPT-BPE\"\n",
    "xlmr_bpe_trained_tokenizer = AutoTokenizer.from_pretrained(xlmr_bpe_trained_path)\n",
    "xlmr_bpe_trained_model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "xlmr_bpe_trained_model = torch.compile(xlmr_bpe_trained_model)\n",
    "xlmr_bpe_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "xlmr_bpe_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "xlmr_bpe_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "xlmr_bpe_trained_predictions_texts = xlmr_bpe_trained_predictions[\"target\"].tolist()\n",
    "xlmr_bpe_trained_predictions_dataset = MaskedTextDataset(xlmr_bpe_trained_predictions_texts, xlmr_bpe_trained_tokenizer)\n",
    "xlmr_bpe_trained_predictions_dataloader = DataLoader(\n",
    "    xlmr_bpe_trained_predictions_dataset, \n",
    "    batch_size=xlmr_bpe_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "xlmr_bpe_trained_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    xlmr_bpe_trained_predictions_dataloader, \n",
    "    xlmr_bpe_trained_model, \n",
    "    xlmr_bpe_trained_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(xlmr_bpe_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained xlmr predictions\n",
    "save_model_variants_df(xlmr_bpe_trained_predictions, \"xlmr_bpe_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mBERT with BPE\n",
    "xlmr_unigram_trained_path = \"model-variants/models/XLM-R_SPT-UNIGRAM\"\n",
    "xlmr_unigram_trained_tokenizer = AutoTokenizer.from_pretrained(xlmr_unigram_trained_path)\n",
    "xlmr_unigram_trained_model = AutoModelForMaskedLM.from_pretrained().to(device)\n",
    "xlmr_unigram_trained_model = torch.compile(xlmr_unigram_trained_model)\n",
    "xlmr_unigram_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "xlmr_unigram_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "xlmr_unigram_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "xlmr_unigram_trained_predictions_texts = xlmr_unigram_trained_predictions[\"target\"].tolist()\n",
    "xlmr_unigram_trained_predictions_dataset = MaskedTextDataset(xlmr_unigram_trained_predictions_texts, xlmr_unigram_trained_tokenizer)\n",
    "xlmr_unigram_trained_predictions_dataloader = DataLoader(\n",
    "    xlmr_unigram_trained_predictions_dataset, \n",
    "    batch_size=xlmr_unigram_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "xlmr_unigram_trained_predictions[\"generated\"] = generate_masked_predictions_batch(\n",
    "    xlmr_unigram_trained_predictions_dataloader, \n",
    "    xlmr_unigram_trained_model, \n",
    "    xlmr_unigram_trained_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(xlmr_unigram_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained xlmr predictions\n",
    "save_model_variants_df(xlmr_unigram_trained_predictions, \"xlmr_unigram_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate predictions\n",
    "def generate_predictions_batch(dataloader, model, tokenizer, spt_processor):\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Generating Predictions\", unit=\"batch\"):\n",
    "        # Apply spt\n",
    "        spt_encoded_batch = [\" \".join(spt_processor.encode_as_pieces(text)) for text in batch]\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(spt_encoded_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate output sequence\n",
    "            output_tokens = model.generate(**inputs, max_length=128)\n",
    "\n",
    "        # Decode generated sequences\n",
    "        decoded_output = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in output_tokens]\n",
    "        predictions.extend(decoded_output)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mT5 with BPE\n",
    "mt5_bpe_trained_path = \"model-variants/models/mT5_SPT-BPE\"\n",
    "mt5_bpe_trained_tokenizer = AutoTokenizer.from_pretrained(mt5_bpe_trained_path)\n",
    "mt5_bpe_trained_model = AutoModelForSeq2SeqLM.from_pretrained().to(device)\n",
    "mt5_bpe_trained_model = torch.compile(mt5_bpe_trained_model)\n",
    "mt5_bpe_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mt5_bpe_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "mt5_bpe_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "mt5_bpe_trained_predictions_texts = mt5_bpe_trained_predictions[\"target\"].tolist()\n",
    "mt5_bpe_trained_predictions_dataset = TextDataset(mt5_bpe_trained_predictions_texts, mt5_bpe_trained_tokenizer)\n",
    "mt5_bpe_trained_predictions_dataloader = DataLoader(\n",
    "    mt5_bpe_trained_predictions_dataset, \n",
    "    batch_size=mt5_bpe_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "mt5_bpe_trained_predictions[\"generated\"] = generate_predictions_batch(\n",
    "    mt5_bpe_trained_predictions_dataloader, \n",
    "    mt5_bpe_trained_model, \n",
    "    mt5_bpe_trained_tokenizer,\n",
    "    spt_models[\"bpe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(mt5_bpe_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained mt5 predictions\n",
    "save_model_variants_df(mt5_bpe_trained_predictions, \"mt5_bpe_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers & models for mT5 with Unigram\n",
    "mt5_unigram_trained_path = \"model-variants/models/mT5_SPT-BPE\"\n",
    "mt5_unigram_trained_tokenizer = AutoTokenizer.from_pretrained(mt5_unigram_trained_path)\n",
    "mt5_unigram_trained_model = AutoModelForSeq2SeqLM.from_pretrained().to(device)\n",
    "mt5_unigram_trained_model = torch.compile(mt5_unigram_trained_model)\n",
    "mt5_unigram_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mt5_unigram_trained_predictions = load_model_variants_df(\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "mt5_unigram_trained_predictions_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "mt5_unigram_trained_predictions_texts = mt5_unigram_trained_predictions[\"target\"].tolist()\n",
    "mt5_unigram_trained_predictions_dataset = TextDataset(mt5_unigram_trained_predictions_texts, mt5_unigram_trained_tokenizer)\n",
    "mt5_unigram_trained_predictions_dataloader = DataLoader(\n",
    "    mt5_unigram_trained_predictions_dataset, \n",
    "    batch_size=mt5_unigram_trained_predictions_batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation\n",
    "mt5_unigram_trained_predictions[\"generated\"] = generate_predictions_batch(\n",
    "    mt5_unigram_trained_predictions_dataloader, \n",
    "    mt5_unigram_trained_model, \n",
    "    mt5_unigram_trained_tokenizer,\n",
    "    spt_models[\"unigram\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(mt5_unigram_trained_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained mt5 predictions\n",
    "save_model_variants_df(mt5_unigram_trained_predictions, \"mt5_unigram_trained_predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
