{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded from S3 successfully!\n"
     ]
    }
   ],
   "source": [
    "# S3 Setup\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"ms-thesis-sagemaker\"  # Replace with your S3 bucket\n",
    "s3_file_path = \"mbert_bpe_hf_dataset.zip\"  # Replace with the file name in S3\n",
    "local_zip_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/data/mbert_bpe_hf_dataset.zip\"  # Where to save in SageMaker\n",
    "\n",
    "# Download the ZIP file from S3\n",
    "s3.download_file(bucket_name, s3_file_path, local_zip_path)\n",
    "print(\"ZIP file downloaded from S3 successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "extract_path = \"/home/ec2-user/SageMaker/ms-thesis/model-variants/data\"  # Where to extract\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"ZIP file extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file deleted to free space.\n"
     ]
    }
   ],
   "source": [
    "os.remove(local_zip_path)\n",
    "print(\"ZIP file deleted to free space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to zip and upload\n",
    "def zip_and_upload_to_s3(source_folder):\n",
    "    bucket_name = \"ms-thesis-sagemaker\"\n",
    "\n",
    "    zip_file = f\"{source_folder}.zip\"\n",
    "    s3_key = f\"uploads/{zip_file}\"\n",
    "\n",
    "    # Create a zip archive\n",
    "    shutil.make_archive(zip_file.replace(\".zip\", \"\"), 'zip', source_folder)\n",
    "    print(f\"Zipped {source_folder} -> {zip_file}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.upload_file(zip_file, bucket_name, s3_key)\n",
    "    print(f\"Uploaded to S3: s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "    # delete zip file\n",
    "    os.remove(zip_file)\n",
    "    print(\"ZIP file deleted to free space.\")\n",
    "\n",
    "    return f\"s3://{bucket_name}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip logs\n",
    "zip_and_upload_to_s3(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "zip_and_upload_to_s3(\"model-variants/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "zip_and_upload_to_s3(\"model-variants/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence_transformers sentencepiece peft datasets bert_score sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c conda-forge conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::rouge-score -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch.nn as nn\n",
    "from utils.dataframe import (\n",
    "    load_gen_df, save_tmp_df, load_tmp_df, load_models_df,\n",
    "    save_model_variants_df, load_model_variants_df,\n",
    "    save_model_variants_hf, load_model_variants_hf,\n",
    "    save_model_variants_gen_df, load_model_variants_gen_df,\n",
    "    convert_to_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    generate_masked_predictions_hf, generate_mt5_predictions,\n",
    "    compute_metrics_hf,\n",
    "    convert_to_mean_scores_df,\n",
    "    get_fine_tuned_model, get_embedded_fine_tuned_model,\n",
    "    compute_multilingual_masked_perplexity_single, compute_multilingual_mt5_perplexity_single,\n",
    "    extract_extended_metrics_from_logs,\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, PrefixTuningConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = {\n",
    "    #\"bpe\": spm.SentencePieceProcessor(\"spt/spt_bpe.model\"),\n",
    "    \"unigram\": spm.SentencePieceProcessor(\"spt/spt_unigram.model\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agrs = {\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": True,\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"logging_steps\": 1000,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"disable_tqdm\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, model_name, is_student):\n",
    "    \"\"\"\n",
    "    Applies LoRA for efficient fine-tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select correct LoRA target layers\n",
    "    if \"t5\" in model_name.lower():\n",
    "        target_modules = [\"q\", \"v\"]  # LoRA for T5/mT5\n",
    "    else:\n",
    "        target_modules = [\"query\", \"value\"]  # LoRA for BERT\n",
    "\n",
    "    # Define LoRA Configuration\n",
    "    if is_student:\n",
    "        lora_config = LoraConfig(\n",
    "            r=4,                    # Rank of LoRA matrices\n",
    "            lora_alpha=8,           # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.05,      # Prevents overfitting\n",
    "            bias=\"none\"\n",
    "        )\n",
    "    else:\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,                    # Rank of LoRA matrices\n",
    "            lora_alpha=16,          # Scaling factor\n",
    "            target_modules=target_modules,  \n",
    "            lora_dropout=0.1,       # Prevents overfitting\n",
    "        )\n",
    "\n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Move model to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"LoRA applied to {model_name} (Target Modules: {target_modules})\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prefix_tuning(model):\n",
    "    \"\"\"\n",
    "    Applies Prefix-Tuning to the model by adding tunable prefix parameters.\n",
    "    \"\"\"\n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,  # Required for mT5\n",
    "        num_virtual_tokens=20,  # Number of prefix tokens to learn\n",
    "        encoder_hidden_size=model.config.d_model,  # Same as mT5 hidden size\n",
    "        prefix_projection=True  # Enables projection into model hidden state\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, prefix_config)\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"Prefix-Tuning applied with {} virtual tokens.\".format(prefix_config.num_virtual_tokens))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Preprocessing\n",
    "Datasets used for training:\n",
    "- myXNLI & ALT Corpus (normalized)\n",
    "- Back-translated datasets (NLLB, Seamless M4T)\n",
    "- Pseudo Parallel Corpus (Minilm, LaBse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_and_rename_columns_multilingual(file_name):\n",
    "    df = load_gen_df(f\"{file_name}\")\n",
    "\n",
    "    column_mapping = {\n",
    "        \"english\": \"source\",\n",
    "        \"burmese\": \"target\",\n",
    "        \"english_back_translated\": \"source\",\n",
    "        \"burmese_translated\": \"target\",\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Ensure only required columns exist\n",
    "    df = df[[\"source\", \"target\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "datasets = {\n",
    "    \"normal\": [\n",
    "        \"myxnli_normalized_1\", \n",
    "        \"myxnli_normalized_2\", \n",
    "        \"alt_combined_normalized\"\n",
    "    ],\n",
    "    \"nllb_back_translated\": [\n",
    "        \"myxnli_nllb_back_translated_final_1\", \n",
    "        \"myxnli_nllb_back_translated_final_2\", \n",
    "        \"alt_combined_nllb_back_translated_final\"\n",
    "    ],\n",
    "    \"seamless_m4t_back_translated\": [\n",
    "        \"myxnli_seamless_m4t_back_translated_final_1\", \n",
    "        \"myxnli_seamless_m4t_back_translated_final_2\", \n",
    "        \"alt_combined_seamless_m4t_back_translated_final\"\n",
    "    ],\n",
    "    \"minilm_pseudo_parallel_corpus\": [\n",
    "        \"minilm_pseudo_parallel_corpus_final\"\n",
    "    ],\n",
    "    \"labse_pseudo_parallel_corpus\": [\n",
    "        \"labse_pseudo_parallel_corpus_final\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "loaded_datasets = {}\n",
    "for key, file_list in datasets.items():\n",
    "    loaded_datasets[key] = [load_and_rename_columns_multilingual(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets\n",
    "combined = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined = combined.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disheartened in hong kong</td>\n",
       "      <td>ဟောင်ကောင်မှာ စိတ်ပျက်သွားတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kyushu is an important part of japanese mythol...</td>\n",
       "      <td>Kyushu ဟာ ဂျပန် ဒဏ္ဍာရီရဲ့ အရေးပါတဲ့ အစိတ်အပို...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it's simple for sure.</td>\n",
       "      <td>ဒါက ရိုးရှင်းတဲ့အတွက် အသေအချာပါ။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and they think that i'm better than i am.</td>\n",
       "      <td>ပြီးတော့ သူတို့က ငါထက် ပိုကောင်းတယ်လို့ ထင်နေက...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there were jokes about these comments, even th...</td>\n",
       "      <td>ဒီမှတ်ချက်တွေအကြောင်း နောက်ပြောင်တာတွေ ရှိခဲ့တ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                          disheartened in hong kong   \n",
       "1  kyushu is an important part of japanese mythol...   \n",
       "2                              it's simple for sure.   \n",
       "3          and they think that i'm better than i am.   \n",
       "4  there were jokes about these comments, even th...   \n",
       "\n",
       "                                              target  \n",
       "0                     ဟောင်ကောင်မှာ စိတ်ပျက်သွားတယ်။  \n",
       "1  Kyushu ဟာ ဂျပန် ဒဏ္ဍာရီရဲ့ အရေးပါတဲ့ အစိတ်အပို...  \n",
       "2                   ဒါက ရိုးရှင်းတဲ့အတွက် အသေအချာပါ။  \n",
       "3  ပြီးတော့ သူတို့က ငါထက် ပိုကောင်းတယ်လို့ ထင်နေက...  \n",
       "4  ဒီမှတ်ချက်တွေအကြောင်း နောက်ပြောင်တာတွေ ရှိခဲ့တ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset length: 1627576\n"
     ]
    }
   ],
   "source": [
    "# print length\n",
    "print(f\"Combined dataset length: {len(combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined, \"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets including pesudo parallel\n",
    "combined_pseudo_parallel_corpus = pd.concat(\n",
    "    loaded_datasets[\"normal\"] + \n",
    "    loaded_datasets[\"nllb_back_translated\"] + \n",
    "    loaded_datasets[\"seamless_m4t_back_translated\"] +\n",
    "    loaded_datasets[\"minilm_pseudo_parallel_corpus\"] +\n",
    "    loaded_datasets[\"labse_pseudo_parallel_corpus\"],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data to prevent order bias\n",
    "combined_pseudo_parallel_corpus = combined_pseudo_parallel_corpus.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i know you've never done it.</td>\n",
       "      <td>မင်းမလုပ်ဘူးဆိုတာ ငါသိတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people who shop at tiffany's are generally ver...</td>\n",
       "      <td>Tiffany's မှာ ဈေးဝယ်သူတွေဟာ အများအားဖြင့် အလွန...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daniel told his colleagues that they are to bl...</td>\n",
       "      <td>ဟောလိဝုဒ်၏ လက်ရှိအခြေအနေအတွက် ၎င်းတို့ကို အပြစ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>julia sweeney's little sister is dead.</td>\n",
       "      <td>Julia Sweeney ရဲ့ ညီမလေး က သေဆုံးသွားတယ်</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she doesn't call this mission ambitious.</td>\n",
       "      <td>သူမဟာ ဒီတာဝန်ကို ရည်မှန်းချက်ကြီးတယ်လို့ မခေါ်...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                       i know you've never done it.   \n",
       "1  people who shop at tiffany's are generally ver...   \n",
       "2  daniel told his colleagues that they are to bl...   \n",
       "3             julia sweeney's little sister is dead.   \n",
       "4           she doesn't call this mission ambitious.   \n",
       "\n",
       "                                              target  \n",
       "0                         မင်းမလုပ်ဘူးဆိုတာ ငါသိတယ်။  \n",
       "1  Tiffany's မှာ ဈေးဝယ်သူတွေဟာ အများအားဖြင့် အလွန...  \n",
       "2  ဟောလိဝုဒ်၏ လက်ရှိအခြေအနေအတွက် ၎င်းတို့ကို အပြစ...  \n",
       "3           Julia Sweeney ရဲ့ ညီမလေး က သေဆုံးသွားတယ်  \n",
       "4  သူမဟာ ဒီတာဝန်ကို ရည်မှန်းချက်ကြီးတယ်လို့ မခေါ်...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display combined dataset\n",
    "display(combined_pseudo_parallel_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 3001668\n"
     ]
    }
   ],
   "source": [
    "# display data size\n",
    "print(f\"Total Size: {len(combined_pseudo_parallel_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "save_model_variants_df(combined_pseudo_parallel_corpus, \"combined_pseudo_parallel_corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-Tuning Transformer Models for Burmese\n",
    "This notebook fine-tunes three transformer models:\n",
    "- mBERT (best perplexity, but weak BLEU/ROUGE)\n",
    "- mT5 (best for generation, but requires more data)\n",
    "- XLM-R (good BLEU/ROUGE, but poor perplexity)\n",
    "\n",
    "Apply:\n",
    "- Sentence-Piece Tokenization for Burmese segmentation\n",
    "- LoRA for efficient fine-tuning\n",
    "- Prefix-Tuning for lightweight adaptations\n",
    "- Mixed Precision Training for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model names\n",
    "train_model_names = {\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"mT5\": \"google/mt5-small\",\n",
    "    \"XLM-R\": \"xlm-roberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizers\n",
    "train_tokenizers = {\n",
    "    \"mBERT\": AutoTokenizer.from_pretrained(train_model_names[\"mBERT\"]),\n",
    "    \"mT5\": AutoTokenizer.from_pretrained(train_model_names[\"mT5\"], use_fast=False, legacy=True),\n",
    "    \"XLM-R\": AutoTokenizer.from_pretrained(train_model_names[\"XLM-R\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "train_models = {\n",
    "    \"mBERT\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"mBERT\"]).to(device),\n",
    "    \"mT5\": AutoModelForSeq2SeqLM.from_pretrained(train_model_names[\"mT5\"]).to(device),\n",
    "    \"XLM-R\": AutoModelForMaskedLM.from_pretrained(train_model_names[\"XLM-R\"]).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples, tokenizer, spt_tokenizer, model_name):\n",
    "    \"\"\"\n",
    "    Tokenizes Burmese text using the selected SentencePiece model before applying Transformer tokenization.\n",
    "    \"\"\"\n",
    "    # Apply SentencePiece Tokenization\n",
    "    batch_source_tokens = [spt_tokenizer.encode(text, out_type=str) for text in examples[\"source\"]]\n",
    "    batch_target_tokens = [spt_tokenizer.encode(text, out_type=str) for text in examples[\"target\"]]\n",
    "\n",
    "    # Flatten and convert tokens to text\n",
    "    batch_source_texts = [\" \".join(tokens) for tokens in batch_source_tokens]\n",
    "    batch_target_texts = [\" \".join(tokens) for tokens in batch_target_tokens]\n",
    "\n",
    "    # Flatten and convert tokens to text\n",
    "    batch_source_texts = [\" \".join(tokens) for tokens in batch_source_tokens]\n",
    "    batch_target_texts = [\" \".join(tokens) for tokens in batch_target_tokens]\n",
    "\n",
    "    if \"t5\" in model_name.lower():\n",
    "        # Tokenize using mT5 tokenizer (Seq2Seq)\n",
    "        model_inputs = tokenizer(\n",
    "            batch_source_texts, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Tokenize target (labels) separately (mT5 requires explicit target tokenization)\n",
    "        labels = tokenizer(\n",
    "            batch_target_texts, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # Replace pad tokens in labels with -100 (Hugging Face ignores -100 in loss calculation)\n",
    "        labels = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels  # Assign labels for training\n",
    "\n",
    "        return model_inputs\n",
    "    \n",
    "    else:\n",
    "        # Tokenize using XLM-R tokenizer\n",
    "        model_inputs = tokenizer(\n",
    "            batch_source_texts, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            batch_target_texts, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels  # Assign labels for MLM-style training\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize for each model and spt\n",
    "for model_name, tokenizer in train_tokenizers.items():\n",
    "    for spt_name, spt_tokenizer in spt_models.items():\n",
    "        # load dataset\n",
    "        if \"t5\" in model_name.lower():\n",
    "            dataset = load_model_variants_df(\"combined_pseudo_parallel_corpus\")\n",
    "        else:\n",
    "            dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "        # Split into 80% train, 20% test\n",
    "        train_df = dataset.sample(frac=0.8, random_state=42)\n",
    "        test_df = dataset.drop(train_df.index)\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        train_df = convert_to_hf(train_df)\n",
    "        test_df = convert_to_hf(test_df)\n",
    "\n",
    "        # apply tokenization to train dataset\n",
    "        train_tokenized = train_df.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            batch_size=16,\n",
    "            desc=f\"Tokenizing train dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # memory occur when mT5 with unigram\n",
    "        if \"t5\" in model_name.lower() and spt_name == \"unigram\":\n",
    "            chunk_size = 5\n",
    "        else:\n",
    "            chunk_size = 1\n",
    "\n",
    "        # save\n",
    "        save_model_variants_hf(train_tokenized, f\"{model_name.lower()}_{spt_name}_train\", chunk_size)\n",
    "\n",
    "        # apply tokenization to train dataset\n",
    "        test_tokenized = test_df.map(\n",
    "            lambda x, _: tokenize(x, tokenizer, spt_tokenizer, model_name),\n",
    "            batched=True,\n",
    "            batch_size=16,\n",
    "            desc=f\"Tokenizing test dataset for {model_name} with {spt_name}\",\n",
    "            with_indices=True,  # Passing index as a second argument\n",
    "            num_proc=10\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        save_model_variants_hf(test_tokenized, f\"{model_name.lower()}_{spt_name}_test\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized train dataset\n",
    "tokenized_train_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_hf(f\"{model_name.lower()}_{spt_name}_train\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized test dataset\n",
    "tokenized_test_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_hf(f\"{model_name.lower()}_{spt_name}_test\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in train_tokenizers.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, spt_name, batch_size):\n",
    "    \"\"\"\n",
    "    Fine-tunes the model with LoRA on the specified SentencePiece tokenization (SPT).\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning {model_name} using SPT-{spt_name.upper()}...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = train_tokenizers[model_name]\n",
    "    model = train_models[model_name]\n",
    "\n",
    "    # Move model to GPU before applying LoRA\n",
    "    model.to(device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    model = apply_lora(model, model_name, False)\n",
    "\n",
    "    if \"t5\" in model_name.lower():\n",
    "        model = apply_prefix_tuning(model)\n",
    "\n",
    "    # display trainable parameters\n",
    "    print(model.print_trainable_parameters())\n",
    "    \n",
    "    # load dataset\n",
    "    train_data = tokenized_train_datasets[model_name][spt_name]\n",
    "    val_data = tokenized_test_datasets[model_name][spt_name]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    trained_model_name = f\"{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        **train_agrs,\n",
    "        output_dir=f\"model-variants/results/{trained_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=5e-5,\n",
    "        logging_dir=f\"model-variants/logs/{trained_model_name}\",\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{trained_model_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model `{model_name}` fine-tuned and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mBERT\", \"bpe\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mbert_bpe_trained_results = extract_extended_metrics_from_logs(\"mBERT_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mbert_bpe_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mbert_bpe_trained_results, \"mbert_bpe_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mBERT\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mbert_unigram_trained_results = extract_extended_metrics_from_logs(\"mBERT_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mbert_unigram_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mbert_unigram_trained_results, \"mbert_unigram_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "xlmr_bpe_trained_results = extract_extended_metrics_from_logs(\"XLM-R_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(xlmr_bpe_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(xlmr_bpe_trained_results, \"xlmr_bpe_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"XLM-R\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "xlmr_unigram_trained_results = extract_extended_metrics_from_logs(\"XLM-R_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(xlmr_unigram_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(xlmr_unigram_trained_results, \"xlmr_unigram_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_model(\"mT5\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mt5_bpe_trained_results = extract_extended_metrics_from_logs(\"mT5_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mt5_bpe_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mt5_bpe_trained_results, \"mt5_bpe_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_model(\"mT5\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mt5_unigram_trained_results = extract_extended_metrics_from_logs(\"mT5_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mt5_unigram_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mt5_unigram_trained_results, \"mt5_unigram_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_fine_tuned_model(model_name, spt_name):\n",
    "    base_model_name = train_model_names[model_name]\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, base_model_name, device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load dataset \n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # remove comment for debug\n",
    "    dataset = dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    if \"t5\" in model_name.lower():\n",
    "        dataset = generate_mt5_predictions(model, tokenizer, device)\n",
    "    else:\n",
    "        dataset = generate_masked_predictions_hf(dataset, model, tokenizer, device)\n",
    "\n",
    "    # Display results\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_df(dataset, f\"{model_name}_{spt_name}_trained_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram\n",
    "generate_predictions_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram\n",
    "generate_predictions_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf(metrics_dataset)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {model_name} with {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {metrics_dataset['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics_dataset['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics_dataset['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics_dataset['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics_dataset['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics_dataset['bert_score'].mean()}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{model_name}_{spt_name}_trained_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with BPE\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with Unigram\n",
    "compute_metric_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with BPE\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with Unigram\n",
    "compute_metric_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with BPE\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mT5 with Unigram\n",
    "compute_metric_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity for fine-tuned model using HF Dataset\n",
    "def compute_perplexity_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset as Hugging Face Dataset\n",
    "    perplexity = load_model_variants_hf(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute perplexity using HF Dataset\n",
    "    if \"t5\" in model_name.lower():\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_mt5_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "    else:\n",
    "        perplexity = perplexity.map(lambda example: {\n",
    "            \"perplexity\": compute_multilingual_masked_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "        })\n",
    "\n",
    "    # Display Perplexity Score\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_trained_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_fine_tuned_model(\"mT5\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "\n",
    "        evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_trained_predictions\")\n",
    "\n",
    "        # load metrics and set\n",
    "        metrics = load_tmp_df(f\"{model_name}_{spt_name}_trained_metrics\")\n",
    "        evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "        evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "        evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "        evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "        evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "        evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "        # load perplexity and set\n",
    "        perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "        evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "        save_model_variants_df(evaluation_results, f\"{model_name}_{spt_name}_trained_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trained_benchmarking_datasets = {}\n",
    "for model_name in train_model_names.keys():\n",
    "    for spt_name in spt_models.keys():\n",
    "        df = load_model_variants_df(f\"{model_name}_{spt_name}_trained_evaluation_results\")\n",
    "        trained_benchmarking_datasets[f\"{model_name} {spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "trained_benchmarking_mean_scores = convert_to_mean_scores_df(trained_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(trained_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(trained_benchmarking_mean_scores, \"trained_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enhance Burmese Contextual Representations\n",
    "- Use LASER, mUSE, and FastText for cross-lingual and morphology-aware training.\n",
    "- Fine-tune mBERT, XLM-R on Burmese dataset after adding contextual embedded.\n",
    "- Train models again using combined embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model names\n",
    "embedding_model_names = [\"mBERT\", \"XLM-R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized train dataset\n",
    "embedded_train_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_hf(f\"contextual_embedded_{model_name.lower()}_{spt_name}_train\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in embedding_model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized test dataset\n",
    "embedded_test_datasets = {\n",
    "    model_name: {\n",
    "        spt_name: load_model_variants_hf(f\"contextual_embedded_{model_name.lower()}_{spt_name}_test\")\n",
    "        for spt_name in spt_models.keys()\n",
    "    }\n",
    "    for model_name in embedding_model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moved to embeddings-dataset-preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_with_contextual_embeddings(model_name, spt_name, batch_size):\n",
    "    \"\"\"\n",
    "    Second-round fine-tuning for XLM-R and mBERT using LoRA + Contextual Embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning {model_name} using SPT-{spt_name.upper()} with contextual embeddings...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    model, tokenizer = get_fine_tuned_model(model_name, spt_name, device)\n",
    "\n",
    "    # Apply LoRA (Retaining Previous Fine-Tuning Weights)\n",
    "    model = apply_lora(model, model_name, False)\n",
    "\n",
    "    # Display Trainable Parameters\n",
    "    print(model.print_trainable_parameters())\n",
    "\n",
    "    # load dataset\n",
    "    train_data = embedded_train_datasets[model_name][spt_name]\n",
    "    val_data = embedded_test_datasets[model_name][spt_name]\n",
    "\n",
    "    # Debugging: Use Smaller Dataset for Fast Testing (Uncomment if Needed)\n",
    "    # train_data = train_data.select(range(100))\n",
    "    # val_data = val_data.select(range(100))\n",
    "\n",
    "    trained_model_name = f\"Embedded_{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    # Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        **train_agrs,\n",
    "        output_dir=f\"model-variants/results/{trained_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=3e-5,\n",
    "        logging_dir=f\"model-variants/logs/{trained_model_name}\",\n",
    "        label_names=[\"labels\", \"input_ids\", \"contextual_embeds\"],\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with Contextual Embeddings\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Train the Model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save Trained Model and Tokenizer\n",
    "    save_path = f\"model-variants/models/{trained_model_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model `{model_name}` fine-tuned with contextual embeddings and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_with_contextual_embeddings(\"mBERT\", \"bpe\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mbert_bpe_embedded_trained_results = extract_extended_metrics_from_logs(\"Embedded_mBERT_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mbert_bpe_embedded_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mbert_bpe_embedded_trained_results, \"mbert_bpe_embedded_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_with_contextual_embeddings(\"mBERT\", \"unigram\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mbert_unigram_embedded_trained_results = extract_extended_metrics_from_logs(\"Embedded_mBERT_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(mbert_unigram_embedded_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mbert_unigram_embedded_trained_results, \"mbert_unigram_embedded_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-BPE\n",
    "fine_tune_with_contextual_embeddings(\"XLM-R\", \"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "xlmr_bpe_embedded_trained_results = extract_extended_metrics_from_logs(\"Embedded_XLM-R_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(xlmr_bpe_embedded_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(xlmr_bpe_embedded_trained_results, \"xlmr_bpe_embedded_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune with SPT-Unigram\n",
    "fine_tune_with_contextual_embeddings(\"XLM-R\", \"unigram\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "xlmr_unigram_embedded_trained_results = extract_extended_metrics_from_logs(\"Embedded_XLM-R_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train results\n",
    "display(xlmr_unigram_embedded_trained_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(xlmr_unigram_embedded_trained_results, \"xlmr_unigram_embedded_trained_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for embedded fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_embedded_fine_tuned_model(model_name, spt_name, device)\n",
    "\n",
    "    # Load dataset \n",
    "    tokenized_dataset = load_model_variants_hf(f\"{model_name}_{spt_name}\")\n",
    "\n",
    "    # remove comment for debug\n",
    "    #tokenized_dataset = tokenized_dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    tokenized_dataset = generate_masked_predictions_hf(tokenized_dataset, model, tokenizer, device)\n",
    "\n",
    "    # remove columns\n",
    "    tokenized_dataset = tokenized_dataset.select_columns([\"source\", \"target\", \"generated\"])\n",
    "\n",
    "    # Display results\n",
    "    display(tokenized_dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_df(tokenized_dataset, f\"model-variants/{model_name}_{spt_name}_embedded_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with BPE\n",
    "generate_predictions_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mbert with Unigram\n",
    "generate_predictions_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with BPE\n",
    "generate_predictions_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for XLM-R with Unigram\n",
    "generate_predictions_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for embedded Fine-Tuned Model using HF Dataset\n",
    "def compute_metric_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_df(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {model_name} with {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf(metrics_dataset)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {model_name} with {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {metrics_dataset['bleu'].mean()}\")\n",
    "    print(f\"ROUGE-1 Score: {metrics_dataset['rouge-1'].mean()}\")\n",
    "    print(f\"ROUGE-2 Score: {metrics_dataset['rouge-2'].mean()}\")\n",
    "    print(f\"ROUGE-L Score: {metrics_dataset['rouge-l'].mean()}\")\n",
    "    print(f\"chrF-S Score: {metrics_dataset['chrf-s'].mean()}\")\n",
    "    print(f\"BERT Score: {metrics_dataset['bert_score'].mean()}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{model_name}_{spt_name}_embedded_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with BPE\n",
    "compute_metric_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned mBERT with Unigram\n",
    "compute_metric_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with BPE\n",
    "compute_metric_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metric for fine tuned XLM-R with Unigram\n",
    "compute_metric_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute perplexity for embedded fine-tuned model using HF Dataset\n",
    "def compute_perplexity_embedded_fine_tuned_model(model_name, spt_name):\n",
    "    # Load dataset as Hugging Face Dataset\n",
    "    perplexity = load_model_variants_hf(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_embedded_fine_tuned_model(model_name, spt_name, device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Compute perplexity using HF Dataset\n",
    "    perplexity = perplexity.map(lambda example: {\n",
    "        \"perplexity\": compute_multilingual_masked_perplexity_single(example[\"generated\"], model, tokenizer, device)\n",
    "    })\n",
    "\n",
    "    # Display Perplexity Score\n",
    "    print(f\"Perplexity Score: {perplexity['perplexity'].mean()}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity, f\"{model_name}_{spt_name}_embedded_perplexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_embedded_fine_tuned_model(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_embedded_fine_tuned_model(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with BPE\n",
    "compute_perplexity_embedded_fine_tuned_model(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity with Unigram\n",
    "compute_perplexity_embedded_fine_tuned_model(\"XLM-R\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for model_name in embedding_model_names:\n",
    "    for spt_name in spt_models.keys():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "\n",
    "        evaluation_results = load_model_variants_df(f\"{model_name}_{spt_name}_embedded_predictions\")\n",
    "\n",
    "        # load metrics and set\n",
    "        metrics = load_tmp_df(f\"{model_name}_{spt_name}_metrics\")\n",
    "        evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "        evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "        evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "        evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "        evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "        evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "        # load perplexity and set\n",
    "        perplexity = load_tmp_df(f\"{model_name}_{spt_name}_perplexity\")\n",
    "        evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "        save_model_variants_df(evaluation_results, f\"{model_name}_{spt_name}_embedded_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Analysis\n",
    "Compare the performance of LSTM BPE, LSTM Unigram, mBERT, and XLM-R using BLEU, ROUGE, chrF-S, BERT Score and Perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "embedded_benchmarking_datasets = {}\n",
    "for model_name in embedding_model_names:\n",
    "    for spt_name in spt_models.keys():\n",
    "        df = load_model_variants_df(f\"{model_name}_{spt_name}_trained_evaluation_results\")\n",
    "        embedded_benchmarking_datasets[f\"{model_name} {spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "embedded_benchmarking_mean_scores = convert_to_mean_scores_df(embedded_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(embedded_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_df(embedded_benchmarking_mean_scores, \"embedded_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
