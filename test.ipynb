{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    generate_masked_predictions_hf_batch, generate_predictions_mT5_hf_batch,\n",
    "    compute_metrics_hf_batch,\n",
    "    convert_to_mean_scores_df,\n",
    "    get_fine_tuned_model, get_embedded_fine_tuned_model,\n",
    "    compute_multilingual_masked_perplexity_hf_batch, compute_multilingual_mt5_perplexity_batch,\n",
    "    extract_metrics_from_logs,\n",
    "    plot_training_metrics, plot_evaluation_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Function to Generate Predictions with Debugging\n",
    "def generate_predictions_mT5_debug(model, tokenizer, sentences, max_length=128):\n",
    "    \"\"\"\n",
    "    Generates predictions for mT5 fine-tuned with Prefix-Tuning + LoRA.\n",
    "    Includes debugging for empty outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # âœ… Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # âœ… Tokenize Input\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "\n",
    "        # âœ… Generate Output with Beam Search & Sampling\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(\n",
    "                **inputs, \n",
    "                max_length=max_length, \n",
    "                num_beams=5, \n",
    "                do_sample=True, \n",
    "                temperature=0.9\n",
    "            )\n",
    "\n",
    "        # âœ… Debugging: Print Raw Output Tokens\n",
    "        print(f\"ğŸ”¹ Input: {sentence}\")\n",
    "        print(f\"ğŸŸ¢ Generated Tokens: {output_tokens}\")\n",
    "\n",
    "        # âœ… Decode Predictions\n",
    "        prediction = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        print(f\"ğŸŸ¢ Decoded Prediction: {prediction}\\n\")\n",
    "\n",
    "        predictions.append({\"input\": sentence, \"prediction\": prediction})\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Input: most of the e-mails are from abroad.\n",
      "ğŸŸ¢ Generated Tokens: tensor([[     0,    259, 211873,  21541, 169990,   2495,  14527,  29974,  10202,\n",
      "           2495,  88859, 148554,  28771,  12868,    259,  22599,  71053,      1]],\n",
      "       device='mps:0')\n",
      "ğŸŸ¢ Decoded Prediction: á€™á€±á€¸á€á€½á€”á€ºá€¸ á€¡á€™á€»á€¬á€¸á€…á€¯á€Ÿá€¬ á€¡á€™á€±á€›á€­á€€á€”á€ºá€™á€¾á€¬ á€›á€¾á€­á€á€šá€ºá‹\n",
      "\n",
      "ğŸ”¹ Input: What is your name?\n",
      "ğŸŸ¢ Generated Tokens: tensor([[     0,    259, 102868,  37588,  40959,    259, 228293,    259, 202929,\n",
      "              1]], device='mps:0')\n",
      "ğŸŸ¢ Decoded Prediction: á€™á€„á€ºá€¸ á€˜á€šá€º á€¡á€™á€Šá€º á€œá€²á‹\n",
      "\n",
      "ğŸ”¹ Input: This is an example sentence.\n",
      "ğŸŸ¢ Generated Tokens: tensor([[     0,  19725,  10202,    259, 230882,  15217,  13288,  14893,  40959,\n",
      "            259,  71814,  21232,      1]], device='mps:0')\n",
      "ğŸŸ¢ Decoded Prediction: á€’á€«á€Ÿá€¬ á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º á€á€…á€ºá€á€¯á€•á€«á‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… Example Usage\n",
    "test_sentences = [\"most of the e-mails are from abroad.\", \"What is your name?\", \"This is an example sentence.\"]\n",
    "model, tokenizer = get_fine_tuned_model(\"mT5\", \"bpe\", \"google/mT5-small\", \"mps\")  # âœ… Load Fine-Tuned Model\n",
    "results = generate_predictions_mT5_debug(model, tokenizer, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Input: they don't have any technology.\n",
      "ğŸ”¹ Prediction: á€á€°á€á€­á€¯á€·á€Ÿá€¬ á€”á€Šá€ºá€¸á€•á€Šá€¬ á€™á€›á€¾á€­á€˜á€°á€¸á‹\n",
      "\n",
      "ğŸ”¹ Input: What is your name?\n",
      "ğŸ”¹ Prediction: á€™á€„á€ºá€¸ á€˜á€šá€ºá€œá€­á€¯ á€¡á€™á€Šá€º á€œá€²á‹\n",
      "\n",
      "ğŸ”¹ Input: This is an example sentence.\n",
      "ğŸ”¹ Prediction: á€’á€«á€Ÿá€¬ á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º á€á€…á€ºá€á€¯á€•á€«á‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#âœ… Print Predictions\n",
    "for res in results:\n",
    "    print(f\"ğŸ”¹ Input: {res['input']}\\nğŸ”¹ Prediction: {res['prediction']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Function to Generate Predictions with Prefix-Tuning + LoRA\n",
    "def generate_predictions_mT5_with_prefix(model, tokenizer, sentences, prefixes, prefix_projection, max_length=128):\n",
    "    \"\"\"\n",
    "    Generates predictions for mT5 with Prefix-Tuning + LoRA.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    predictions = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "        # âœ… Ensure Prefix is on GPU\n",
    "        num_prefixes = prefixes.num_embeddings\n",
    "        prefix_ids = torch.arange(num_prefixes, device=device)\n",
    "        expanded_prefixes = prefixes(prefix_ids).unsqueeze(0).expand(inputs[\"input_ids\"].shape[0], -1, -1)\n",
    "\n",
    "        # âœ… Project Prefix to Hidden Dim\n",
    "        projected_prefixes = prefix_projection(expanded_prefixes)\n",
    "\n",
    "        # âœ… Convert token IDs to embeddings\n",
    "        inputs_embeds = model.encoder.embed_tokens(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "        # âœ… Concatenate Prefix Embeddings with Inputs\n",
    "        inputs_embeds = torch.cat([projected_prefixes, inputs_embeds], dim=1)\n",
    "\n",
    "        # âœ… Update Attention Mask\n",
    "        new_seq_length = inputs_embeds.shape[1]\n",
    "        updated_attention_mask = torch.ones((inputs[\"attention_mask\"].shape[0], new_seq_length), device=device)\n",
    "        updated_attention_mask[:, projected_prefixes.shape[1]:] = inputs[\"attention_mask\"]\n",
    "\n",
    "        # âœ… Generate Output Using Prefix-Tuning\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=updated_attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        # âœ… Decode Predictions\n",
    "        prediction = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        predictions.append({\"input\": sentence, \"prediction\": prediction})\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Prefix Tuning Parameters (THIS IS WHERE THEY ARE CREATED)\n",
    "prefix_length = 10\n",
    "num_prefixes = model.config.num_layers * 2\n",
    "prefix_projection_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prefix embeddings:\n",
    "prefixes = torch.nn.Embedding(num_prefixes, prefix_length * prefix_projection_dim).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prefix projection layer (optional, but often recommended):\n",
    "prefix_projection = torch.nn.Sequential(\n",
    "    torch.nn.Linear(prefix_length * prefix_projection_dim, model.config.d_model),\n",
    "    torch.nn.Tanh()  # Or another activation function\n",
    ").to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"most of the e-mails are from abroad.\", \"What is your name?\", \"This is an example sentence.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = generate_predictions_mT5_with_prefix(model, tokenizer, test_sentences, prefixes, prefix_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'most of the e-mails are from abroad.',\n",
       "  'prediction': 'á€‚á€»á€¬á€™á€”á€º á á€™á€±á€¸á€á€½á€”á€ºá€¸ á€¡á€™á€»á€¬á€¸á€…á€¯á€á€Šá€º á€”á€­á€¯á€„á€ºá€„á€¶á€á€¼á€¬á€¸ á€á€­á€¯á€· á€›á€±á€¬á€€á€ºá€›á€¾á€­ á€€á€¼á€á€Šá€ºá‹'},\n",
       " {'input': 'What is your name?', 'prediction': 'á€˜á€šá€ºá€œá€­á€¯ á€¡á€“á€­á€•á€¹á€•á€«á€šá€º á€œá€² ?'},\n",
       " {'input': 'This is an example sentence.',\n",
       "  'prediction': 'á€‚á€»á€¬á€™á€”á€º á á€¡á€“á€­á€•á€¹á€•á€¬á€šá€º á€á€Šá€º áá€„á€ºá€¸á á€¡á€“á€­á€•á€¹á€•á€«á€šá€º á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€á€Šá€ºá‹'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
