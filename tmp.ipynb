{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ✅ Function to Compare Model Activations with and without Contextual Embeddings\n",
    "def compare_model_activations(loRa_model, loRa_context_model, tokenizer, sentence):\n",
    "    \"\"\"\n",
    "    Runs a sample sentence through both models:\n",
    "    1. LoRA-only model (baseline from first fine-tuning)\n",
    "    2. LoRA + Contextual Embeddings model (second fine-tuning)\n",
    "    Then, it visualizes the difference in hidden states.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Tokenize Input Sentence\n",
    "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(loRa_model.device)\n",
    "    attention_mask = tokenized_input[\"attention_mask\"].to(loRa_model.device)\n",
    "\n",
    "    # ✅ Run Model 1 (LoRA Only - First Fine-Tuning)\n",
    "    with torch.no_grad():\n",
    "        output_loRa = loRa_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "    # ✅ Generate Contextual Embeddings for Model 2\n",
    "    contextual_embeddings = torch.randn_like(output_loRa) * 0.1  # Simulating external embeddings\n",
    "    contextual_embeddings = contextual_embeddings.to(loRa_model.device)\n",
    "\n",
    "    # ✅ Run Model 2 (LoRA + Contextual Embeddings - Second Fine-Tuning)\n",
    "    with torch.no_grad():\n",
    "        output_loRa_context = loRa_context_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            contextual_embeds=contextual_embeddings  # ✅ Contextual Embeddings Added\n",
    "        ).last_hidden_state\n",
    "\n",
    "    # ✅ Compute Activation Differences\n",
    "    activation_diff = (output_loRa_context - output_loRa).abs().mean(dim=-1).cpu().numpy()\n",
    "\n",
    "    # ✅ Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(activation_diff, cmap=\"coolwarm\", annot=False)\n",
    "    plt.xlabel(\"Token Position\")\n",
    "    plt.ylabel(\"Batch Sample\")\n",
    "    plt.title(\"Activation Differences: LoRA Only vs. LoRA + Contextual Embeddings\")\n",
    "    plt.show()\n",
    "\n",
    "# ✅ Example Usage\n",
    "# model_loRa = load_fine_tuned_model(\"XLM-R\", use_contextual_embeddings=False)  # First fine-tuned model (LoRA only)\n",
    "# model_loRa_context = load_fine_tuned_model(\"XLM-R\", use_contextual_embeddings=True)  # Second fine-tuned model (LoRA + Context)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Test Sentence\n",
    "# test_sentence = \"This is an example sentence to analyze contextual embeddings.\"\n",
    "\n",
    "# Compare Model Activations\n",
    "# compare_model_activations(model_loRa, model_loRa_context, tokenizer, test_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
