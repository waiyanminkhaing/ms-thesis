{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils.dataframe import (\n",
    "    convert_to_hf,\n",
    "    load_model_variants_df,\n",
    "    save_model_variants_hf\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    ")\n",
    "from IPython.display import display\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from laserembeddings import Laser\n",
    "from utils.gpu import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enhance Burmese Contextual Representations\n",
    "- Use LASER, mUSE, and FastText for cross-lingual and morphology-aware training.\n",
    "- Fine-tune mBERT, XLM-R on Burmese dataset after adding contextual embeddings.\n",
    "- Train models again using combined embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get fine tuned tokenizer\n",
    "def get_fine_tuned_tokenizer(model_name, spt_name):\n",
    "    # Load LoRA Weights\n",
    "    lora_checkpoint_path = f\"model-variants/models/{model_name}_{spt_name.upper()}\"\n",
    "\n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_checkpoint_path)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASER Embeddings\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 09:14:38.675295: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-02-15 09:14:38.675322: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2025-02-15 09:14:38.675327: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739578478.675350 38843022 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1739578478.675374 38843022 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-15 09:14:40.801946: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# mUSE (Multilingual Universal Sentence Encoder)\n",
    "muse = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText for Morphology-Aware Training\n",
    "fasttext.util.download_model('my', if_exists='ignore')  # Download Burmese FastText model\n",
    "fasttext_model = fasttext.load_model('cc.my.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Contextual Embeddings\n",
    "@torch.no_grad()  # Prevents autograd from storing computation graphs\n",
    "def generate_contextual_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Generates contextual embeddings using LASER, mUSE, and FastText.\n",
    "    The output embeddings will be used to enhance XLM-R and mBERT fine-tuning.\n",
    "    \"\"\"\n",
    "    # Compute Multilingual Embeddings\n",
    "    embeddings_laser = laser.embed_sentences(sentences, lang=\"my\")  # (batch_size, hidden_size)\n",
    "    embeddings_muse = muse(sentences).numpy()  # (batch_size, hidden_size)\n",
    "    embeddings_fasttext = np.array([fasttext_model.get_sentence_vector(sentence) for sentence in sentences])  # (batch_size, hidden_size)\n",
    "\n",
    "    # Compute Mean-Pooled Contextual Embedding\n",
    "    combined_embedding = (embeddings_laser + embeddings_muse + embeddings_fasttext) / 3\n",
    "\n",
    "    # Convert to Torch Tensor & Move to GPU with bf16 Support\n",
    "    return torch.tensor(combined_embedding, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Tokenize and Add Contextual Embeddings\n",
    "def tokenize_with_contextual_embeddings(examples, tokenizer):\n",
    "    sentences = examples[\"source\"]\n",
    "    \n",
    "    # Generate Contextual Embeddings\n",
    "    contextual_embeddings = generate_contextual_embeddings(sentences)\n",
    "\n",
    "    # Tokenize Input Text\n",
    "    tokenized_input = tokenizer(sentences, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    tokenized_output = tokenizer(examples[\"target\"], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Ensure Embeddings Match Expected Shape: (batch_size, sequence_length, hidden_size)\n",
    "    batch_size = tokenized_input[\"input_ids\"].shape[0]\n",
    "    hidden_size = contextual_embeddings.shape[-1]\n",
    "\n",
    "    # Expand contextual embeddings for entire sequence (assuming CLS-based representation)\n",
    "    contextual_embeddings = contextual_embeddings.unsqueeze(1).expand(\n",
    "        batch_size, \n",
    "        tokenized_input[\"input_ids\"].shape[1], \n",
    "        hidden_size\n",
    "    ).to(tokenized_input[\"input_ids\"].device)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_input[\"attention_mask\"],\n",
    "        \"labels\": tokenized_output[\"input_ids\"],\n",
    "        \"contextual_embeds\": contextual_embeddings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Prepare Dataset with Contextual Embeddings\n",
    "def prepare_contextual_embedding_dataset(model_name, spt_name):\n",
    "    # get dataset\n",
    "    dataset = load_model_variants_df(\"combined\")\n",
    "\n",
    "    # Split into 80% train, 20% test\n",
    "    train_df = dataset.sample(frac=0.8, random_state=42)\n",
    "    test_df = dataset.drop(train_df.index)\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    train_df = convert_to_hf(train_df)\n",
    "    test_df = convert_to_hf(test_df)\n",
    "    \n",
    "    # get tokenizer\n",
    "    tokenizer = get_fine_tuned_tokenizer(model_name, spt_name)\n",
    "\n",
    "    # Apply Tokenization with Contextual Embeddings to train dataset\n",
    "    train_tokenized = train_df.map(\n",
    "        lambda x: tokenize_with_contextual_embeddings(x, tokenizer),\n",
    "        batched=True,\n",
    "        desc=f\"Tokenizing train dataset for {model_name} with {spt_name} (Contextual Embeddings)\",\n",
    "        num_proc=10\n",
    "    )\n",
    "\n",
    "    # Save Processed train Dataset\n",
    "    save_model_variants_hf(train_tokenized, f\"contextual_embedded_{model_name.lower()}_{spt_name}_train\")\n",
    "\n",
    "    # Apply Tokenization with Contextual Embeddings to test dataset\n",
    "    test_tokenized = test_df.map(\n",
    "        lambda x: tokenize_with_contextual_embeddings(x, tokenizer),\n",
    "        batched=True,\n",
    "        desc=f\"Tokenizing train dataset for {model_name} with {spt_name} (Contextual Embeddings)\",\n",
    "        num_proc=10\n",
    "    )\n",
    "\n",
    "    # Save Processed test Dataset\n",
    "    save_model_variants_hf(test_tokenized, f\"contextual_embedded_{model_name.lower()}_{spt_name}_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding with bpe\n",
    "prepare_contextual_embedding_dataset(\"mBERT\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding with unigram\n",
    "prepare_contextual_embedding_dataset(\"mBERT\", \"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding with bpe\n",
    "prepare_contextual_embedding_dataset(\"XLM-R\", \"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding with unigram\n",
    "prepare_contextual_embedding_dataset(\"XLM-R\", \"unigram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis-tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
