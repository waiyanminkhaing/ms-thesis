{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.dataframe import (\n",
    "    save_model_variants_chunk_hf,\n",
    "    load_model_variants_hf, save_model_variants_hf\n",
    ")\n",
    "from IPython.display import display\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from laserembeddings import Laser\n",
    "from utils.gpu import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enhance Burmese Contextual Representations\n",
    "- Use LASER, mUSE, and FastText for cross-lingual and morphology-aware training.\n",
    "- Fine-tune mBERT, XLM-R on Burmese dataset after adding contextual embeddings.\n",
    "- Train models again using combined embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splti train data\n",
    "# for model_name in [\"mbert\", \"xlm-r\"]:\n",
    "#     for spt_name in [\"bpe\", \"unigram\"]:\n",
    "#         name = f\"{model_name}_{spt_name}_train\"\n",
    "#         train_dataset = load_model_variants_hf(name)\n",
    "\n",
    "#         # split and save\n",
    "#         save_model_variants_hf(train_dataset, f\"embedded_{name}\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splti test data\n",
    "# for model_name in [\"mbert\", \"xlm-r\"]:\n",
    "#     for spt_name in [\"bpe\", \"unigram\"]:\n",
    "#         name = f\"{model_name}_{spt_name}_test\"\n",
    "#         train_dataset = load_model_variants_hf(name)\n",
    "\n",
    "#         # split and save\n",
    "#         save_model_variants_hf(train_dataset, f\"embedded_{name}\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASER Embeddings\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:32:51.739562: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-02-24 21:32:51.739604: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2025-02-24 21:32:51.739611: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740400371.739632 12534025 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1740400371.739654 12534025 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-24 21:32:53.925074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# mUSE (Multilingual Universal Sentence Encoder)\n",
    "muse = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText for Morphology-Aware Training\n",
    "fasttext.util.download_model('my', if_exists='ignore')  # Download Burmese FastText model\n",
    "fasttext_model = fasttext.load_model('cc.my.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize projection layer (1836 → 768) in bf16\n",
    "projection_layer = nn.Linear(1836, 768).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(batch):\n",
    "    \"\"\"\n",
    "    Generates contextual embeddings using mUSE (TensorFlow), LASER, and FastText.\n",
    "    Uses Hugging Face Datasets to process batches efficiently.\n",
    "    Logs errors and stops execution on failure.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = batch[\"target\"]\n",
    "    sentences = [str(text) if text is not None else \"\" for text in sentences]\n",
    "\n",
    "    # Ensure all sentences are valid strings\n",
    "    sentences = [s.replace(\"\\n\", \" \").strip() if s else \"\" for s in sentences]\n",
    "\n",
    "    # Generate mUSE embeddings (GPU-accelerated)\n",
    "    batch_tensor = tf.convert_to_tensor(sentences)\n",
    "    embeddings_muse = muse(batch_tensor).numpy()  # Shape: (batch_size, 512)\n",
    "\n",
    "    # Generate FastText embeddings\n",
    "    embeddings_fasttext = np.array([\n",
    "        fasttext_model.get_sentence_vector(sentence) for sentence in sentences\n",
    "    ])\n",
    "\n",
    "    # Generate LASER embeddings\n",
    "    embeddings_laser = laser.embed_sentences(sentences, lang=\"my\")\n",
    "\n",
    "    # Stack embeddings\n",
    "    combined_embeddings = np.hstack([\n",
    "        embeddings_muse,\n",
    "        embeddings_laser,\n",
    "        embeddings_fasttext\n",
    "    ])\n",
    "\n",
    "    # Convert to tensor & bf16\n",
    "    tensor_embeddings = torch.tensor(combined_embeddings, dtype=torch.bfloat16)\n",
    "\n",
    "    # Apply projection (1836 → 768)\n",
    "    projected_embeddings = projection_layer(tensor_embeddings).detach().tolist()\n",
    "\n",
    "    return {\"contextual_embeddings\": projected_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and spt\n",
    "model_name = \"xlm-r\"\n",
    "spt_name = \"bpe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk num\n",
    "chunk_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train path\n",
    "chunk_dataset_path = f\"embedded_{model_name}_{spt_name}_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-variants/data/embedded_xlm-r_bpe_test_hf_dataset/chunk_4\n",
      "Chunk Dataset Length: 65103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>some of them are only part of the basic buildi...</td>\n",
       "      <td>တချို့ကို ဖယ်ရှားထားလို့ အခြေခံ အဆောက်အအုံ အပိ...</td>\n",
       "      <td>[0, 3060, 111, 2856, 621, 4734, 2831, 111, 70,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 14725, 191256, 149272, 43871, 39208, 10882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well, i don't think it would be good if we los...</td>\n",
       "      <td>ကောင်းပြီ၊ ကျွန်တော်တို့ ဒါကို လုံးဝ ဆုံးရှုံး...</td>\n",
       "      <td>[0, 5299, 6, 4, 17, 2301, 242, 808, 5351, 442,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 6, 44373, 64043, 1586, 6, 86960, 20409, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was born in new mexico in 1995.</td>\n",
       "      <td>၁၉၉၅ မှာ New Mexico မှာမွေးတယ်။</td>\n",
       "      <td>[0, 17, 509, 103122, 23, 3525, 163, 73140, 23,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 54586, 29722, 21963, 81987, 2356, 276, 111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cynthia was a pharmacist.</td>\n",
       "      <td>Cynthia သည် ဆေးဆိုင်ဖြစ်သည်။</td>\n",
       "      <td>[0, 18986, 4450, 11, 509, 10, 189258, 39, 1030...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 49715, 4450, 11, 6, 8885, 6, 62520, 6, 743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's not good to forget something because the ...</td>\n",
       "      <td>စူးစမ်းရေးမှူးဟာ သေးငယ်တာကြောင့် တစ်ခုခုကို လျ...</td>\n",
       "      <td>[0, 442, 242, 91, 959, 4127, 47, 90820, 9844, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 16528, 36371, 196008, 6, 9177, 155551, 887...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  some of them are only part of the basic buildi...   \n",
       "1  well, i don't think it would be good if we los...   \n",
       "2                  i was born in new mexico in 1995.   \n",
       "3                          cynthia was a pharmacist.   \n",
       "4  it's not good to forget something because the ...   \n",
       "\n",
       "                                              target  \\\n",
       "0  တချို့ကို ဖယ်ရှားထားလို့ အခြေခံ အဆောက်အအုံ အပိ...   \n",
       "1  ကောင်းပြီ၊ ကျွန်တော်တို့ ဒါကို လုံးဝ ဆုံးရှုံး...   \n",
       "2                    ၁၉၉၅ မှာ New Mexico မှာမွေးတယ်။   \n",
       "3                       Cynthia သည် ဆေးဆိုင်ဖြစ်သည်။   \n",
       "4  စူးစမ်းရေးမှူးဟာ သေးငယ်တာကြောင့် တစ်ခုခုကို လျ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [0, 3060, 111, 2856, 621, 4734, 2831, 111, 70,...   \n",
       "1  [0, 5299, 6, 4, 17, 2301, 242, 808, 5351, 442,...   \n",
       "2  [0, 17, 509, 103122, 23, 3525, 163, 73140, 23,...   \n",
       "3  [0, 18986, 4450, 11, 509, 10, 189258, 39, 1030...   \n",
       "4  [0, 442, 242, 91, 959, 4127, 47, 90820, 9844, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 14725, 191256, 149272, 43871, 39208, 10882...  \n",
       "1  [0, 6, 44373, 64043, 1586, 6, 86960, 20409, 25...  \n",
       "2  [0, 54586, 29722, 21963, 81987, 2356, 276, 111...  \n",
       "3  [0, 49715, 4450, 11, 6, 8885, 6, 62520, 6, 743...  \n",
       "4  [0, 16528, 36371, 196008, 6, 9177, 155551, 887...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load chunk dataset\n",
    "chunk_dataset = load_model_variants_hf(chunk_dataset_path, chunk_num=chunk_num)\n",
    "print(f\"Chunk Dataset Length: {len(chunk_dataset)}\")\n",
    "display(chunk_dataset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_embeddings at 0x36fc1a4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function generate_embeddings at 0x36fc1a4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5578a0a147e4020ab677528c2677bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate embeddings\n",
    "chunk_dataset = chunk_dataset.map(generate_embeddings, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(chunk_dataset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunk dataset\n",
    "save_model_variants_chunk_hf(chunk_dataset, chunk_dataset_path, chunk_num=chunk_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_generated_list = []\n",
    "for i in range(5):\n",
    "    dataset = load_model_variants_hf(chunk_dataset_path, chunk_num=i)\n",
    "\n",
    "    if \"contextual_embeddings\" not in dataset.to_pandas().columns:\n",
    "        not_generated_list.append(i)\n",
    "\n",
    "print(not_generated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis-tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
