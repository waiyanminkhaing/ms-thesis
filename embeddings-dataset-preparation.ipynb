{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.dataframe import (\n",
    "    save_model_variants_chunk_hf,\n",
    "    load_model_variants_hf, save_model_variants_hf\n",
    ")\n",
    "from IPython.display import display\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from laserembeddings import Laser\n",
    "from utils.gpu import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Enhance Burmese Contextual Representations\n",
    "- Use LASER, mUSE, and FastText for cross-lingual and morphology-aware training.\n",
    "- Fine-tune mBERT, XLM-R on Burmese dataset after adding contextual embeddings.\n",
    "- Train models again using combined embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk size\n",
    "embedded_train_chunk_sizes = {\n",
    "    \"mbert\": 20,\n",
    "    \"xlm-r\": 20,\n",
    "}\n",
    "embedded_test_chunk_sizes = {\n",
    "    \"mbert\": 5,\n",
    "    \"xlm-r\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splti train data\n",
    "# for model_name in [\"mbert\", \"xlm-r\"]:\n",
    "#     for spt_name in [\"bpe\", \"unigram\"]:\n",
    "#         name = f\"{model_name}_{spt_name}_train\"\n",
    "#         train_dataset = load_model_variants_hf(name)\n",
    "\n",
    "#         # split and save\n",
    "#         save_model_variants_hf(train_dataset, f\"embedded_{name}\", embedded_train_chunk_sizes[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # splti test data\n",
    "# for model_name in [\"mbert\", \"xlm-r\"]:\n",
    "#     for spt_name in [\"bpe\", \"unigram\"]:\n",
    "#         name = f\"{model_name}_{spt_name}_test\"\n",
    "#         train_dataset = load_model_variants_hf(name)\n",
    "\n",
    "#         # split and save\n",
    "#         save_model_variants_hf(train_dataset, f\"embedded_{name}\", embedded_test_chunk_sizes[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASER Embeddings\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 13:55:31.867673: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-02-25 13:55:31.867701: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 48.00 GB\n",
      "2025-02-25 13:55:31.867705: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 18.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740459331.867721 13994675 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1740459331.867741 13994675 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-25 13:55:34.024274: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# mUSE (Multilingual Universal Sentence Encoder)\n",
    "muse = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText for Morphology-Aware Training\n",
    "fasttext.util.download_model('my', if_exists='ignore')  # Download Burmese FastText model\n",
    "fasttext_model = fasttext.load_model('cc.my.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize projection layer (1836 → 768) in bf16\n",
    "projection_layer = nn.Linear(1836, 768).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(batch):\n",
    "    \"\"\"\n",
    "    Generates contextual embeddings using mUSE (TensorFlow), LASER, and FastText.\n",
    "    Uses Hugging Face Datasets to process batches efficiently.\n",
    "    Logs errors and stops execution on failure.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = batch[\"target\"]\n",
    "    sentences = [str(text) if text is not None else \"\" for text in sentences]\n",
    "\n",
    "    # Ensure all sentences are valid strings\n",
    "    sentences = [s.replace(\"\\n\", \" \").strip() if s else \"\" for s in sentences]\n",
    "\n",
    "    # Generate mUSE embeddings (GPU-accelerated)\n",
    "    batch_tensor = tf.convert_to_tensor(sentences)\n",
    "    embeddings_muse = muse(batch_tensor).numpy()  # Shape: (batch_size, 512)\n",
    "\n",
    "    # Generate FastText embeddings\n",
    "    embeddings_fasttext = np.array([\n",
    "        fasttext_model.get_sentence_vector(sentence) for sentence in sentences\n",
    "    ])\n",
    "\n",
    "    # Generate LASER embeddings\n",
    "    embeddings_laser = laser.embed_sentences(sentences, lang=\"my\")\n",
    "\n",
    "    # Stack embeddings\n",
    "    combined_embeddings = np.hstack([\n",
    "        embeddings_muse,\n",
    "        embeddings_laser,\n",
    "        embeddings_fasttext\n",
    "    ])\n",
    "\n",
    "    # Convert to tensor & bf16\n",
    "    tensor_embeddings = torch.tensor(combined_embeddings, dtype=torch.bfloat16)\n",
    "\n",
    "    # Apply projection (1836 → 768)\n",
    "    projected_embeddings = projection_layer(tensor_embeddings).detach().tolist()\n",
    "\n",
    "    return {\"contextual_embeddings\": projected_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and spt\n",
    "model_name = \"xlm-r\"\n",
    "spt_name = \"unigram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk num\n",
    "chunk_num = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train path\n",
    "chunk_dataset_path = f\"embedded_{model_name}_{spt_name}_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-variants/data/embedded_xlm-r_unigram_train_hf_dataset/chunk_17\n",
      "Chunk Dataset Length: 65103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in a few days the light will be on a holy boat.</td>\n",
       "      <td>တစ်ချို့ရက်တွေမှာ လရောင်ဟာ သန့်ရှင်းတဲ့ လှေပေါ...</td>\n",
       "      <td>[0, 23, 10, 10846, 13312, 70, 22729, 1221, 186...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 53069, 191256, 3246, 97156, 6, 203333, 325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the hierbas is consumed before and after meals.</td>\n",
       "      <td>Hierbas ကို အစာမစားမီနှင့် စားပြီးနောက် စားသုံ...</td>\n",
       "      <td>[0, 70, 1274, 72, 961, 91, 83, 18588, 13, 104,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 2673, 72, 961, 91, 6134, 1160, 10946, 3684...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she was usually busy, so she didn't entertain ...</td>\n",
       "      <td>သူမဟာ အမြဲလိုလို အလုပ်ရှုပ်နေတော့ သူတို့ကို လု...</td>\n",
       "      <td>[0, 2412, 509, 56104, 86352, 6, 4, 221, 2412, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 143404, 11236, 6, 237211, 9509, 9509, 1145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lsc has done a round of technology initiative ...</td>\n",
       "      <td>LSC သည် ယခင်က Technology Initiative Grants မျာ...</td>\n",
       "      <td>[0, 96, 10382, 1556, 16940, 10, 68807, 111, 55...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 339, 14495, 6, 8885, 54277, 65113, 1489, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the kibbutz hotel in ein gedi's time was a fan...</td>\n",
       "      <td>Ein Gedi ရဲ့ ခေတ်မီ Kibbutz ဟိုတယ်ဟာ အံ့ဩစရာကေ...</td>\n",
       "      <td>[0, 70, 472, 6, 2566, 1284, 97, 3018, 23, 28, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 241, 23, 2206, 45, 100556, 6, 209098, 4530...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0    in a few days the light will be on a holy boat.   \n",
       "1    the hierbas is consumed before and after meals.   \n",
       "2  she was usually busy, so she didn't entertain ...   \n",
       "3  lsc has done a round of technology initiative ...   \n",
       "4  the kibbutz hotel in ein gedi's time was a fan...   \n",
       "\n",
       "                                              target  \\\n",
       "0  တစ်ချို့ရက်တွေမှာ လရောင်ဟာ သန့်ရှင်းတဲ့ လှေပေါ...   \n",
       "1  Hierbas ကို အစာမစားမီနှင့် စားပြီးနောက် စားသုံ...   \n",
       "2  သူမဟာ အမြဲလိုလို အလုပ်ရှုပ်နေတော့ သူတို့ကို လု...   \n",
       "3  LSC သည် ယခင်က Technology Initiative Grants မျာ...   \n",
       "4  Ein Gedi ရဲ့ ခေတ်မီ Kibbutz ဟိုတယ်ဟာ အံ့ဩစရာကေ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [0, 23, 10, 10846, 13312, 70, 22729, 1221, 186...   \n",
       "1  [0, 70, 1274, 72, 961, 91, 83, 18588, 13, 104,...   \n",
       "2  [0, 2412, 509, 56104, 86352, 6, 4, 221, 2412, ...   \n",
       "3  [0, 96, 10382, 1556, 16940, 10, 68807, 111, 55...   \n",
       "4  [0, 70, 472, 6, 2566, 1284, 97, 3018, 23, 28, ...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 53069, 191256, 3246, 97156, 6, 203333, 325...  \n",
       "1  [0, 2673, 72, 961, 91, 6134, 1160, 10946, 3684...  \n",
       "2  [0, 143404, 11236, 6, 237211, 9509, 9509, 1145...  \n",
       "3  [0, 339, 14495, 6, 8885, 54277, 65113, 1489, 4...  \n",
       "4  [0, 241, 23, 2206, 45, 100556, 6, 209098, 4530...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load chunk dataset\n",
    "chunk_dataset = load_model_variants_hf(chunk_dataset_path, chunk_num=chunk_num)\n",
    "print(f\"Chunk Dataset Length: {len(chunk_dataset)}\")\n",
    "display(chunk_dataset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_embeddings at 0x366c1e0e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function generate_embeddings at 0x366c1e0e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a88a2b806a24e7a9028df717f7bf0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate embeddings\n",
    "chunk_dataset = chunk_dataset.map(generate_embeddings, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "display(chunk_dataset.to_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunk dataset\n",
    "save_model_variants_chunk_hf(chunk_dataset, chunk_dataset_path, chunk_num=chunk_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_generated_list = []\n",
    "# for i in range(5):\n",
    "#     dataset = load_model_variants_hf(chunk_dataset_path, chunk_num=i)\n",
    "\n",
    "#     if \"contextual_embeddings\" not in dataset.to_pandas().columns:\n",
    "#         not_generated_list.append(i)\n",
    "\n",
    "# print(not_generated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis-tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
