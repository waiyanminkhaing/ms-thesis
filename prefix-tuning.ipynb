{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ba45c8",
   "metadata": {},
   "source": [
    "# Importing all the important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65efe3b-27ae-42cf-a711-56c6e273ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.45.2 sentence-transformers==3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff0754-7db6-4837-aab3-f95be80017c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 05:41:58.067615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-19 05:41:58.091754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-19 05:41:58.091785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 05:41:58.107221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-19 05:41:58.941033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Transformers is only compatible with Keras 2, but you have explicitly set `TF_USE_LEGACY_KERAS` to `0`. This may result in unexpected behaviour or errors if Keras 3 objects are passed to Transformers models.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, EarlyStoppingCallback, Trainer\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    get_prefix, apply_lora, \n",
    "    TRAIN_ARGS, \n",
    "    extract_metrics_from_logs,\n",
    "    plot_training_metrics,\n",
    "    plot_evaluation_metrics,\n",
    "    get_prefix_fine_tuned_model,\n",
    "    generate_mt5_predictions_hf_batch\n",
    ")\n",
    "from utils.dataframe import (\n",
    "    load_model_variants_hf,\n",
    "    save_model_variants_gen_df,\n",
    "    load_models_df,\n",
    "    convert_to_hf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcac4d8",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9af449-ff25-4bca-abf4-a7c5f2b4d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using PyTorch device: cuda\n",
      "GPU Name: NVIDIA A10G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 05:42:00.879213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-19 05:42:00.917558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-02-19 05:42:00.919528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "mT5_model_name = \"google/mT5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spt models\n",
    "spt_models = [\"bpe\", \"unigram\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0d677",
   "metadata": {},
   "source": [
    "# 2. Prefix Tuning Transformer Models for Burmese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edf393",
   "metadata": {},
   "source": [
    "## Prefix Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df919903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized train dataset\n",
    "tokenized_train_datasets = {\n",
    "    spt_name: load_model_variants_hf(f\"mt5_{spt_name}_train\")\n",
    "    for spt_name in spt_models\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356abc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized test dataset\n",
    "tokenized_test_datasets = {\n",
    "    spt_name: load_model_variants_hf(f\"mt5_{spt_name}_test\")\n",
    "    for spt_name in spt_models\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99375489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom class for Prefix Tuning\n",
    "class PrefixTuningTrainer(Trainer):\n",
    "    def __init__(self, *args, prefixes, prefix_projection, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        device = self.model.device\n",
    "        self.prefixes = prefixes.to(device)  # Ensure prefix embeddings are on GPU\n",
    "        self.prefix_projection = prefix_projection.to(device)  # Ensure prefix projection is on GPU\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Compute loss for prefix tuning.\n",
    "        \"\"\"\n",
    "        device = model.device  # Ensure all tensors are on the same device\n",
    "\n",
    "        # Move inputs to GPU\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        decoder_input_ids = inputs[\"labels\"].to(device)\n",
    "\n",
    "        # Fix padding issue (-100 should be replaced with decoder_start_token_id)\n",
    "        decoder_input_ids = decoder_input_ids.masked_fill(\n",
    "            decoder_input_ids == -100, model.config.decoder_start_token_id\n",
    "        )\n",
    "\n",
    "        # Ensure prefix tensor is on GPU\n",
    "        num_prefixes = self.prefixes.num_embeddings\n",
    "        prefix_ids = torch.arange(num_prefixes, device=device)\n",
    "        expanded_prefixes = self.prefixes(prefix_ids).unsqueeze(0).expand(input_ids.shape[0], -1, -1)\n",
    "\n",
    "        # Project prefix embeddings to model hidden dimension\n",
    "        projected_prefixes = self.prefix_projection(expanded_prefixes)\n",
    "        assert projected_prefixes.shape[-1] == model.config.d_model, \"Prefix projection mismatch!\"\n",
    "\n",
    "        # Convert token IDs to embeddings\n",
    "        inputs_embeds = model.encoder.embed_tokens(input_ids).to(device)\n",
    "        decoder_inputs_embeds = model.decoder.embed_tokens(decoder_input_ids).to(device)\n",
    "\n",
    "        # Concatenate prefix embeddings with inputs\n",
    "        inputs_embeds = torch.cat([projected_prefixes, inputs_embeds], dim=1)\n",
    "\n",
    "        # Update attention mask\n",
    "        new_seq_length = inputs_embeds.shape[1]\n",
    "        updated_attention_mask = torch.ones((attention_mask.shape[0], new_seq_length), device=device)\n",
    "        updated_attention_mask[:, projected_prefixes.shape[1]:] = attention_mask\n",
    "\n",
    "        # Forward pass with correct decoder embeddings\n",
    "        outputs = model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=updated_attention_mask,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,  # Use embeddings instead of decoder_input_ids\n",
    "            labels=decoder_input_ids  # Ensure loss is computed\n",
    "        )\n",
    "\n",
    "        # Extract loss\n",
    "        loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[\"loss\"]\n",
    "\n",
    "        if loss is None:\n",
    "            raise ValueError(\"Model did not return a loss. Ensure labels are provided.\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model_with_prefix_tuning(spt_name, batch_size):\n",
    "    \"\"\"\n",
    "    Fine-tunes the model with LoRA on the specified SentencePiece tokenization (SPT).\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning mT5 using SPT-{spt_name.upper()} with prefix tuning...\")\n",
    "\n",
    "    # Load tokenizer & model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mT5_model_name, use_fast=False, legacy=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(mT5_model_name)\n",
    "\n",
    "    # Move model to GPU before applying LoRA\n",
    "    model.to(device)\n",
    "\n",
    "    # get prefix\n",
    "    prefixes, prefix_projection = get_prefix(model, device)\n",
    "\n",
    "    # Apply LoRA for efficient parameter tuning\n",
    "    model = apply_lora(model, mT5_model_name, device)\n",
    "\n",
    "    # display trainable parameters\n",
    "    print(model.print_trainable_parameters())\n",
    "    \n",
    "    # load dataset\n",
    "    train_data = tokenized_train_datasets[spt_name]\n",
    "    val_data = tokenized_test_datasets[spt_name]\n",
    "\n",
    "    # for debug, remove comment\n",
    "    #train_data = train_data.select(range(100))\n",
    "    #val_data = val_data.select(range(100))\n",
    "\n",
    "    trained_model_name = f\"Prefix_mT5_{spt_name.upper()}\"\n",
    "\n",
    "    # Define TrainingArguments\n",
    "    training_args = TrainingArguments(\n",
    "        **TRAIN_ARGS,\n",
    "        output_dir=f\"model-variants/results/{trained_model_name}\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        logging_dir=f\"model-variants/logs/{trained_model_name}\",\n",
    "        label_names=[\"labels\", \"input_ids\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize PrefixTuningTrainer\n",
    "    trainer = PrefixTuningTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        prefixes=prefixes,\n",
    "        prefix_projection=prefix_projection\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model and tokenizer\n",
    "    save_path = f\"model-variants/models/{trained_model_name}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model `mT5` fine-tuned and saved at `{save_path}`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe\n",
    "fine_tune_model_with_prefix_tuning(\"bpe\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9972f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram\n",
    "fine_tune_model_with_prefix_tuning(\"bpe\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1d45f",
   "metadata": {},
   "source": [
    "## Train Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fad8c5",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mt5_bpe_prefix_fined_tuned_train_results, mt5_bpe_prefix_fined_tuned_eval_results, mt5_bpe_prefix_fine_tuned_final_results = extract_metrics_from_logs(\"Prefix_mT5_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55efba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train results\n",
    "plot_training_metrics(mt5_bpe_prefix_fined_tuned_train_results, \"Prefix mT5 BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot eval results\n",
    "plot_evaluation_metrics(mt5_bpe_prefix_fined_tuned_eval_results, \"Prefix mT5 BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c374a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display final result\n",
    "display(mt5_bpe_prefix_fine_tuned_final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mt5_bpe_prefix_fined_tuned_train_results, \"mt5_bpe_prefix_fined_tuned_train_results\")\n",
    "save_model_variants_gen_df(mt5_bpe_prefix_fined_tuned_eval_results, \"mt5_bpe_prefix_fined_tuned_eval_results\")\n",
    "save_model_variants_gen_df(mt5_bpe_prefix_fine_tuned_final_results, \"mt5_bpe_prefix_fine_tuned_final_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1713a8f",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb09792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train results\n",
    "mt5_unigram_prefix_fined_tuned_train_results, mt5_unigram_prefix_fined_tuned_eval_results, mt5_unigram_prefix_fine_tuned_final_results = extract_metrics_from_logs(\"Prefix_mT5_UNIGRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e19e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train results\n",
    "plot_training_metrics(mt5_unigram_prefix_fined_tuned_train_results, \"Prefix mT5 Unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot eval results\n",
    "plot_evaluation_metrics(mt5_unigram_prefix_fined_tuned_eval_results, \"Prefix mT5 Unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ed6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display final result\n",
    "display(mt5_unigram_prefix_fine_tuned_final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train results\n",
    "save_model_variants_gen_df(mt5_unigram_prefix_fined_tuned_train_results, \"mt5_unigram_prefix_fined_tuned_train_results\")\n",
    "save_model_variants_gen_df(mt5_unigram_prefix_fined_tuned_eval_results, \"mt5_unigram_prefix_fined_tuned_eval_results\")\n",
    "save_model_variants_gen_df(mt5_unigram_prefix_fine_tuned_final_results, \"mt5_unigram_prefix_fine_tuned_final_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f5ecc",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad70b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions for fine-tuned model using Hugging Face Dataset\n",
    "def generate_predictions_prefix_fine_tuned_model(spt_name):\n",
    "    # Load tokenizers & models\n",
    "    model, tokenizer = get_prefix_fine_tuned_model(\"mT5\", spt_name, mT5_model_name, device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load dataset \n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # remove comment for debug\n",
    "    # dataset = dataset.select(range(10))\n",
    "\n",
    "    # Run text generation\n",
    "    dataset = generate_mt5_predictions_hf_batch(dataset, model, tokenizer, device)\n",
    "\n",
    "    # Display results\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_gen_df(dataset, f\"mt5_{spt_name}_trained_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74604336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with BPE\n",
    "generate_predictions_prefix_fine_tuned_model(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for mT5 with Unigram\n",
    "generate_predictions_prefix_fine_tuned_model(\"unigram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
