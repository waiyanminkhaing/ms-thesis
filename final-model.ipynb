{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils.dataframe import (\n",
    "    save_tmp_df, load_tmp_df, load_models_df,\n",
    "    save_model_variants_gen_df, load_model_variants_gen_df,\n",
    "    convert_to_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    compute_metrics_hf_batch,\n",
    "    convert_to_mean_scores_df,\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, MT5ForConditionalGeneration\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 model path\n",
    "model_names = {\n",
    "    \"bpe\": \"model-variants/models/mT5_BPE\",\n",
    "    \"unigram\": \"model-variants/models/mT5_UNIGRAM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model_with_contextual_embeddings(spt_name):\n",
    "    # Load tokenizers & models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[spt_name], use_fast=False, legacy=True)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "    model = PeftModel.from_pretrained(model, model_names[spt_name]).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load Contextual Embeddings\n",
    "    contextual_embeddings = torch.load(f\"model-variants/gen/{spt_name}_projected_contextual_embeddings.pt\").to(device)\n",
    "\n",
    "    return model, tokenizer, contextual_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(spt_name, batch_size=128, max_length=512):\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # load data\n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # dataset = dataset.select(range(100))\n",
    "\n",
    "    def predict_fn(batch):\n",
    "        batch_size = len(batch[\"burmese\"])\n",
    "\n",
    "        # Tokenize input texts\n",
    "        inputs = tokenizer(\n",
    "            batch[\"burmese\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]  # Get sequence length\n",
    "\n",
    "        # Fix Contextual Embeddings Shape\n",
    "        contextual_embeds = contextual_embeddings[:batch_size]  # Ensure batch size matches\n",
    "        if contextual_embeds.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            contextual_embeds = contextual_embeds.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Ensure correct device\n",
    "        contextual_embeds = contextual_embeds.to(device)\n",
    "\n",
    "        # Convert tokenized inputs to embeddings\n",
    "        input_embeds = model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "\n",
    "        # Inject contextual embeddings by **adding** them to token embeddings\n",
    "        final_embeds = input_embeds + contextual_embeds\n",
    "\n",
    "        # Generate text using **concatenated embeddings**\n",
    "        output_tokens = model.generate(\n",
    "            inputs_embeds=final_embeds,  # Inject contextual embeddings\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=2,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.5,  # Avoids excessive repetition\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        # Decode predictions\n",
    "        generated_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return {\"generated\": generated_texts}\n",
    "\n",
    "\n",
    "    dataset = dataset.map(predict_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    save_model_variants_gen_df(dataset, f\"{spt_name}_final_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4633784eda406785fc3538ce5e796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "      <td>အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင် ပွဲကို ကြည့်ဖို့ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "      <td>ဝေးလံခေါင် မြို့ကို လည်ပတ် နေတဲ့ ရွာတွေ ရှိတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုး တွေကို လုပ်ပေး တာ အရမ်းကေ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...</td>\n",
       "      <td>ဇင်ဘာဘွေ သည် ဇင်ဘာဘွေ ၏ အကြိမ်အကြိမ် များကို ပ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...</td>\n",
       "      <td>အခွန်ထမ်း များသည် ၎င်းတို့၏ လုပ်ငန်းတာဝန် များ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...   \n",
       "1  စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...   \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...   \n",
       "3  ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...   \n",
       "4  အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...   \n",
       "\n",
       "                                           generated  \n",
       "0  အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင် ပွဲကို ကြည့်ဖို့ ...  \n",
       "1    ဝေးလံခေါင် မြို့ကို လည်ပတ် နေတဲ့ ရွာတွေ ရှိတယ်။  \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုး တွေကို လုပ်ပေး တာ အရမ်းကေ...  \n",
       "3  ဇင်ဘာဘွေ သည် ဇင်ဘာဘွေ ၏ အကြိမ်အကြိမ် များကို ပ...  \n",
       "4  အခွန်ထမ်း များသည် ၎င်းတို့၏ လုပ်ငန်းတာဝန် များ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266823fd70464e8586c053e4b3e35adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with bpe\n",
    "generate_predictions(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "generate_predictions(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric(spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    metrics_dataset = convert_to_hf(metrics_dataset)\n",
    "\n",
    "    # if debug, remove comment\n",
    "    #metrics_dataset = metrics_dataset.select(range(100))  # Keep this for debugging\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf_batch(metrics_dataset, device)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {np.mean(metrics_dataset['bleu'])}\")\n",
    "    print(f\"ROUGE-1 Score: {np.mean(metrics_dataset['rouge-1'])}\")\n",
    "    print(f\"ROUGE-2 Score: {np.mean(metrics_dataset['rouge-2'])}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(metrics_dataset['rouge-l'])}\")\n",
    "    print(f\"chrF-S Score: {np.mean(metrics_dataset['chrf-s'])}\")\n",
    "    print(f\"BERT Score: {np.mean(metrics_dataset['bert_score'])}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{spt_name}_final_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_metric(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_metric(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a batch of text using an mT5 model with contextual embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Prepare labels (same as input_ids, but padding tokens should be ignored)\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss calculation\n",
    "\n",
    "    # Expand contextual embeddings\n",
    "    if contextual_embeddings is not None:\n",
    "        contextual_embeddings = contextual_embeddings.to(device)\n",
    "\n",
    "        if contextual_embeddings.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            seq_len = inputs[\"input_ids\"].shape[1]\n",
    "            contextual_embeddings = contextual_embeddings.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Inject embeddings into model\n",
    "        outputs = model(inputs_embeds=contextual_embeddings, attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "        logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Shift logits & labels (for T5)\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention_mask = inputs[\"attention_mask\"][:, 1:].contiguous()\n",
    "\n",
    "    # Compute per-token loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    per_token_loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # Reshape loss\n",
    "    per_token_loss = per_token_loss.view(shift_labels.shape)\n",
    "\n",
    "    # Mask out padding tokens\n",
    "    per_token_loss *= shift_attention_mask\n",
    "\n",
    "    # Compute sentence-level mean loss\n",
    "    sentence_loss = per_token_loss.sum(dim=1) / shift_attention_mask.sum(dim=1)\n",
    "\n",
    "    # Convert to perplexity\n",
    "    perplexity_scores = torch.exp(sentence_loss).cpu().numpy()\n",
    "\n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(spt_name, batch_size=16):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a fine-tuned model using Hugging Face Dataset in batches.\n",
    "    \"\"\"\n",
    "    # Load\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # load dataset\n",
    "    print(f\"Loading dataset for {spt_name}...\")\n",
    "    perplexity_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    perplexity_dataset = convert_to_hf(perplexity_dataset)\n",
    "\n",
    "    # for debug, remove comment\n",
    "    # perplexity_dataset = perplexity_dataset.select(range(100))\n",
    "\n",
    "    print(f\"Computing perplexity in batches of {batch_size}...\")\n",
    "    \n",
    "    def compute_perplexity_batch(batch):\n",
    "        texts = batch[\"generated\"]  # Get text batch\n",
    "        \n",
    "        # Ensure all elements are strings and remove None values\n",
    "        texts = [str(text) if text is not None else \"\" for text in texts]\n",
    "\n",
    "        perplexity_scores = compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings)\n",
    "\n",
    "        return {\"perplexity\": perplexity_scores}\n",
    "\n",
    "    # Compute perplexity in batches\n",
    "    perplexity_dataset = perplexity_dataset.map(compute_perplexity_batch, batched=True, batch_size=batch_size)\n",
    "\n",
    "    # Display Results\n",
    "    mean_perplexity = np.mean(perplexity_dataset[\"perplexity\"])\n",
    "    print(f\"Perplexity Score: {mean_perplexity:.4f}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity_dataset, f\"{spt_name}_final_perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_perplexity(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_perplexity(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for spt_name in model_names.keys():\n",
    "    print(f\"Processing {spt_name}...\")\n",
    "\n",
    "    evaluation_results = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"{spt_name}_final_metrics\")\n",
    "    evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "    evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"{spt_name}_final_perplexity\")\n",
    "    evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_model_variants_gen_df(evaluation_results, f\"{spt_name}_final_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "final_benchmarking_datasets = {}\n",
    "for spt_name in model_names.keys():\n",
    "    df = load_model_variants_gen_df(f\"{spt_name}_final_evaluation_results\")\n",
    "    final_benchmarking_datasets[f\"{spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "final_benchmarking_mean_scores = convert_to_mean_scores_df(final_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(final_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_gen_df(final_benchmarking_mean_scores, \"final_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
