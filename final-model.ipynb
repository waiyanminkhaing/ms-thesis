{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from utils.dataframe import (\n",
    "    load_gen_df, save_tmp_df, load_tmp_df, load_models_df,\n",
    "    save_model_variants_df, load_model_variants_df,\n",
    "    save_model_variants_hf, load_model_variants_hf,\n",
    "    save_model_variants_gen_df, load_model_variants_gen_df,\n",
    "    convert_to_hf, save_model_variants_chunk_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    apply_lora, TRAIN_ARGS,\n",
    "    generate_masked_predictions_hf_batch, generate_mt5_predictions_hf_batch,\n",
    "    compute_metrics_hf_batch,\n",
    "    convert_to_mean_scores_df,\n",
    "    get_fine_tuned_model, get_embedded_fine_tuned_model,\n",
    "    compute_multilingual_masked_perplexity_hf_batch, compute_mt5_perplexity_batch,\n",
    "    extract_metrics_from_logs,\n",
    "    plot_training_metrics, plot_evaluation_metrics\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, MT5ForConditionalGeneration\n",
    ")\n",
    "from peft import PeftModel\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 model path\n",
    "model_names = {\n",
    "    \"bpe\": \"model-variants/models/mT5_BPE\",\n",
    "    \"unigram\": \"model-variants/models/mT5_UNIGRAM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model_with_contextual_embeddings(spt_name):\n",
    "    # Load tokenizers & models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[spt_name], use_fast=False, legacy=True)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "    model = PeftModel.from_pretrained(model, model_names[spt_name]).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load Contextual Embeddings\n",
    "    contextual_embeddings = torch.load(f\"model-variants/gen/{spt_name}_projected_contextual_embeddings.pt\").to(device)\n",
    "\n",
    "    return model, tokenizer, contextual_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions\n",
    "def generate_predictions(spt_name, batch_size=128, max_length=512):\n",
    "\n",
    "    # Load\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # Load dataset \n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # remove comment for debug\n",
    "    # dataset = dataset.select(range(100))\n",
    "\n",
    "    def predict_fn(batch):\n",
    "        \"\"\"\n",
    "        Processes a batch of text inputs with contextual embeddings.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch[\"burmese\"])\n",
    "\n",
    "        # Tokenize input texts\n",
    "        inputs = tokenizer(\n",
    "            batch[\"burmese\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]  # Get sequence length\n",
    "\n",
    "        # Expand contextual embeddings to match input length\n",
    "        contextual_embeds = contextual_embeddings[:batch_size]  # Ensure batch size matches\n",
    "        if contextual_embeds.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            contextual_embeds = contextual_embeds.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Generate text (without decoder_input_ids)\n",
    "        output_tokens = model.generate(\n",
    "            inputs_embeds=contextual_embeds,\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=2,\n",
    "            repetition_penalty=1.5,  # Reduce excessive repetition\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # Decode predictions\n",
    "        generated_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return {\"generated\": generated_texts}\n",
    "    \n",
    "    # Process dataset in batches\n",
    "    dataset = dataset.map(predict_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    # Display results\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    # Save dataset\n",
    "    save_model_variants_gen_df(dataset, f\"{spt_name}_final_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "generate_predictions(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "generate_predictions(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric(spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    metrics_dataset = convert_to_hf(metrics_dataset)\n",
    "\n",
    "    # if debug, remove comment\n",
    "    #metrics_dataset = metrics_dataset.select(range(100))  # Keep this for debugging\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf_batch(metrics_dataset, device)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {np.mean(metrics_dataset['bleu'])}\")\n",
    "    print(f\"ROUGE-1 Score: {np.mean(metrics_dataset['rouge-1'])}\")\n",
    "    print(f\"ROUGE-2 Score: {np.mean(metrics_dataset['rouge-2'])}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(metrics_dataset['rouge-l'])}\")\n",
    "    print(f\"chrF-S Score: {np.mean(metrics_dataset['chrf-s'])}\")\n",
    "    print(f\"BERT Score: {np.mean(metrics_dataset['bert_score'])}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{spt_name}_final_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_metric(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_metric(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a batch of text using an mT5 model with contextual embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Prepare labels (same as input_ids, but padding tokens should be ignored)\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss calculation\n",
    "\n",
    "    # Expand contextual embeddings\n",
    "    if contextual_embeddings is not None:\n",
    "        contextual_embeddings = contextual_embeddings.to(device)\n",
    "\n",
    "        if contextual_embeddings.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            seq_len = inputs[\"input_ids\"].shape[1]\n",
    "            contextual_embeddings = contextual_embeddings.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Inject embeddings into model\n",
    "        outputs = model(inputs_embeds=contextual_embeddings, attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "        logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Shift logits & labels (for T5)\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention_mask = inputs[\"attention_mask\"][:, 1:].contiguous()\n",
    "\n",
    "    # Compute per-token loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    per_token_loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # Reshape loss\n",
    "    per_token_loss = per_token_loss.view(shift_labels.shape)\n",
    "\n",
    "    # Mask out padding tokens\n",
    "    per_token_loss *= shift_attention_mask\n",
    "\n",
    "    # Compute sentence-level mean loss\n",
    "    sentence_loss = per_token_loss.sum(dim=1) / shift_attention_mask.sum(dim=1)\n",
    "\n",
    "    # Convert to perplexity\n",
    "    perplexity_scores = torch.exp(sentence_loss).cpu().numpy()\n",
    "\n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(spt_name, batch_size=16):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a fine-tuned model using Hugging Face Dataset in batches.\n",
    "    \"\"\"\n",
    "    # Load\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # load dataset\n",
    "    print(f\"Loading dataset for {spt_name}...\")\n",
    "    perplexity_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    perplexity_dataset = convert_to_hf(perplexity_dataset)\n",
    "\n",
    "    # for debug, remove comment\n",
    "    # perplexity_dataset = perplexity_dataset.select(range(100))\n",
    "\n",
    "    print(f\"Computing perplexity in batches of {batch_size}...\")\n",
    "    \n",
    "    def compute_perplexity_batch(batch):\n",
    "        texts = batch[\"generated\"]  # Get text batch\n",
    "        \n",
    "        # Ensure all elements are strings and remove None values\n",
    "        texts = [str(text) if text is not None else \"\" for text in texts]\n",
    "\n",
    "        perplexity_scores = compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings)\n",
    "\n",
    "        return {\"perplexity\": perplexity_scores}\n",
    "\n",
    "    # Compute perplexity in batches\n",
    "    perplexity_dataset = perplexity_dataset.map(compute_perplexity_batch, batched=True, batch_size=batch_size)\n",
    "\n",
    "    # Display Results\n",
    "    mean_perplexity = np.mean(perplexity_dataset[\"perplexity\"])\n",
    "    print(f\"Perplexity Score: {mean_perplexity:.4f}\")\n",
    "\n",
    "    # Save dataset\n",
    "    save_tmp_df(perplexity_dataset, f\"{spt_name}_final_perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_perplexity(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_perplexity(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for spt_name in model_names.keys():\n",
    "    print(f\"Processing {spt_name}...\")\n",
    "\n",
    "    evaluation_results = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"{spt_name}_final_metrics\")\n",
    "    evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "    evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"{spt_name}_final_perplexity\")\n",
    "    evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_model_variants_gen_df(evaluation_results, f\"{spt_name}_final_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "final_benchmarking_datasets = {}\n",
    "for spt_name in model_names.keys():\n",
    "    df = load_model_variants_gen_df(f\"{spt_name}_final_evaluation_results\")\n",
    "    final_benchmarking_datasets[f\"{spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "final_benchmarking_mean_scores = convert_to_mean_scores_df(final_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(final_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_gen_df(final_benchmarking_mean_scores, \"final_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
