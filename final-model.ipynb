{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils.dataframe import (\n",
    "    save_tmp_df, load_tmp_df, load_models_df,\n",
    "    save_model_variants_gen_df, load_model_variants_gen_df,\n",
    "    convert_to_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    compute_metrics_hf_batch,\n",
    "    convert_to_mean_scores_df,\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, MT5ForConditionalGeneration\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 model path\n",
    "model_names = {\n",
    "    \"bpe\": \"model-variants/models/mT5_BPE\",\n",
    "    \"unigram\": \"model-variants/models/mT5_UNIGRAM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model_with_contextual_embeddings(spt_name):\n",
    "    # Load tokenizers & models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[spt_name], use_fast=False, legacy=True)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "    model = PeftModel.from_pretrained(model, model_names[spt_name]).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load Contextual Embeddings\n",
    "    contextual_embeddings = torch.load(f\"model-variants/gen/{spt_name}_projected_contextual_embeddings.pt\", map_location=device)\n",
    "\n",
    "    return model, tokenizer, contextual_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(spt_name, batch_size=128, max_length=512):\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # load data\n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # dataset = dataset.select(range(100))\n",
    "\n",
    "    def predict_fn(batch):\n",
    "        batch_size = len(batch[\"burmese\"])\n",
    "\n",
    "        # Tokenize input texts\n",
    "        inputs = tokenizer(\n",
    "            batch[\"burmese\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]  # Get sequence length\n",
    "\n",
    "        # Fix Contextual Embeddings Shape\n",
    "        contextual_embeds = contextual_embeddings[:batch_size]  # Ensure batch size matches\n",
    "        if contextual_embeds.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            contextual_embeds = contextual_embeds.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Ensure correct device\n",
    "        contextual_embeds = contextual_embeds.to(device)\n",
    "\n",
    "        # Convert tokenized inputs to embeddings\n",
    "        input_embeds = model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "\n",
    "        # Inject contextual embeddings by **adding** them to token embeddings\n",
    "        final_embeds = input_embeds + contextual_embeds\n",
    "\n",
    "        # Generate text using **concatenated embeddings**\n",
    "        output_tokens = model.generate(\n",
    "            inputs_embeds=final_embeds,  # Inject contextual embeddings\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=2,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.5,  # Avoids excessive repetition\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        # Decode predictions\n",
    "        generated_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return {\"generated\": generated_texts}\n",
    "\n",
    "\n",
    "    dataset = dataset.map(predict_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    save_model_variants_gen_df(dataset, f\"{spt_name}_final_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4633784eda406785fc3538ce5e796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>burmese</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it's not worth seeing the nubian floor exhibit...</td>\n",
       "      <td>အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...</td>\n",
       "      <td>အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင် ပွဲကို ကြည့်ဖို့ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there are remote whitewashed villages that adv...</td>\n",
       "      <td>စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...</td>\n",
       "      <td>ဝေးလံခေါင် မြို့ကို လည်ပတ် နေတဲ့ ရွာတွေ ရှိတယ်။</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she makes these little tricks, very good, and ...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...</td>\n",
       "      <td>သူမက ဒီပျဉ်းစေ့ကြိုး တွေကို လုပ်ပေး တာ အရမ်းကေ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the pair regained zimbabwe's times and finishe...</td>\n",
       "      <td>ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...</td>\n",
       "      <td>ဇင်ဘာဘွေ သည် ဇင်ဘာဘွေ ၏ အကြိမ်အကြိမ် များကို ပ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>potential of clarifying its notices to taxpaye...</td>\n",
       "      <td>အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...</td>\n",
       "      <td>အခွန်ထမ်း များသည် ၎င်းတို့၏ လုပ်ငန်းတာဝန် များ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  it's not worth seeing the nubian floor exhibit...   \n",
       "1  there are remote whitewashed villages that adv...   \n",
       "2  she makes these little tricks, very good, and ...   \n",
       "3  the pair regained zimbabwe's times and finishe...   \n",
       "4  potential of clarifying its notices to taxpaye...   \n",
       "\n",
       "                                             burmese  \\\n",
       "0  အထက် အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင်ပြပွဲကို ကြည့...   \n",
       "1  စွန့်စားချင်သူတွေ လည်ပတ်ချင်ကြတဲ့ ဝေးလံခေါင်သီ...   \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုးတွေ လုပ်ပေးတယ် အရမ်းကောင်း...   \n",
       "3  ထိုစုံတွဲသည် ဇင်ဘာဘွေ၏ အကြိမ်များကို ပြန်လည်ရရ...   \n",
       "4  အခွန်ထမ်းများထံ ၎င်း၏သတိပေးချက်များကို ရှင်းလင...   \n",
       "\n",
       "                                           generated  \n",
       "0  အီဂျစ်မှာ နူဘီးယား ကြမ်းပြင် ပွဲကို ကြည့်ဖို့ ...  \n",
       "1    ဝေးလံခေါင် မြို့ကို လည်ပတ် နေတဲ့ ရွာတွေ ရှိတယ်။  \n",
       "2  သူမက ဒီပျဉ်းစေ့ကြိုး တွေကို လုပ်ပေး တာ အရမ်းကေ...  \n",
       "3  ဇင်ဘာဘွေ သည် ဇင်ဘာဘွေ ၏ အကြိမ်အကြိမ် များကို ပ...  \n",
       "4  အခွန်ထမ်း များသည် ၎င်းတို့၏ လုပ်ငန်းတာဝန် များ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266823fd70464e8586c053e4b3e35adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with bpe\n",
    "generate_predictions(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "generate_predictions(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric(spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    metrics_dataset = convert_to_hf(metrics_dataset)\n",
    "\n",
    "    # if debug, remove comment\n",
    "    #metrics_dataset = metrics_dataset.select(range(100))  # Keep this for debugging\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf_batch(metrics_dataset, device)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {np.mean(metrics_dataset['bleu'])}\")\n",
    "    print(f\"ROUGE-1 Score: {np.mean(metrics_dataset['rouge-1'])}\")\n",
    "    print(f\"ROUGE-2 Score: {np.mean(metrics_dataset['rouge-2'])}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(metrics_dataset['rouge-l'])}\")\n",
    "    print(f\"chrF-S Score: {np.mean(metrics_dataset['chrf-s'])}\")\n",
    "    print(f\"BERT Score: {np.mean(metrics_dataset['bert_score'])}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{spt_name}_final_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_metric(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_metric(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a batch of text using an mT5 model with contextual embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    ).to(device)\n",
    "\n",
    "    # Prepare labels (ignore padding)\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss calculation\n",
    "\n",
    "    # Ensure contextual embeddings match batch size\n",
    "    if contextual_embeddings is not None:\n",
    "        contextual_embeddings = contextual_embeddings[:len(texts)].to(device).half()\n",
    "\n",
    "        # Normalize embeddings to prevent extreme values\n",
    "        contextual_embeddings = contextual_embeddings / (contextual_embeddings.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "        if contextual_embeddings.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            seq_len = inputs[\"input_ids\"].shape[1]  # Get tokenized input sequence length\n",
    "            contextual_embeddings = contextual_embeddings[:, :seq_len]  # Trim extra tokens if needed\n",
    "            contextual_embeddings = contextual_embeddings.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Fix hidden dimension mismatch (should match model embedding size)\n",
    "        hidden_dim = model.get_input_embeddings().weight.shape[-1]\n",
    "        if contextual_embeddings.shape[-1] != hidden_dim:\n",
    "            contextual_embeddings = torch.nn.functional.pad(\n",
    "                contextual_embeddings,\n",
    "                (0, hidden_dim - contextual_embeddings.shape[-1]),\n",
    "                \"constant\",\n",
    "                0\n",
    "            )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert tokenized inputs to embeddings\n",
    "        input_embeds = model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "\n",
    "        # Inject contextual embeddings using direct addition (matching `generate_predictions`)\n",
    "        if contextual_embeddings is not None:\n",
    "            final_embeds = input_embeds + contextual_embeddings  # **Ensures consistency with generation**\n",
    "        else:\n",
    "            final_embeds = input_embeds\n",
    "\n",
    "        outputs = model(\n",
    "            inputs_embeds=final_embeds,\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    # Shift logits & labels for loss calculation\n",
    "    shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention_mask = inputs[\"attention_mask\"][:, 1:].contiguous()\n",
    "\n",
    "    # Compute per-token loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "    per_token_loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    \n",
    "    # Reshape loss\n",
    "    per_token_loss = per_token_loss.view(shift_labels.shape)\n",
    "\n",
    "    # Mask out padding tokens\n",
    "    per_token_loss *= shift_attention_mask\n",
    "\n",
    "    # Compute sentence-level mean loss\n",
    "    sentence_loss = per_token_loss.sum(dim=1) / shift_attention_mask.sum(dim=1)\n",
    "\n",
    "    # Convert to perplexity (clamping max loss to prevent explosion)\n",
    "    perplexity_scores = torch.exp(torch.clamp(sentence_loss, max=10)).cpu().numpy()\n",
    "\n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(spt_name, part_num=1, batch_size=8, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a specific part of the dataset, using the same method as `generate_predictions`.\n",
    "    \n",
    "    Arguments:\n",
    "        spt_name (str): Model name identifier.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        part_num (int): Part number (1-2) to process.\n",
    "        max_length (int): Maximum sequence length (must match `generate_predictions`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate part number\n",
    "    num_splits = 2\n",
    "    if part_num not in range(1, num_splits + 1):\n",
    "        raise ValueError(f\"Invalid part number. Please choose between 1 and {num_splits}.\")\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset for {spt_name} (Part {part_num})...\")\n",
    "    dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # Load contextual embeddings\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # Split dataset into 6 parts\n",
    "    split_size = len(dataset) // num_splits\n",
    "    datasets = [dataset.select(range(i * split_size, (i + 1) * split_size)) for i in range(num_splits)]\n",
    "\n",
    "    # Split contextual embeddings\n",
    "    contextual_splits = [None] * num_splits  # Default to None if no embeddings\n",
    "    if contextual_embeddings is not None:\n",
    "        contextual_splits = [contextual_embeddings[i * split_size: (i + 1) * split_size] for i in range(num_splits)]\n",
    "\n",
    "    # Get the dataset and contextual embeddings for the selected part\n",
    "    dataset_part = datasets[part_num - 1]\n",
    "    contextual_embeddings_part = contextual_splits[part_num - 1]\n",
    "\n",
    "    # remove comment for debug\n",
    "    # dataset_part = dataset_part.select(range(100))\n",
    "\n",
    "    print(f\"Processing Part {part_num} with {len(dataset_part)} samples...\")\n",
    "\n",
    "    def compute_perplexity_fn(batch):\n",
    "        \"\"\"\n",
    "        Compute perplexity for a batch of text.\n",
    "        \"\"\"\n",
    "        batch_size = len(batch[\"generated\"])  # Ensure batch size consistency\n",
    "\n",
    "        # Extract text inputs\n",
    "        texts = [str(text) if text is not None else \"\" for text in batch[\"generated\"]]\n",
    "\n",
    "        # Fix Contextual Embeddings Shape\n",
    "        contextual_embeds = contextual_embeddings_part[:batch_size] if contextual_embeddings_part is not None else None\n",
    "        if contextual_embeds is not None and contextual_embeds.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            seq_len = max(len(tokenizer.encode(text)) for text in texts)  # Get max seq length in batch\n",
    "            contextual_embeds = contextual_embeds[:, :seq_len]  # Trim contextual embeddings to match seq length\n",
    "            contextual_embeds = contextual_embeds.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Compute perplexity\n",
    "        perplexity_scores = compute_perplexity_batch(texts, model, tokenizer, contextual_embeds, max_length)\n",
    "\n",
    "        return {\"perplexity\": perplexity_scores}\n",
    "\n",
    "    # Compute perplexity in batches\n",
    "    dataset_part = dataset_part.map(compute_perplexity_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(dataset_part, f\"{spt_name}_final_perplexity_part_{part_num}\")\n",
    "\n",
    "    print(f\"Completed Part {part_num} Processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "compute_perplexity(\"bpe\", part_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "compute_perplexity(\"bpe\", part_num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "compute_perplexity(\"unigram\", part_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "compute_perplexity(\"unigram\", part_num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine evaluation results\n",
    "for spt_name in model_names.keys():\n",
    "    print(f\"Processing {spt_name}...\")\n",
    "\n",
    "    evaluation_results = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"{spt_name}_final_metrics\")\n",
    "    evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "    evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity = load_tmp_df(f\"{spt_name}_final_perplexity\")\n",
    "    evaluation_results[\"perplexity\"] = perplexity[\"perplexity\"]\n",
    "\n",
    "    save_model_variants_gen_df(evaluation_results, f\"{spt_name}_final_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "final_benchmarking_datasets = {}\n",
    "for spt_name in model_names.keys():\n",
    "    df = load_model_variants_gen_df(f\"{spt_name}_final_evaluation_results\")\n",
    "    final_benchmarking_datasets[f\"{spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "final_benchmarking_mean_scores = convert_to_mean_scores_df(final_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mean scores\n",
    "display(final_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_gen_df(final_benchmarking_mean_scores, \"final_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
