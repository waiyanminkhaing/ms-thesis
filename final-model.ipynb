{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from utils.dataframe import (\n",
    "    save_tmp_df, load_tmp_df, load_models_df,\n",
    "    save_model_variants_gen_df, load_model_variants_gen_df,\n",
    "    convert_to_hf,\n",
    ")\n",
    "from utils.gpu import get_device\n",
    "from utils.common import (\n",
    "    compute_metrics_hf_batch,\n",
    "    convert_to_mean_scores_df,\n",
    ")\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    logging,\n",
    "    AutoTokenizer, MT5ForConditionalGeneration\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# gpu device \n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mT5 model path\n",
    "model_names = {\n",
    "    \"bpe\": \"model-variants/models/mT5_BPE\",\n",
    "    \"unigram\": \"model-variants/models/mT5_UNIGRAM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model_with_contextual_embeddings(spt_name):\n",
    "    # Load tokenizers & models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[spt_name], use_fast=False, legacy=True)\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "    model = PeftModel.from_pretrained(model, model_names[spt_name]).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load Contextual Embeddings\n",
    "    contextual_embeddings = torch.load(f\"model-variants/gen/{spt_name}_projected_contextual_embeddings.pt\", map_location=device)\n",
    "\n",
    "    return model, tokenizer, contextual_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(spt_name, batch_size=128, max_length=512):\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # load data\n",
    "    dataset = load_models_df(\"multilingual_combined\")\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # dataset = dataset.select(range(100))\n",
    "\n",
    "    def predict_fn(batch):\n",
    "        batch_size = len(batch[\"burmese\"])\n",
    "\n",
    "        # Tokenize input texts\n",
    "        inputs = tokenizer(\n",
    "            batch[\"burmese\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        seq_len = inputs[\"input_ids\"].shape[1]  # Get sequence length\n",
    "\n",
    "        # Fix Contextual Embeddings Shape\n",
    "        contextual_embeds = contextual_embeddings[:batch_size]  # Ensure batch size matches\n",
    "        if contextual_embeds.dim() == 2:  # (batch_size, hidden_dim)\n",
    "            contextual_embeds = contextual_embeds.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Ensure correct device\n",
    "        contextual_embeds = contextual_embeds.to(device)\n",
    "\n",
    "        # Convert tokenized inputs to embeddings\n",
    "        input_embeds = model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "\n",
    "        # Inject contextual embeddings by **adding** them to token embeddings\n",
    "        final_embeds = input_embeds + contextual_embeds\n",
    "\n",
    "        # Generate text using **concatenated embeddings**\n",
    "        output_tokens = model.generate(\n",
    "            inputs_embeds=final_embeds,  # Inject contextual embeddings\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=2,\n",
    "            use_cache=True,\n",
    "            repetition_penalty=1.5,  # Avoids excessive repetition\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        # Decode predictions\n",
    "        generated_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return {\"generated\": generated_texts}\n",
    "\n",
    "\n",
    "    dataset = dataset.map(predict_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    display(dataset.to_pandas().head())\n",
    "\n",
    "    save_model_variants_gen_df(dataset, f\"{spt_name}_final_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "generate_predictions(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "generate_predictions(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Compute BLEU, ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-L, chrF-S, BERTScore and Perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity num splits\n",
    "perplexity_num_splits = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Compute Metrics for Fine-Tuned Model using HF Dataset\n",
    "def compute_metric(spt_name):\n",
    "    # Load dataset\n",
    "    metrics_dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    metrics_dataset = convert_to_hf(metrics_dataset)\n",
    "\n",
    "    # if debug, remove comment\n",
    "    #metrics_dataset = metrics_dataset.select(range(100))  # Keep this for debugging\n",
    "\n",
    "    # Compute metrics\n",
    "    print(f\"Processing Data for {spt_name.upper()}...\")\n",
    "    metrics_dataset = compute_metrics_hf_batch(metrics_dataset, device)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Metrics scores for {spt_name.upper()}:\")\n",
    "    print(f\"BLEU Score: {np.mean(metrics_dataset['bleu'])}\")\n",
    "    print(f\"ROUGE-1 Score: {np.mean(metrics_dataset['rouge-1'])}\")\n",
    "    print(f\"ROUGE-2 Score: {np.mean(metrics_dataset['rouge-2'])}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(metrics_dataset['rouge-l'])}\")\n",
    "    print(f\"chrF-S Score: {np.mean(metrics_dataset['chrf-s'])}\")\n",
    "    print(f\"BERT Score: {np.mean(metrics_dataset['bert_score'])}\")\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(metrics_dataset, f\"{spt_name}_final_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bpe\n",
    "compute_metric(\"bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with unigram\n",
    "compute_metric(\"unigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a batch of text using an mT5 model with contextual embeddings.\n",
    "    \"\"\"\n",
    "    batch_size = len(texts)  # Ensure batch size consistency\n",
    "    \n",
    "    # Tokenize texts\n",
    "    inputs = tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "    ).to(device)\n",
    "\n",
    "    seq_len = inputs[\"input_ids\"].shape[1]  # Get sequence length\n",
    "\n",
    "    # Fix Contextual Embeddings Shape\n",
    "    contextual_embeddings = contextual_embeddings[:batch_size]  # Ensure batch size matches\n",
    "    if contextual_embeddings.dim() == 2:  # (batch_size, hidden_dim)\n",
    "        contextual_embeddings = contextual_embeddings.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "    # Ensure correct device\n",
    "    contextual_embeddings = contextual_embeddings.to(device)\n",
    "\n",
    "    # Prepare labels (ignore padding)\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss calculation\n",
    "\n",
    "    # Convert tokenized inputs to embeddings\n",
    "    with torch.no_grad():\n",
    "        input_embeds = model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "\n",
    "        # Inject contextual embeddings by **adding** them to token embeddings\n",
    "        final_embeds = input_embeds + contextual_embeddings\n",
    "\n",
    "        outputs = model(\n",
    "            inputs_embeds=final_embeds,\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    # Shift logits & labels for loss calculation\n",
    "    shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_attention_mask = inputs[\"attention_mask\"][:, 1:].contiguous()\n",
    "\n",
    "    # Compute per-token loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")\n",
    "    per_token_loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    \n",
    "    # Reshape loss\n",
    "    per_token_loss = per_token_loss.view(shift_labels.shape)\n",
    "\n",
    "    # Mask out padding tokens\n",
    "    per_token_loss *= shift_attention_mask\n",
    "\n",
    "    # Compute sentence-level mean loss\n",
    "    sentence_loss = per_token_loss.sum(dim=1) / shift_attention_mask.sum(dim=1)\n",
    "\n",
    "    # Convert to perplexity (clamping max loss to prevent explosion)\n",
    "    perplexity_scores = torch.exp(torch.clamp(sentence_loss, max=10)).cpu().numpy()\n",
    "\n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(spt_name, part_num=1, batch_size=8, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for a specific part of the dataset, using the same method as `generate_predictions`.\n",
    "    \n",
    "    Arguments:\n",
    "        spt_name (str): Model name identifier.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        part_num (int): Part number (1-2) to process.\n",
    "        max_length (int): Maximum sequence length (must match `generate_predictions`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate part number\n",
    "    if part_num not in range(1, perplexity_num_splits + 1):\n",
    "        raise ValueError(f\"Invalid part number. Please choose between 1 and {perplexity_num_splits}.\")\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset for {spt_name} (Part {part_num})...\")\n",
    "    dataset = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "    dataset = convert_to_hf(dataset)\n",
    "\n",
    "    # Load contextual embeddings\n",
    "    model, tokenizer, contextual_embeddings = get_final_model_with_contextual_embeddings(spt_name)\n",
    "\n",
    "    # Split dataset into 6 parts\n",
    "    split_size = len(dataset) // perplexity_num_splits\n",
    "    datasets = [dataset.select(range(i * split_size, (i + 1) * split_size)) for i in range(perplexity_num_splits)]\n",
    "\n",
    "    # Split contextual embeddings\n",
    "    contextual_splits = [None] * perplexity_num_splits  # Default to None if no embeddings\n",
    "    if contextual_embeddings is not None:\n",
    "        contextual_splits = [contextual_embeddings[i * split_size: (i + 1) * split_size] for i in range(perplexity_num_splits)]\n",
    "\n",
    "    # Get the dataset and contextual embeddings for the selected part\n",
    "    dataset_part = datasets[part_num - 1]\n",
    "    contextual_embeddings_part = contextual_splits[part_num - 1]\n",
    "\n",
    "    # remove comment for debug\n",
    "    # dataset_part = dataset_part.select(range(100))\n",
    "\n",
    "    print(f\"Processing Part {part_num} with {len(dataset_part)} samples...\")\n",
    "\n",
    "    def compute_perplexity_fn(batch):\n",
    "        \"\"\"\n",
    "        Compute perplexity for a batch of text.\n",
    "        \"\"\"\n",
    "        # Extract text inputs\n",
    "        texts = [str(text) if text is not None else \"\" for text in batch[\"generated\"]]\n",
    "\n",
    "        # Compute perplexity\n",
    "        perplexity_scores = compute_perplexity_batch(texts, model, tokenizer, contextual_embeddings_part, max_length)\n",
    "\n",
    "        return {\"perplexity\": perplexity_scores}\n",
    "\n",
    "    # Compute perplexity in batches\n",
    "    dataset_part = dataset_part.map(compute_perplexity_fn, batched=True, batch_size=batch_size)\n",
    "\n",
    "    # Save results\n",
    "    save_tmp_df(dataset_part, f\"{spt_name}_final_perplexity_part_{part_num}\")\n",
    "\n",
    "    print(f\"Completed Part {part_num} Processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "compute_perplexity(\"bpe\", part_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "compute_perplexity(\"bpe\", part_num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "compute_perplexity(\"unigram\", part_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "compute_perplexity(\"unigram\", part_num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bpe...\n",
      "Processing unigram...\n"
     ]
    }
   ],
   "source": [
    "# combine evaluation results\n",
    "for spt_name in model_names.keys():\n",
    "    print(f\"Processing {spt_name}...\")\n",
    "\n",
    "    evaluation_results = load_model_variants_gen_df(f\"{spt_name}_final_predictions\")\n",
    "\n",
    "    # load metrics and set\n",
    "    metrics = load_tmp_df(f\"{spt_name}_final_metrics\")\n",
    "    evaluation_results[\"bleu\"] = metrics[\"bleu\"]\n",
    "    evaluation_results[\"rouge-1\"] = metrics[\"rouge-1\"]\n",
    "    evaluation_results[\"rouge-2\"] = metrics[\"rouge-2\"]\n",
    "    evaluation_results[\"rouge-l\"] = metrics[\"rouge-l\"]\n",
    "    evaluation_results[\"chrf-s\"] = metrics[\"chrf-s\"]\n",
    "    evaluation_results[\"bert_score\"] = metrics[\"bert_score\"]\n",
    "\n",
    "    # load perplexity and set\n",
    "    perplexity_df_list = []\n",
    "    for i in range(perplexity_num_splits):\n",
    "        perplexity = load_tmp_df(f\"{spt_name}_final_perplexity_part_{i + 1}\")\n",
    "        perplexity_df_list.append(perplexity)\n",
    "    \n",
    "    evaluation_results[\"perplexity\"] = pd.concat(perplexity_df_list, ignore_index=True)[\"perplexity\"]\n",
    "\n",
    "    save_model_variants_gen_df(evaluation_results, f\"{spt_name}_final_evaluation_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "final_benchmarking_datasets = {}\n",
    "for spt_name in model_names.keys():\n",
    "    df = load_model_variants_gen_df(f\"{spt_name}_final_evaluation_results\")\n",
    "    final_benchmarking_datasets[f\"{spt_name.upper()}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to mean score df\n",
    "final_benchmarking_mean_scores = convert_to_mean_scores_df(final_benchmarking_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>chrF-S</th>\n",
       "      <th>BERT Score</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BPE</th>\n",
       "      <td>BPE</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.183905</td>\n",
       "      <td>0.080225</td>\n",
       "      <td>0.183851</td>\n",
       "      <td>33.708929</td>\n",
       "      <td>0.784354</td>\n",
       "      <td>3861.230787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNIGRAM</th>\n",
       "      <td>UNIGRAM</td>\n",
       "      <td>0.037648</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.080458</td>\n",
       "      <td>0.182468</td>\n",
       "      <td>35.214015</td>\n",
       "      <td>0.789341</td>\n",
       "      <td>3756.724939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name      BLEU   ROUGE-1   ROUGE-2   ROUGE-L     chrF-S  \\\n",
       "BPE          BPE  0.039361  0.183905  0.080225  0.183851  33.708929   \n",
       "UNIGRAM  UNIGRAM  0.037648  0.182512  0.080458  0.182468  35.214015   \n",
       "\n",
       "         BERT Score   Perplexity  \n",
       "BPE        0.784354  3861.230787  \n",
       "UNIGRAM    0.789341  3756.724939  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display mean scores\n",
    "display(final_benchmarking_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save benchmarking results\n",
    "save_model_variants_gen_df(final_benchmarking_mean_scores, \"final_evaluation_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
